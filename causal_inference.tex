\documentclass[11pt,reqno,titlepage]{amsart}

\input{preamble}

\title[Causal Inference in Accounting]{Causal Inference in Accounting Research}

\author{Ian D. Gow}
\author{David F. Larcker}
\author{Peter C. Reiss}

%\date{}   % Activate to display a given date or no date

\begin{document}
\usetikzlibrary{automata, shapes, calc, positioning}

\bibliographystyle{chicago}
% Quick LaTeX Guide for Dave (originally for Suraj).

% - Percent signs (%) mark comments. To get a percent sign, escape it by putting a backslash in front.
%  & is another special character in LaTeX. Use \& to get &.
% Note that each part of the document is in a separate file (so we can edit in parallel).
% Citations are automatic with the correct key. 
% LaTeX doesn't pay attention to multiple spaces. Also adjacent lines get collapsed into single paragraphs.
% Insert a blank line between lines that are part of two separate paragraphs.
% It's actually helpful to put every sentence on a separate line. 
% You need two line breaks to indicate a paragraph.
% \section, \subsection, and \subsubsection have the obvious meanings.
% Note that there is a file jar_methods.bib in the list of files to the right that this pulls bibliographic information from.
\begin{titlepage}
  \centering
  	\begin{large}
  	\textbf{Causal Inference in Accounting Research\footnote{We thank seminar participants at London Business School, Karthik Balakrishnan, Philip Berger, Bob Kaplan, Alexander Ljungqvist, Eugene Soltes, Dan Taylor, Ro Verrecchia, Charlie Wang, and Anastasia Zakolyukina for helpful discussions and feedback.}} \\	
  	\end{large}
  	\vspace{60pt}
	\textbf{Ian D. Gow} \\
	Harvard Business School \\
	email: igow@hbs.edu

  	\vspace{30pt}
	\textbf{David F. Larcker} \\
	Stanford Graduate School of Business \\
	Rock Center for Corporate Governance \\
	email: dlarcker@stanford.edu \\
		
	\vspace{30pt}
	\textbf{Peter C. Reiss} \\
	Stanford Graduate School of Business \\
	email: preiss@stanford.edu \\

	\vspace{30pt}
	\today
	
	\vspace{30pt}
	\centerline{\bf Rough Draft}
	

\end{titlepage}


\begin{abstract}
	This paper examines the approaches accounting researchers use to draw causal inferences using observational (or non-experimental) data. 
	The vast majority of accounting research papers draw causal inferences notwithstanding the well-known difficulties with doing so with observational data.
	While a minority of papers seek to use quasi-experimental methods to draw inferences, there are concerns about how these methods are typically applied.
	We believe that accounting research would benefit from: 
		a greater focus on the study of causal mechanisms (or causal pathways); 
		increased emphasis on structural modeling of the phenomena of interest; 
		and, more in-depth descriptive research. 
	We argue these changes are possible and offer a practical path forward for rigorous accounting research. 
\end{abstract}

\maketitle
\clearpage

\section{Introduction}

\begin{quotation}\begin{singlespace} 
There is perhaps no more controversial practice in social and biomedical research than drawing inferences from observational data.
Despite \dots problems, observational data are widely available in many scientific fields and are routinely used to draw inferences about the causal impact of interventions.
The key issue, therefore, is not whether such studies should be done, but how they may be done well.
\attrib{\citealt{Berk:1999uz}}
\end{singlespace}
\end{quotation}

\begin{doublespace} 
Most empirical research in accounting relies on observational data (i.e., data produced by processes outside the control of the researcher).
This paper evaluates the different approaches accounting researchers use to draw causal inferences from observational data. 
Our discussion draws on developments in 
fields such as statistics, econometrics and epidemiology. The goal of this paper is to identify areas for improvement and suggest how empirical accounting research can improve inferences from the analysis of observational data.

The importance of causal inference in accounting research is clear from the research questions that accounting researchers seek to answer. 
Most long-standing questions in accounting research are causal: 
Does conservatism affect the terms of loan contracts?
Do higher quality earnings reports lead to lower information asymmetry? 
Did IFRS cause an increase in liquidity in the jurisdictions that adopted it?
Do managerial incentives lead to managerial manipulations and misstatements of financial reports?
That accounting researchers focus on causal inference is consistent with the view that ``the most interesting research in social science is about questions of cause and effect" \cite[p. 3]{Angrist:2008vk}.
Simply documenting descriptive correlations provides little basis for understanding what would happen should circumstances change, 
whereas using data to make inferences that support or refute broader theories could facilitate these kinds of predictions.

Accounting researchers are aware of problems that can arise from the use of observational data.
% refer to Heckman-style methods as "treatment effect models?  Maybe cite Francis paper in TAR here?
For instance, many researchers who estimate so-called treatment effect models are aware that random assignment is usually required to ensure that differences between the treatment and control samples is due to the treatment. 
Despite this awareness, many accounting ``treatments" are not randomly assigned, and the researcher then is in the position of arguing that they can account for all other reasons that might lead to a difference between the treatment and control samples.

To provide some contemporary insight into what is actually done in empirical accounting research, we examined all empirical papers published in the leading accounting journals in 2014. 
We found that  these papers overwhelmingly used observational data and descriptive statistical models to estimate the causal impact of "$X$ on $y$." 
This approach makes strong assumptions about the functional relationship between $X$ and $y$, and requires that sufficient controls are present to account for changes in conditions related to changes in $X$.
Section \ref{sec:causal} examines these assumptions in detail.
We also introduce causal graphs as a device that can clearly communicate the causal reasoning underlying an empirical model.
We believe that these graphs are very useful for clearly communicating the cause-and-effect logic underlying the typical regression analyses used with observational data.
%Nonetheless, the fact that treatment is not randomly assigned leads many researchers to be skeptical of any efforts to use regression analyses of observational data for causal inference.


Recently, some social scientists have held out hope that better research designs and statistical methods can increase the credibility of causal inferences.
For example, \citet{Angrist:2010jv} suggest that ``empirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact.''  
\citet[p. 26]{Angrist:2010jv} argue that such ``improvement has come mostly from better research designs, either by virtue of outright experimentation or through the well-founded and careful implementation of quasi-experimental methods."
Our survey of research published in 2014 finds five studies claiming to study natural experiments (or ``exogenous shocks") and ten studies using instrumental variables.
Thus, quasi-experimental methods are used to some degree in accounting research, and we believe their use will increase in future research efforts.\footnote{
We use the term ``quasi-experimental" methods to refer to those methods that have a plausible claim to ``as if" random assignment to treatment conditions.
The term ``as if" is used by \citet{Dunning:2012tt} to acknowledge the fact that assignment is not random is such settings, but is claimed to be \emph{as if} random assignment had occurred.}

In Section \ref{sec:quasi}, we examine and evaluate the use of quasi-experimental methods in accounting research.
We believe that these applications generally exhibit a variety of shortcomings that seriously limit the ability of authors to drawn causal inferences from their research results.  For example, variations in treatments are rarely random and it is exceedingly difficult to find exogenous instrument variables.
We demonstrate some of these concerns using causal diagrams.  In general, it appears that the assumptions underlying quasi-experimental methods are unlikely to apply to much empirical accounting research using observational data.


% Dave: "One especially promising path is use of field experiments with randomized treatments to measure causal effects (a good example -- Roberts QJE paper)." % \citet{Roberts:2013cz} 
% Ian: "I think we could discuss these as interesting and a path forward, but scope them out as our focus is on observational data."
Ultimately, we believe that accounting research needs to lean less heavily on statistical models applied to observational data.
Statistical methods alone cannot solve the inference issues that arise in observational data. 
The second part of the paper (Sections 4, 5, and 6) identifies approaches that can provide a plausible framework for guiding future accounting research:
% 

\vskip -10pt
\begin{itemize}
\item There should be an increased emphasis on the study of causal mechanisms.
Here we believe that causal diagrams can help clarify what assumptions an accounting researcher must make about putative causes and putative effects. 
Progress can occur even in the absence of knowing the exact process that generated the (observational) data.
\item There should be an increased use of structural modeling methods. Structural models provide a more complete characterization of the behavior and institutions that produce a phenomenon of interest. We readily acknowledge that structural modeling does not solve endogeneity concerns, but makes it the assumed causal structure explicit and gives the researchers a rigorous way to assess what would happen if some features of the model change (i.e., provide counterfactuals). 
%TODO: Flesh this out somewhat. What are the benefits of doing structural modeling?
\item There are many important questions in accounting that have not yet been addressed by formal models.  In these settings, it is important to conduct sophisticated descriptive research aimed at understanding the real institutional settings and developing hypotheses, particularly when it comes to causes and effects. In our view, many hypotheses that are tested with observational data are only loosely tied to the accounting institutions and business phenomena of interest. Hopefully, these descriptive studies will provide insights to theorists that can be used to build models that empiricists can actually "take to data" using structural modeling approaches.
	%TODO: The 2017 JAR call for papers is arguably evidence of this concern being shared by others. Considering adding a footnote saying this.  DL -- this is a good idea
	%A better approach would be to draw on detailed studies that better \emph{describe} 
	%Such studies likely will have a greater chance of identifying causal pathways and entice theorists to build better models of the phenomena of interest.
	%TODO: Trim the "descriptive research" bullet point.
\end{itemize}

\vskip 10pt
The remainder of the paper is structured as follows.
Section \ref{sec:causal} provides an overview of the issues observational data pose for drawing causal inferences in accounting research; 
it also suggests frameworks for identifying and analyzing these issues.
Section \ref{sec:quasi} evaluates the use of quasi-experimental methods in accounting research.
Section \ref{sec:mech} sketches the idea of mechanism-based causal inference.
Section \ref{sec:struct} illustrates how structural modeling approaches might be used by accounting researchers, with some emphasis of the strengths and weaknesses of this approach.
In Section \ref{sec:desc} we argue for richer descriptive research that can shed light on causal issues.
Concluding remarks are provided in Section \ref{sec:conclude}.

\section{Causal inference: An overview} \label{sec:causal}

\subsection{Causal inference in accounting research}
%TODO: Flag experimental research, then say we're focused on studies using observational data.
%TODO: Add a footnote to the guy doing something similar is AOS -- claims that only 3% of papers are causal.
%TODO: Add a footnote *somewhere* discussing other work on "causal diagrams" in accounting research. I think we want to do no more than mention that these exist and suggest that what we're doing is a bit more formal.


To get a sense for the importance of causal questions in accounting research,
we examined all papers published in 2014 in the \textit{Journal of Accounting Research}, \textit{The Accounting Review}, and the \textit{Journal of Accounting and Economics}.
We counted 139 papers, of which 125 are original research papers. Another 14 papers survey or discuss other papers.
We classify each of the 125 research papers into one of four categories:  ``Theoretical'' (7); ``Experimental'' (12); ``Field" (3); or ``Archival Data" (103). 
For our discussion below, we collect the field and archival data papers into a single category, "Observational".

For each non-theoretical paper, we determine whether the primary or secondary research questions are ``causal". One often does not have to look far to find causal claims. Words
such as ``effect of \dots" or ``impact of \dots" in titles often signal causal inferences   
\citep[e.g.][]{Cohen:2014jl,Clorproell:2014cv}. 
Abstracts and conclusions also reveal that authors have causal inferences as a goal. 
For example, \citet{deFranco:2014ct} asks ``how the tone of sell-side debt analysts' discussions about debt-equity conflict events \emph{affects} the informativeness of debt analysts' reports in debt markets.''

We recognize that some authors might disagree with our classifications.
We know some would say their paper only claimed that ``theory predicts $X$ is associated $Y$ and, consistent with that theory, we show $X$ is associated with $Y$".
However in the abstracts, introductions and conclusions of papers, these qualifications are rarely present. 
Instead the writing suggests the evidence in the paper tilts the interpretation in the direction of confirming a causal hypothesis.\footnote{
Papers that seek to estimate a causal effect of $X$ on $Y$ are a subset of papers we classify as causal.
A paper that argues that $Z$ is a common cause of $X$ and $Y$ and claims to find evidence of this is still making causal inferences (i.e., that $Z$ causes $X$ and $Z$ causes $Y$.
However, we do not find this kind of reasoning to be common in our survey.}
%TODO: Find an example of this kind of paper. I know there are some.

Of the 106 original papers using observational data, we coded 91 as about causality.\footnote{While we exclude research papers using experimental methods, all of these papers also seek to draw causal inferences.}
Of the remaining empirical papers, we coded 7 papers as having a goal of ``description'' (including two of the three field papers). 
For example, \citet{Soltes:2013ba} uses data collected from one firm to describe analysts' private interactions with management. Understanding how these interactions take place is key to understanding whether and how they transmit information to the market.
We coded 5 papers as having a goal of ``prediction.'' 
For example, \citet{Czerney:2014bv} examine whether the inclusion of ``explanatory language" in unqualified audit reports can be used to predict the detection of financial misstatements in the future.
We coded 3 papers as having a goal of ``measurement.'' 
For example, \citet{Cready:2014ji} examine whether inferences about traders based on trade size are reliable and suggest improvements to the measurement of variables used by accounting researchers.

In summary, we find that most original research papers use observational data and that about 90\% of these papers seek to draw causal inferences.
The most common estimation methods used in these studies include ordinary least-squares (OLS) regression, difference-in-difference estimates, and propensity-score matching.
While it is widely understood that OLS regressions that use observational data produce unbiased estimates of causal effects only under very strong assumptions, the credibility of these assumptions is rarely explicitly addressed.\footnote{
There are settings where difference-in-difference and fixed effect estimators may deliver causal estimates.
For example, if assignment to treatment is random, then it is possible for a difference-in-difference estimate using pre- and post-treatment data to yield unbiased estimates of causal effects.
But in this case, it is the detailed understanding of the research setting, not the method \emph{per se}, that makes these estimates credible.}
%TODO: So why do we see so many papers using OLS for causal inference? My conjecture: If the results is the "right" one, no-one cares about endogeneity. So WTF are we doing when we do research? (Even by the standards of this paper, this point is a little "heavy" I think.)

\subsubsection{Difference-in-difference and fixed effect estimators}
Accounting researchers have come to view some statistical methods as requiring fewer assumptions and thus being less subject to problems when it comes to drawing causal inferences. 
\citet[p.\,12]{Angrist:2010jv} include so-called difference-in-difference (DD) estimators on their list of such quasi-experimental methods, along with ``instrumental variables and regression discontinuity methods."\footnote{As \citet[p.\,228]{Angrist:2008vk} argue that ``DD is a version of fixed effects estimation," we discuss these methods together.}
Enthusiasm for DD designs perhaps stems from a belief that these are ``quasi-experimental" methods in the same sense as the other two approaches cited by \citet[p.\,12]{Angrist:2010jv}.
But the essential feature that instrumental variables and regression discontinuity methods rely on is the ``as if" random treatment assignment mechanism.
If treatment assignment is driven by unobserved confounding variables, then DD and fixed-effect estimates will be biased and inconsistent. 
It is not clear that there are many settings in accounting that are likely to satisfy random treatment assignment.
In order to draw a causal inference, this means that researchers  must explain why they believe that DD or fixed-effect estimators allow them to recover unbiased estimates of causal effects.

%Even when assignment to treatment is random, care needs to be taken in interpreting estimates as causal effects.
%For example, \citet[p.\,1305]{Cadman:2014cr} conjecture that ``VCs have strong incentives to design compensation schemes that provide CEOs with short-horizon incentives in the fiscal years after the IPO." 
%The main analysis in support of this hypothesis is a regression DD analysis \citep[pp.\,233--241]{Angrist:2008vk} using the pre-IPO year as the pre-treatment period, and the two years after IPO as the post-treatment period. 
%However, one treatment of interest (i.e., being VC-backed) is likely implemented well before the ``pre-treatment" period, making it difficult to consider the pre-treatment values of the outcome variable as not being caused by the treatment.

\subsubsection{Propensity score matching}
Another method that has become popular in accounting research is propensity score matching (PSM).
Regression methods can be viewed as making model-based adjustments to address confounding variables.  
Stuart and Rubin (2007) argue that 

\begin{quote}
``[M]atching methods are preferable to these model-based adjustments for two key reasons. 
First, matching methods do not use the outcome values in the design of the study and thus preclude the selection of a particular design to yield a desired result.
Second, when there are large differences in the covariate distributions between the groups, standard model-based adjustments rely heavily on extrapolation and model-based assumptions.
Matching methods highlight these differences and also provide a way to limit reliance on the inherently untestable modeling assumptions and the consequential sensitivity to those assumptions."
\end{quote}
For these reasons, PSM methods can prove useful when faced with observational data.
However, PSM does \emph{not} provide ``the closest archival approximation to a true random experiment" and does \emph{not} not represent "the most appropriate and rigorous research design for testing the effects of an ex ante treatment" \citep[p.\,1429]{Kirk:2014gx}.
\citet[pp.\,73-75]{Rosenbaum:2009ul} points out that matching is ``a fairly mechanical  task," and when assignment to treatment is driven by unobservable variables, PSM-based estimates may be biased as much as regression estimates.
We agree with \citet{MinuttiMeza:2014fn} who argues that ``matching does not necessarily eliminate the endogeneity problem resulting from unobservable variables driving [treatment] and [outcomes]."

%TODO: Add a general evaluation of the use of PSM for causal inferences in accounting 
%TODO: Expand survey to discuss use of PSM in accounting.
%TODO: Discuss papers matching on post-treatment variables.
%TODO: Find a home for discussion of bounding. I don't think it belongs with PSM.

\subsection{Causal inference: A brief overview}
In recent decades, the definition and logic of causality has seen renewed interest from researchers in such diverse fields as epidemiology, sociology, statistics, and computer science. 
Work by \citet{Rubin:1974im,Rubin:1977dv} and \citet{Holland:1986p7458} formalizing ideas from the potential-outcome framework of \citet{Neyman:1923aa} have led to the so-called the Rubin causal model. 
Other fields have used path analysis, as initially studied by geneticist Sewell Wright \citep{Wright:1921aa}, as an organizing framework.
In economics and econometrics, early proponents of structural models were quite clear about how causal statements must be tied to theoretical economic models.
As discussed by \citet{Heckman:2015ez}, \citet{Haavelmo:1943cl,Haavelmo:1944jq} promoted structural models ``based on a system of structural equations that define causal relationships among a set of variables."
%standard econometric texts generally avoid explicit discussion of causation.
%For example, Greene (2003) does not discuss causality except for Granger causality, which is widely recognized as a purely statistical notion quite distinct from notions of one variable causing another.
% However, some economists have explic
\citet[p.\,979]{Goldberger:1972cq} promoted a similar notion: ``By structural equation models, I refer to stochastic models in which each equation represents a causal link, rather than a mere empirical association \dots
Generally speaking the structural parameters do not coincide with coefficients of regressions among observable variables, but the model does impose constraints on those regression coefficients."
\citet{Goldberger:1972cq} focuses on linking such approaches to the path analysis of Wright.

An important point worth emphasizing is that model-based causal reasoning is distinct from statistical reasoning. Suppose we observed data on $x$ and $y$ and know only that one causes the other. How do we distinguish between whether $x$ causes $y$ or $y$ causes $x$? 
Statistics can help us determine whether $x$ and $y$ are correlated, but correlations do not establish causality.
Only with assumptions about causal relations between $x$, $y$, and other variables (i.e., a theory) can we infer causality.
While theories may be informed by evidence (e.g., prior research may suggest a given theory is more or less plausible), they also encode our understanding of causal mechanisms (e.g., barometers do not cause rain) and economic and behavioral assumptions.

\subsection{Causal diagrams: A primer}
Computer and decision scientists, as well as researchers in other disciplines, have recently sought to develop an analytical framework for thinking about causal models and their connection to probability statements \citep{Pearl:2009kh}.
Pearl's framework, which he calls the structural causal model, uses causal diagrams to describe causal relationships. 
These diagrams encode causal assumptions and visually communicate how a causal inference is being drawn from a given research design.
Given a \emph{correctly specified} causal diagram, these criteria can be used to verify conditioning strategies, instrumental variable designs, and mechanism-based causal inferences.\footnote{While \citet[p.248]{Pearl:2009kh} defines an instrument in terms of causal diagrams, additional assumptions (e.g., linearity) are often needed to estimate causal effects using an instrument \citep{Angrist:1996p7456}.}

We use Figure \ref{fig:basic} to illustrate the basic ideas of causal diagrams and how they can be used to facilitate causal inference.
Figure \ref{fig:basic} depicts three variants of a simple causal graph with three variables that are all of assumed to be observable.
In each case, we are interested in estimating the causal effect of $X$ on $Y$ in the presence of a third variable, $Z$ that is related to $X$ and $Y$ in some fashion.
The only difference between the three graphs is in the direction of arrows linking either $X$ and $Z$ or $Y$ and $Z$.
The boxes (or ``nodes") represent random variables and the arrows (or ``edges") connecting boxes represent hypothesized causal relations, with each arrow pointing from a cause to a variable assumed to be affected by it.\footnote{
That arrows have a direction accounts for the ``D" in DAG (Directed Acyclic Graph). 
The acyclic (``A") component means that there must be no cycles in the graph. 
Cycles cause obvious problems in causal reasoning.
 An example would be $X \rightarrow Y \rightarrow Z \rightarrow X$. 
In this graph there is no ultimate cause. 
Graphs make a distinction between observed and unobserved random variables.
In some cases, an unobserved joint determinant of two random variables will not be represented explicitly, but replaced by a dashed, ``undirected" edge between those two random variables.}

The criterion developed by \citet{Pearl:2009vo} implies that very different conditioning strategies are needed for each of the causal diagrams (see Appendix \ref{append} for a more formal treatment). 
\citet{Pearl:2009vo} shows that, if we are interested in assessing the causal effect of $X$ on $Y$, we may be able to do so by conditioning on a set of variables, $Z$, that satisfies what \citet{Pearl:2009vo} labels the ``back-door criterion" \citep[p.79]{Pearl:2009vo}.\footnote{
Intuitively, the back-door criterion requires that $Z$ blocks (and does not open) ``back-door" paths.
A back-door path can be thought of as a way for $X$ to be associated with $Y$ due to associations with other variables rather than causal links from $X$ to $Y$.
See Appendix \ref{append} for a more formal discussion.}
While conditioning is much like the standard notion of ``controlling for" variables by including them as additional regressors in OLS regression, there are critical differences.
First, reflecting the non-parametric nature of causal diagrams in their most general form, conditioning in principle means estimating effects for each distinct level of the set of variables in $Z$.
Second, as we discuss below, the inclusion of a variable in $Z$ may actually result in biased estimates of causal effects.
We next apply these criteria to each of the three figures in Figure \ref{fig:basic}.	

Figure \ref{fig:confound} is straightforward. 
Here it is apparent that we need to condition on $Z$ and, if we do so, we can estimate the causal effect of $X$ on $Y$.
This situation is a generalization of a linear model in which $Y = X \beta + Z \gamma + \epsilon_Y$ and $\epsilon_Y$ is independent of $X$ and $Z$, but $X$ and $Z$ are correlated.
In this case, it is well known that omission of $Z$ would result in a biased estimate of $\beta$, the causal effect of $X$ on $Y$, but by including $Z$ in the regression, we get an unbiased estimate of $\beta$.\footnote{
Inclusion of $Z$ blocks the back-door path from $Y$ to $X$ via $Z$.}
In this situation, $Z$ is called a \emph{confounder}.

Figure \ref{fig:mech} is a bit different. Here we have $Z$ acting as a \emph{mediator} of the effect of $X$ on $Y$.
No conditioning is required in this setting to obtain an unbiased estimate of the effect of $X$ on $Y$.
But, the back-door criterion not only implies that we need not condition on $Z$ to obtain an unbiased estimate of the causal effect of $X$ on $Y$, but that we should not condition of $Z$ to get such an estimate.

Finally in Figure \ref{fig:collider}, we have $Z$ acting as what is referred to as a ``collider" variable \citep{Glymour:2008aa,Pearl:2009kh}.\footnote{
The two arrows from $X$ and $Y$ ``collide" in $Z$.} 
The back-door criterion not only implies that we need not condition on $Z$, but that we \emph{should} not condition of $Z$ to get an unbiased estimate of the causal effect of $X$ on $Y$.
While in epidemiology, the issue of ``collider bias \dots can be just as severe as confounding" \citep[p.\,186]{Glymour:2008aa}, it appears to receive less attention in accounting research than confounding.\footnote{
Many intuitive examples of collider bias involve selection or stratification.
Admission to a college could be a function of combined test scores and interview performance exceeding a threshold, i.e., $T + I \geq C$. Even if $T$ and $I$ are unrelated unconditionally, a regression of $T$ on $I$ conditioned on admission to college is likely to show a negative relation between these two variables.}

\subsubsection{Causal diagrams: Applications in accounting}
A typical paper in accounting research will include many variables  to ``control for" potential confounding of causal effects.
But why many of these variables should be considered confounders, in which case they should be controlled for, rather than mediators or colliders, in which case ``controlling for" these variables may lead to bias, is often unclear.

One paper that does discuss this distinction is \citet{Larcker:2007aa}, who use a multiple regression (or logistic) model of the form:\footnote{We alter the mathematical notation of  \citet{Larcker:2007aa} to conform with notation we use here.}
\begin{equation}
Y = \alpha + \sum_{r \in R} \gamma _r Z_r + \sum_{s \in S} \beta_s X_s + \epsilon \label{eqn:lrt1}
\end{equation}

\citet{Larcker:2007aa} suggest that 
\begin{quote}
``One important feature in the structure of Equation \ref{eqn:lrt1} is that the governance factors [$X$] are assumed to have no impact on the controls (and thus no indirect impact on the dependent variable). 
As a result, this structure may result in conservative estimates for the impact of governance on the dependent variable. Another approach is to only include governance factors as independent variables, or:
\begin{equation}
Y = \alpha + \sum_{s \in S} \beta_s X_s + \epsilon \label{eqn:lrt2}
\end{equation}
The structure in Equation \ref{eqn:lrt2} would be appropriate if governance impacts the control variables and both the governance and control variables impact the dependent variable (i.e., the estimated regression coefficients for the governance variables will capture the total effect or the sum of the direct effect and the indirect effect through the controls).''
\end{quote}

But there are some subtle issues here.
If some elements of $Z_r$ are mediators and others are confounders, then both equations will be subject to bias. 
Equation \ref{eqn:lrt2} will be biased due to omission of confounders, while Equation \ref{eqn:lrt1}  will be biased due to inclusion of mediating variables.
Additionally, the claim that the estimates are ``conservative" is only correct if the indirect effect via mediators is of the same sign as the direct (i.e., unmediated) effect. 
If this is not the case, then the relation between the magnitude (and even the sign) of the direct effect and the indirect effect is unclear.

Additionally, this discussion does not allow for the possibility of colliders.
For example, governance plausibly affects leverage choices, while performance is also likely to affect leverage.
If so, ``controlling for" leverage might induce associations between governance and performance even absent a true relation between these variables.\footnote{
Note that \citet{Larcker:2007aa} do not in fact use leverage as a control when performance is a dependent variable.}
%For example, \citet{Cadman:2014cr} study, \emph{inter alia}, the effect of being a venture capital (VC)-backed firm on CEO incentive horizons after IPO. In their analysis they include as controls variables such as \emph{Toptier Underwriter} and \emph{R\&D/Assets}.\footnote{See Table 5 of \citet{Cadman:2014cr}.} But, given the timing of events, being VC-backed affecting the choice of underwriter is the most plausible causal relation between these two variables. Also, given that \citet{Cadman:2014cr} include pre-IPO observations, it is plausible that the CEO's incentive horizons would affect variables such as \emph{Toptier Underwriter} and \emph{R\&D/Assets}. If these assertions are correct,  \emph{Toptier Underwriter} and \emph{R\&D/Assets} are colliders. While these variables may have little impact on the results of \citet{Cadman:2014cr}, we argue that more discussion about why researchers include controls is warranted.
While the with-and-without-controls approach used by \citet{Larcker:2007aa} has intuitive appeal, a more robust approach would involve careful thinking about the plausible causal relations between the treatment variables, the outcomes of interest, and the candidate control variables.

\section{Quasi-experimental methods in accounting research} \label{sec:quasi}
While most studies in accounting use methods of conditioning on confounding variables in some kind of regression or matching framework, a number of studies use quasi-experimental methods that rely on ``as if" random assignment to identify causal effects \citep{Dunning:2012tt}.
Of the 91 papers in accounting research in 2014 seeking to draw causal inference from observational data, we identify 14 that use quasi-experimental methods for inferences. Despite the low count, we believe that papers using these methods are considered stronger research contributions and there seems a clear trend toward the use of quasi-experimental approaches.
% Do we want " are pressures from editors and reviewers to incorporate quasi-experimental approaches into research designs."?
In this section, we discuss and evaluate the use of these methods in accounting research.

\subsection{Natural experiments}
Natural experiments occur when observations are assigned by nature (or some other force outside the control of the researcher) to treatment and control groups in a way that is random or ``as if'' random \citep{Dunning:2012tt}. 
Truly (as if) random assignment to treatment and control provides a sound basis for causal inference, enhancing the appeal of natural experiments for social science research.
However, \citet[\,p.3, emphasis added]{Dunning:2012tt} argues that this appeal ``may provoke \emph{conceptual stretching}, in which an attractive label is applied to research designs that only implausibly meet the definitional features of the method.'' 

Our survey of accounting research in 2014 identified five papers that exploited either a ``natural experiment'' or a ``exogenous shock'' to identify causal effects.\footnote{These are \citet{Lo:2013jk,Aier:2014ii,Kirk:2014gx,Houston:2014hv} and \citet{Hail:2014fq}.}
An examination of these papers reveals how difficult it is to find a clear natural experiment in observational data.

The most important concern is that that most "exogenous shocks" (e.g., SEC regulatory changes or Delaware court ruling) generally do not sort firms into treatment and control groups based on a random assignment. For example, an early version of Dodd-Frank contained a provision that would force companies to remove a staggered board structure. It is tempting to use this event to assess the valuation consequences of having a staggered board by looking at excess returns for firms with and without a staggered board around the announcement of this Dodd-Frank provision.  Although potentially interesting, this "natural experiment" does not randomly assign firms to treatment and control groups regarding a staggered board.  That is, firms made an endogenous choice about staggered boards and the regulation is potentially forcing firms to change their choice.  This feature substantially limits the ability to use the excess return results to produce causal inferences about staggered boards. The results may reveal something about the costs and benefits confronting a firms making a board structure decision, but it does not tell us much about the causal valuation consequences of selecting a staggered board.  

Another important concern is that the the exogenous shock should not only be random, but should only affect the of interest through its effect on the treatment. This is similar to the necessary assumptions for a valid instrumental variable.  It is important to develop and present causal diagrams that show the basis of the causal claims and why there are unlikely to be alternative back-door paths that confound inferences. 

Finally, it is important to carefully consider the choice of explanatory variables in studies that rely on natural experiments. In particular, researchers sometimes inadvertently use covariates that are affected by the treatment in their analysis.  As noted by \cite{Imbens:2015aa}, using a post-treatment variable as a covariate severely limits making any causal inference.

%First, some studies using exogenous shocks plausibly suffer from issues of confounding, as treatment assignment is non-random. For example, \citet{Hail:2014fq} study the effect of the ``exogenous shocks" of mandatory IFRS adoption and enforcement of insider trading laws on firms' dividend payments. While these shocks sort firms into treatment and control groups, they clearly do not do so randomly, as they apply to firms in specific countries and a variety of time-varying country-level effects plausibly exist.\footnote{According to Table 1 of \citet{Hail:2014fq}, about 60\% of treatment firms are European firms that adopted IFRS in 2005. Figure 1, Panel A of \citet{Hail:2014fq} suggests that most of the impact of IFRS adoption occurs three years after adoption; i.e., for European firms in 2008, when there may have been other reasons for reducing dividends that applied to those firms more than controls.}

%Second, because ``exogenous shocks" often do not directly sort firms into treatment and control groups, they rely on assumptions analogous to those required for instrumental variables. That is, the exogenous shock should not only be random, but should only affect the outcome through its effect on the treatment.\footnote{In some cases, if the necessary assumptions apply, it would be more appropriate to use instrumental variable methods to estimate causal effects.In other cases, \citep[e.g.][]{Aier:2014ii}, the treatment of interest is unobserved, making such an approach unfeasible.} For example, \cite{Aier:2014ii} exploit a 1991 Delaware court ruling as a ``natural experiment'' for the purpose of understanding the causal effect of debtholders' demand for conservatism (the treatment variable) on financial reporting conservatism (the outcome of interest).\footnote{The court ruling ``expanded the scope of directors' fiduciary duties to include creditors when a Delaware incorporated firm is in the `vicinity of insolvency.'"} For the court ruling to be a valid instrument for debtholders' demand for conservatism, it must only affect the outcome through its effect on the treatment of interest.But if the 1991 Delaware court ruling  ruling in question caused directors to dispose of assets leading to recognition of losses and affecting measures of conservatism, as it plausibly did, then the identification strategy is invalid.\footnote{Similar issues plausibly affect \citet{Houston:2014hv}, which uses the ``exogenous shock" of the 2008 financial crisis and the ``natural experiment" of midterm elections to study ``whether the political connections of listed firms in the United States affect the cost and terms of loan contracts,'' and \citet{Kirk:2014gx}, who ``exploit the natural experiment setting created by the exogenous shock of Reg FD" to examine ``the effect of investments in internal investor relations (IR) departments on firm outcomes."} 
% Random assignment is also dubious here.

%The evidence from research published in 2014 suggests that accounting researchers apply the term ``natural experiment" to circumstances where it is not clear that it applies. While ``exogenous shocks" may provide interesting settings for research, if random assignment does not apply, then researchers should exercise caution in giving causal explanations to associations observed in the data. Readers should be alert to the fact that terms like ``exogenous shock" and ``natural experiment" are often used when ``as if" random assignment is not plausible.

Extending our survey beyond research published in 2014, we find papers with very credible natural experiments.
One such paper is \citet{Michels:2015aa}, who exploits the difference in disclosure requirements for significant events that occur before financial statements are issued that differ according to whether the event occurs before or after the balance sheet date.
He finds evidence that the market reacts more strongly to recognized events.
Moreover, as recognized by \citet{Michels:2015aa}, there are possibly different materiality criteria that affect the relation been underlying events and the disclosures he relies on, and \citet{Michels:2015aa} takes care to address this concern.

Another credible natural experiment is examined in \citet[p.\,80]{Li:2015he}, who study the experiment whereby the SEC ``mandated temporary suspension of short-sale price tests for a set of randomly selected pilot stocks." \citet[p.\,79]{Li:2015he} conjecture ``that managers respond to a positive exogenous shock to short selling pressure \dots by reducing the precision of bad news forecasts." Although a very plausible natural experiment, it is important assess whether exogenous shock also potentially affects the properties of the forecast (i.e., makes it endogenous).


%But if the exogenous shock affects the properties of the forecast (i.e., makes it endogenous), the ``natural experiment" aspect of the research design is undone by the decision to include such properties in the regression analysis.\footnote{\citet{Li:2015he} include the magnitude of the forecast surprise (\textit{MFSURP}) in their regressions (e.g., regressions in Table 2 where abnormal returns around the forecast is the dependent variable).
%See \citet[p.\,116]{Imbens:2015aa} for discussion of ``the dangers of using a post-treatment variable \dots as a covariate."}

If true natural experiments can be found, they are an excellent design for drawing causal inferences from observational data.  Unfortunately, real natural experiments are very rare Certainly researchers should exploit these natural experiments when they occur \citep[e.g.][]{Michels:2015aa,Li:2015he}, but care is needed in doing so.


\subsection{Instrumental variables}
\citet[p.114]{Angrist:2008vk} describe instrumental variables (IV) as ``the most powerful weapon in the arsenal of [statistical tools]" in econometrics. 
Accounting researchers have long used instrument variables to address concerns about endogeneity \citep{Larcker:2010fq} and continue to do so.
Our survey of research published in 2014 identifies 10 papers using instrumental variables.\footnote{
These are \citet{Cannon:2014im,Cohen:2014jl,Kim:2014fm,Vermeer:2014bs,Fox:2014io,Guedhami:2013cj,Houston:2014hv,deFranco:2014ct,Erkens:2014hj} and \citet{Correia:2014fp}.}
Much has been written on the challenges for researchers in using instrumental variables (IV) as the basis for causal inference \citep[e.g.,][]{Roberts:2013cz}, and it is useful to use this background to evaluate the application of this approach in accounting research. 

\subsubsection{Evaluating IVs requires careful theoretical causal (not statistical) reasoning}

With respect to accounting research, \citet{Larcker:2010fq} lament that ``some researchers consider the choice of instrumental variables to be a purely statistical exercise with little real economic foundation'' and call for 
``accounting researchers \dots to be much more rigorous in selecting and justifying their instrumental variables.'' 
\citet[p.117]{Angrist:2008vk} argue that ``good instruments come from a combination of institutional knowledge and ideas about the process determining the variable of interest."
One study that illustrates this is \citet{Angrist:1990dk}.
In that setting, the draft lottery is well understood as random and the process of mapping from the lottery to draft eligibility is well understood.
Furthermore, there are good reasons to believe that the draft lottery does not affect anything else directly except for draft eligibility.

%\footnote {Of course, this seemingly "ideal" instrument has been subject to considerable criticism. I think the idea is that even if you had a low draft number it was not clear that you actually went to the army.  In fact, upper class kids did not go (and the probably had much better skills) than the white hillbilly trash and minorities that ended up going to VN}
%TODO: Get reference for criticism of Angrist instrument

Although somewhat subjective, we believe that many researchers in accounting view causal inference as a purely statistical exercise.
These is typically very little or no theoretical justification for the validity of variables that are selected as instruments.  It is generally difficult to believe that variables such as firm size, leverage, corporate governance, or director age are viable instrumental variables for typical studies.  Similarly, there are serious concerns about using lagged values of endogenous variables or industry averages of endogenous variables as instruments.  We do not believe that these largely arbitrary choices allow researchers to claim causal inference or that their "results are robust to controlling for endogenity."


% For example, to address endogeneity \citet{Cohen:2014jl} use ``two instrumental variables. The first is the natural log of industry size, measured as the number of companies within each two-digit SIC. The second measures industry competition using the Herfindahl-Hirschman index, which is well-established as a measure of competitive industries. Our untabulated results using this approach are qualitatively similar to our main analysis, thus indicating that endogeneity is not a concern when assessing the reliability of our findings.''\footnote{Three other studies used a similar approach. \citet{Vermeer:2014bs} ``use Maddala's (1988) two-stage procedure'' in order to ``control for endogeneity'' without providing any explanation at all and in fact seem to be assuming the each of three endogeneous variables can used as an instrument for the other two.\citet[p.48]{Fox:2014io} state in a footnote that they ``instrumented for the price index employing a two stage least squares estimator'' without further details, simply noting that their ``conclusions are robust with respect to these concerns.''\citet{Cannon:2014im} uses ``industry-level capacity unit cost and selling price changes'' as instruments for firm-level capacity unit cost changes with no more justification than the fact that these ``are outside management's control.'' But being outside management control does not make a variable an adequate instrument.}

%In most remaining cases, the reasoning in support of the validity of an instrument is evidently flawed. 

As an illustration, \citet{Kim:2014fm} examine the interesting research question of how outside directors affect firm performance. One of their key variables is director tenure which they acknowledge as being endogenous. They then use director age as an instrument for director tenure. However, their justification for this instrument seem instead to provide reasons to believe that it is not valid. 
``Importantly, research finds little or no association between age and performance \dots and a small negative association between age and executive functions \dots. 
Related to directors, Ferris et al. (2003) suggest that any positive effects from director experience increasing with age may be offset by older directors having less energy, posing a last-period risk, and viewing directorships as lucrative part-time jobs for their retirement years.'' 
But these arguments seem to invalidate age as an instrument for tenure. 
For age to be a valid instrument, there should be no unblocked causal path between age and performance except for the path via tenure.
That possible positive effects \emph{may} be offset by negative effects is not a valid basis for claiming age to be a valid instrument.  

%\footnote{We omit discussion of  \citet{Erkens:2014hj,Houston:2014hv} and \citet{deFranco:2014ct} for reasons of space. But in each case, the instruments have obvious flaws and no convincing arguments for their validity are offered (details available on request).}

\subsubsection{There are no simple (statistical) tests for the validity of instruments}
It is very common for accounting researchers to claim that they have tested for the validity of their instrumental variables (actually the exclusion restriction) using some type of statistical test.  Sometimes this claim is stated as 


Although perhaps obvious, the standard statistical tests applied by authors using instrumental variables provide little insight into the quality of the chosen instruments. 
% \citet{Guedhami:2013cj} use $\textit{CAPITAL}$, an indicator for a firm being located in a capital city, as an instrument for political connectivity in a study looking at the effect of political connections on the use of a Big 4 auditor ($\textit{BIG 4}$).
For example, the only justification \citet{Guedhami:2013cj} provide for their instrument is that ``importantly, the correlation between $\textit{CAPITAL}$ and $\textit{BIG 4}$ is small in our data set $(\rho = 0.05)$, helping to justify the validity of this exclusion restriction.''\footnote{
 \citet{Guedhami:2013cj} cite \citet{Larcker:2010fq} as a reference for this approach, even though \citet{Larcker:2010fq} carefully explain why simple tests like this cannot be used to justify instruments.}

%\citet{Correia:2014fp} is relatively thorough. \citet{Correia:2014fp} 
Even more sophisticated tests of weak instruments and tests of over-identifying restrictions are not obviously helpful, as can be demonstrated with a simple simulation exercise.
Suppose that we are interested in a model such as $y = X \beta + \epsilon$, but with $X$ and $\epsilon$ having correlation $\rho(X, \epsilon) > 0$ (i.e., $X$ is endogenous) and $\beta = 0$ (i.e., there is no causal relation between $X$ and $y$). 
Now, suppose we \emph{construct} the following three instruments 
$z_1 = x +\eta_1$, $z_2 = \eta_2$, and $z_3 = \eta_3$, with $\eta_1, \eta_2,  \eta_3 \sim N(0, \sigma_{\eta}^2)$ and independent. 
That is, $z_1$ is $X$ plus noise (e.g., industry averages or lagged values of $X$ would seem to approximate $z_1$), while $z_2$ and $z_3$ are random noise (many variables could be candidates here).\footnote{A paper using instruments with apparently similar properties is \citet{Correia:2014fp}. 
\citet{Correia:2014fp} uses ``average level of political contributions made by the other firms in the same industry'' as an instrument for political contributions by a firm, as well as two additional instruments: ``the percentage of sales made to the government, and the number of years in the previous five years in which there was a close election involving two candidates in the firm's state.''  \citet{Reiss:2007ej} suggest that there is no reason to view industry averages as valid instruments.}
Obviously, these ``instruments" are silly choices and completely inappropriate.

Assuming that $X$ and $\epsilon$ are bivariate-normally distributed with variance of $1$ and $\rho(X, \epsilon)=0.2$ and that $\sigma_{\eta}=0.03$, we run 1000 simulations and  estimate the IV regression using these instruments on the simulated data in each case.
Doing so, we find a mean estimated coefficient on $X$ of $0.201$, which is statistically significant at the 5\% level 100\% of the time.\footnote{Note that this coefficient is close to $\rho(X, \epsilon) = 0.2$, which is to be expected given how the data were generated.} 
Based on a test statistic of 30, which easily exceeds the thresholds suggested by \citet{Stock:2002aa}, the null hypothesis of weak instruments is rejected 100\% of the time. 
The test of overidentifying restrictions fails to reject a null hypothesis of valid instruments (at the 5\% level) 95.7\% of the time.

In other words, it is quite possible for completely spurious instruments to deliver bad inferences, yet easily pass tests for weak instruments and tests of overidentifying restrictions.
%\footnote{It is also common for accounting researchers to claim that they have established the validity of their instruments by implementing some type of Hausman test.  It is very clear that these types of overidentifying tests require the researcher to actually have one valid IV. 
In general, there is no test that enables a researcher to verify that their IVs satisfy the exclusion restriction.

\subsubsection{Causal diagrams and instruments}
To illustrate the application of causal diagrams to the evaluation of instrumental variables, we consider \citet{Armstrong:2013io}.
%
\citet{Armstrong:2013io} studies the effect of shareholder voting ($\textit{Shareholder support}_{t}$) on future executive compensation ($\textit{Compensation}_{t+1}$) .
Because of the plausible existence of unobserved confounding variables that affect both future compensation and shareholder support, a simple regression of $\textit{Compensation}_{t+1}$ on $\textit{Shareholder support}_{t}$ and controls would not allow \citet{Armstrong:2013io} to obtain an unbiased estimate of the causal relation.
Among other analyses, \citet{Armstrong:2013io} use an instrument variable to estimate the causal relation of interest.
\citet{Armstrong:2013io} claim that their instrument is valid based on reasoning that can be expressed as the  Figure \ref{fig:agl}.
By conditioning on $\textit{Compensation}_{t-1}$ and using ISS recommendations as an instrument, \citet{Armstrong:2013io} argue that they can identify a consistent estimate of the causal effect of shareholder voting on $\textit{Compensation}_{t+1}$, even though there is an unobserved confounder, namely determinants of future compensation observed by shareholders, but not the researcher.\footnote{
In Figure \ref{fig:agl}, we depict the unobservability of this variable (to the researcher) by putting it in a dashed box.
Note that we have omitted the controls included by \citet{Armstrong:2013io} for simplicity, though a good causal analysis would consider these carefully.}

However, a critical assumption for this analysis is, as the authors note, that the ``validity of this instrument depends on ISS recommendations not having an influence on future compensation decisions conditional on shareholder support (i.e., firms listen to their shareholders, with ISS having only an indirect impact on corporate policies through its influence on shareholders' voting decisions)" \citep[p.\,912]{Armstrong:2013io}.

In other words, the IV analysis in \citet[p.\,912]{Armstrong:2013io} requires that the causal diagram in Figure \ref{fig:agl} is correct and that there is no arrow from $\textit{ISS recommendation}_t$ to $\textit{Compensation}_{t+1}$. 
Unfortunately, this assumption seems inconsistent with the findings of \citet{Gow:2013aa}, who provide evidence that firms are carefully calibrating compensation plans (i.e., factors that directly affect $\textit{Compensation}_{t+1}$) to comply with the requirements of ISS's policies so as to get a favorable $\textit{ISS recommendation}_t$, implying a path from $\textit{ISS recommendation}_t$ to $\textit{Compensation}_{t+1}$ that does not pass through $\textit{Shareholder support}_{t}$.
Thus this new evidence suggests that the instrument of \citet[p.\,912]{Armstrong:2013io} is not credibly valid for the causal effect they seek to estimate.

% BBKL discussion will go between here ...

% ... and here. Don't edit between these two lines.

\subsubsection{IV in accounting research: An evaluation}
A review of IV in published research in accounting in 2014 suggests that researchers have paid little heed to the suggestions and warnings of  \citet{Larcker:2010fq} and \citet{Roberts:2013cz}.
We find no case of an instrumental variable analysis in our survey of accounting research that can withstand rigorous scrutiny.
This is perhaps not unsurprising, as plausible instruments have tended to rely on some explicit randomization, which is likely to be extremely rare in accounting research settings.
While IV is a classic textbook approach for credible causal inference, its applicability in actual research settings seems very limited and it seems unlikely that IV will provide a sound basis for causal inference in accounting research for the vast majority of research questions.
 
 % \citet{Houston:2014hv} use variables variables that are related to the location of the company's headquarters as instruments for political connection and argue that ``these instruments should not be conceptually related to loan spreads. The key insight here is that the geographic locations of headquarters for companies are predetermined and are unlikely to affect banks' financing decision on loan costs. In summary, our identification assumption is that the costs of bank loans are not directly related to the companies' geographic locations, after controlling for a series of firm and loan characteristics'' (p.228). In justifying the relevance of the instrument, the authors seem eager to justify a connection, suggesting that ``the presumption is that the company's geographic location affects the company's ability to attract politically connected directors.'' But it far from clear why a company's geographic location would not also affect the its ability to attract directors with connections to \emph{financial institutions}, which plausibly affects financing terms directly \citep{Guner:2008tp}.\footnote{\citet{Houston:2014hv} also use firm age as an instrument, arguing that ``firm age affects a firm's incentive and capability in building up political connections''; but it is not clear why firm age would not also affect a firm's ``incentive and capability in building up'' financial connections.} 
 
% Researchers tend to very unclear about the determinants of their selected instruments.  In many cases, it seems that the instruments are also endogenous.  This makes it very difficult to to rule out the possibility that the instrument directly affects (or is correlated with) variables other than the endogenous variable of interest.
 % \citet{Erkens:2014hj} ``use the following three instrumental variables that capture the extent to which lenders are more likely to serve on a firm's board, which is studied for its potential effect on accounting conservatism. We use \emph{Industry importance to primary lender} because industry specialization increases the importance of acquiring information about a firm's industry, \emph{Primary lender within 50 mile radius} because physical proximity to lenders' headquarters reduces the cost of serving on the board, and \emph{Number of commercial banks within 50 mile radius} because the close proximity of multiple banks increases competition for board seats from other lenders.'' If industry specialization affects information-acquisition incentives, it seems it would do so through channels outside of board membership. With respect to the second instrument, it's quite likely that proximity affects information-gathering independent of service on the board. With respect to the third instrument, it is also implausible that the only direct effect of this variable is one on the service of bankers on the board (for example, this may lead to lower search costs in choosing potential lenders).

 %\citet{deFranco:2014ct} ``find that the number of covenants is positively related to the interest rate, likely due to endogeneity between the interest rate and covenants.'' To address this using they use ``the number of covenants by calendar year indicators as the instrument'' for the number of covenants. Apart from the issues with using an average as an instrument discussed in \citet{Reiss:2007ej}, the authors justify their instrument by suggesting that ``the strictness of covenant packages significantly deteriorated during the years of the credit boom that preceded the financial crisis.'' But it seems likely that the credit boom would have a direct effect on interest rates on bond issues. 

\subsection{Regression discontinuity designs}
In discussing the recent ``flurry of research" using regression discontinuity (RD) designs, \citet[p.\,282]{Lee:2010hya} point out that they ``require seemingly mild assumptions compared to those needed for other nonexperimental approaches \dots and that causal inferences from RD designs are potentially more credible than those from typical `natural experiment' strategies."
% I would add a reference to the "first application" -- Thistlewaite, D.; Campbell, D. (1960). "Regression-Discontinuity Analysis: An alternative to the ex post facto experiment". Journal of Educational Psychology 51 (6): 309–317

%Imbens, G.; Lemieux, T. (2008). "Regression Discontinuity Designs: A Guide to Practice". Journal of Econometrics 142 (2): 615–635. 
Recently, RD designs have attracted the interest of accounting researchers, as a number of phenomena of interest to accounting researchers involve discontinuities. For example, whether an executive compensation plan is approved is a discontinuous function of shareholder support \citet{Armstrong:2013io} and whether a firm had to comply with provisions of Sarbanes-Oxley Act in 2004 \citep{Iliev:2010ic} is a discontinuous function of market float.

While RD designs make relatively mild assumptions, in practice these assumptions may be violated.
In particular, manipulation of the running variable (or the variable that determines whether an observation is assigned to a treatment)  may occur.
%TODO: Add discussion of Listokin, McCrary, etc.  
Another issue with RD designs is that the causal effect estimated is a local estimate (i.e., it relates to observations close to the discontinuity.
This effect may be very different from the effect at points away from the discontinuity.
For example, in designating a public float of \$75 million, the SEC may have reasoned that at that point the benefits of Sarbanes-Oxley, which may have been increasing in firm size, were approximately equal to the approximately fixed costs of complying with the law.
If true, we would expect to see an estimate of approximately zero effect, even if there benefits of the law for shareholders of firms well about is positive.
Similarly, a vote that receives approximately 50\% support may reflect the fact that costs and benefits are approximately balanced, while measures that receive much greater support may have very different levels of benefits.

It is also important to note that so-called ``quasi-RD" designs have only a superficial resemblance to RD designs.
For example, as he cannot observe ``the specific covenant thresholds in [his] primary dataset," \citet{Tan:2013ce} is constrained to estimate a ``quasi-RD" design like that estimated in \citet{Roberts:2009ka}.
But this ``regression discontinuity design" is essentially ordinary-least squares with an indicator for covenant violation and thus does not represent a method for estimating unbiased causal effects with observational data.
%TODO: Add discussion of Ertimur, Ferri and Oesch (2014).

%TODO: Seems like we want to include in this section:  plot the data and if you can't see it in the data, it probably is not actually there (Imbens), do not use the high level polynomial approach, and other similar issues.  Probably something on whether the magnitude of the results is actually believable -- Yonca's, the prize winning JF paper, and others results seem implausibly large for governance topics.

\subsection{Quasi-experimental methods: An evaluation}
We agree that the revolution in econometric methods for causal inference has been an exciting development.
However, we have serious concerns regarding the value of these methods in accounting research. 
First, it is evident that accounting researchers often apply these methods poorly and inappropriately.
Second, it is far from clear that these methods, properly applied, can support more than a small fraction of accounting research.

\section{Mechanisms and causal inference} \label{sec:mech}

\begin{quotation}
\begin{singlespace} 
	\addtolength{\leftmargin}{.25in}
	\addtolength{\rightmargin}{.25in}
In complex fields like the social sciences and epidemiology, there are only few (if any) real life situations where we can make enough compelling assumptions that would lead to identification of causal effects.
\attrib{Judea Pearl, cited in \citealt[p.\,287]{Freedman:2004ix}}
\end{singlespace}
\end{quotation}

In the first half of the paper, we have argued that, while causal inference is the goal of most accounting research using observational data, research designs that yield output that can be viewed as unbiased estimates of causal effects using such data likely do not exist most research settings.
So, what should researchers do? Do we stop doing research? Do we need to give up on causal inference? 
We believe that this is too pessimistic and that there are viable paths forward that do not rely on researchers identifying ``clever" identification strategies to answer questions of interest.
The objective of the second part of this paper is to discuss these paths forward.
The first path we discuss is an increased focus on causal mechanisms.
%TODO: Define the term "mechanism"

\subsection{Causal mechanisms: Some examples}
Accounting research is not alone in relying primarily on observational data.
Other fields also seek to draw causal inferences, but need to grapple with the reality of observational data. 
Yet in many cases, these fields have successfully drawn causal inferences.
In the following, we briefly discuss case studies of plausible causal inference in other fields and highlight features that enhanced the credibility of inference.

\subsubsection{John Snow and cholera}
A widely cited case of causal inference involves John Snow's work on cholera.
As there are many excellent accounts of Snow's work, we will focus on the barest details.
As discussed in  \citet[p.\,339]{Freedman:2009ur}
``John Snow was a physician in Victorian London.
 In 1854, he demonstrated that cholera was an infectious disease, which could be prevented by cleaning up the water supply. 
The demonstration took advantage of a natural experiment.
 A large area of London was served by two water companies. 
 The Southwark and Vauxhall company distributed contaminated water, and households served by it had a death rate`between eight and nine times as great as in the houses supplied by the Lambeth company, ' which supplied relatively pure water."

But there was much more to Snow's work than the use of a convenient natural experiment.
First, Snow's reasoning (much of which was surely done before ``the arduous task of data collection" began) was about the  mechanism through which cholera spread. Existing theory suggested ``odors generated by decaying organic material."
Snow reasoned qualitatively that such a mechanism was implausible.
Instead, drawing on his medical knowledge and the facts at hand, Snow conjectured that ``A living organism enters the body, as a contaminant of water or food, multiplies in the body, and creates the symptoms of the disease. Many copies of the organism are expelled with the dejecta, contaminate water or food, then infect other victims" \citep[p.\,342]{Freedman:2009ur}.
With a hypothesis at hand, Snow then needed to collect data to prove it.
His data collection involved a house-to-house survey in the area surrounding the Broad Street pump operated by  Southwark and Vauxhall.
As part of his data collection, Snow needed to account for anomalous cases (such as the brewery workers who drank beer, not water).
It is important to note that this qualitative reasoning and diligent data collection were critical elements establishing (to a modern reader) the ``as if" random nature of the treatment assignment mechanism provided by the Broad Street pump.
This contrasts with the speculative guesses often used to justify natural experiments by modern researchers.

But another important feature of the case of John Snow and cholera is that widespread acceptance of Snow's hypothesis did not occur until compelling evidence of the mechanism was provided.
``However, widespread acceptance was achieved only when Robert Koch isolated the causal agent (\emph{Vibrio cholerae}, a comma-shaped bacillus) during the Indian epidemic of 1883"  \citep[p.\,342]{Freedman:2009ur}.
Only once persuasive evidence of a plausible mechanism was provided (i.e., direct observation of microorganisms now known to cause the disease) did Snow's ideas become widely accepted.

\subsubsection{Smoking and heart disease}
A more recent illustration of plausible causal inference is discussed by \citet{Gillies2011-GILTRT-3}.
\citet{Gillies2011-GILTRT-3} points discusses the paper by \citet{Doll:1976aa}, which studies the mortality rates of male doctors between 1951 and 1971.
The data that \citet{Doll:1976aa} had showed ``a striking correlation between smoking and lung cancer" \citep[p.\,111]{Gillies2011-GILTRT-3}.
\citet{Gillies2011-GILTRT-3} argues that ``this correlation was accepted at the time by most researchers (if not quite all!) as establishing a causal link between smoking and lung cancer. Indeed Doll and Peto themselves say explicitly (p.\,1535) that the excess mortality from cancer of the lung in cigarette smokers is caused by cigarette smoking."
In contrast, while \citet{Doll:1976aa} also had highly statistically significant evidence of an association between smoking and heart disease, they were cautious about drawing inferences of a direct causal explanation for the association.
\citet[p.\,1528]{Doll:1976aa} say ``To say that these conditions were related to smoking does not necessarily imply that smoking caused \dots them. The relation may have been secondary in that smoking was associated with some other factor, such as alcohol consumption or a feature of the personality, that caused the disease.''
 
\citet{Gillies2011-GILTRT-3} then discusses extensive research into atherosclerosis between 1979 and 1989 and concludes that ``by the end of the 1980s, it was established that the oxidation of LDL was an important step in the process which led to atherosclerotic plaques."
Later research established evidence of much higher levels of a new measure (levels of $F_2$-isoprostanes in blood samples) of the relevant oxidation in the body ``provides compelling evidence that smoking causes oxidative modification of biologic components in humans. 
\shortcites{Morrow:1995gz}
This conclusion is greatly strengthened by the finding that levels of $F_2$-isoprostanes in the smokers fell significantly after two weeks of abstinence from smoking" \citep[pp.\,1201--2]{Morrow:1995gz}.  
\citet[p.\,120]{Gillies2011-GILTRT-3} points out that this evidence did not establish a confirmed mechanism linking smoking with heart disease, because the required oxidation needs to occur in the artery wall, not in the blood stream. 
But later research established a link: ``Smoking produced oxidative stress. 
This increased the adhesion of leukocytes to the \dots artery, which in turn accelerated the formation of atherosclerotic plaques" \citep[p.\,123]{Gillies2011-GILTRT-3}.
Thus, a causal link or mechanism between smoking and atherosclerosis was established.
%TODO: Get an example from the social sciences, e.g., political science.

\subsubsection{Implications of cases on mechanism}
 \citet{Gillies2011-GILTRT-3} avers that the process by which a causal link between smoking and atherosclerosis was established illustrates the ``Russo-Williamson thesis."
 \citet[p.\,159]{Russo:2007iz} suggest that ``mechanisms allow us to generalize a causal relation: while an appropriate dependence in the sample data can warrant a causal claim `$C$ causes $E$ in the sample population,' a plausible mechanism or theoretical connection is required to warrant the more general claim `$C$ causes $E$.' Conversely, mechanisms also impose negative constraints: if there is no plausible mechanism from $C$ to $E$, then any correlation is likely to be spurious. Thus mechanisms can be used to differentiate between causal models that are underdetermined by probabilistic evidence alone."

The Russo-Williamson thesis was arguably also at work in the case of Snow and cholera, where the establishment of a mechanism (\emph{Vibrio cholerae}) was essential before the causal explanation offered by Snow was widely accepted and also in the case of smoking and lung cancer, which was initially conjectured based on associational evidence, but was widely accepted by 1976.\footnote{
The persuasive force of Snow's natural experiment, coming decades before the work of Neyman and Fisher, might be considered greater today.}

Our view is that accounting researchers can learn from fields such as epidemiology and political science. 
These fields grapple with the reality of observational data.
While randomized controlled trials are a gold standard of sorts in epidemiology, in many cases it is unfeasible or unethical to use such trials.
And in political science, it is not possible to randomly assign countries to treatment conditions such as \emph{democracy} or \emph{socialism}.
Yet these fields have often been able to draw plausible causal inferences by establishing clear mechanisms, or causal pathways, from putative causes to putative effects.
%TODO: Get better examples from fields other than epidemiology.

We argue that the value of the study of mechanisms is perhaps underestimated, perhaps due to a belief that the path to credible causal inferences involves clever identification strategies.

One paper that has a fairly compelling identification strategy is \citet{Brown:2015ik}, which examines ``the influence of mobile communication on local information flow and local investor activity using the enforcement of state-wide distracted driving restrictions."
The authors find that ``these restrictions \dots inhibit local information flow and \dots the market activity of stocks headquartered in enforcement states."
\citet[p.\,9]{Miller:2015ec} suggest that ``given the authors' setting and research design, it is difficult to imagine a story under which the types of reverse causality or correlated omitted variables explanations that we normally worry about in disclosure research are at play."\footnote{
One issue with the study is the fact the authors do not adjust their standard errors for the well-known cross-sectional dependence in the dependent variable they focus on, trading volume, in addition to the time-series dependence they do adjust for.}
However, notwithstanding the apparent robustness of the research design, it seems that the results would be more compelling with detailed evidence of a causal mechanism through which the estimated  effect occurs.
For example, if evidence were provided of trading activity by local investors while driving prior to, but not after, the implementation of distracted driving restrictions, this would seem to quite persuasive even incremental to a compelling identification strategy.\footnote{
Note that the authors disclaim reliance on trading while driving: ``our conjectures do not depend on the presumption that local investors are driving when they execute stock trades \dots [as] we expect such behavior to be uncommon."
However, even if not \emph{necessary}, given the small effect size documented in the paper (approximately 1\% decrease in volume), a small amount of such activity could be \emph{sufficient} to account for their results.}

As another example, many published papers have suggested that managers adopt conditional conservatism as a reporting strategy to obtain benefits such as reduced debt costs \citep{Ahmed:2002aa,Zhang:2008bc}.
But, as \citet[p\,317]{Beyer:2010cj} point out, an ex ante commitment to such a reporting strategy ``requires a mechanism that allows managers to credibly commit to withholding good news or to commit to an accounting information system that implements a higher degree of verification for gains than for losses," yet research has only recently begun to focus on the mechanisms through which such commitments are made \citep[e.g.,][]{Erkens:2014hj}.

Better understanding of mechanisms also allows researchers to identify gaps in research based on archival data and verbal theorizing.
\cite{Soltes:2014gr} provides an insightful discussion of the pitfalls from exclusive reliance on archival data. 

%If a plausible causal mechanism for their empirical research question (using theory and/or institutional observation), the next step in the research process is to assess the ability of this mechanism to explain real-world data. 
% As we have argued above, the traditional quasi-experimental methods used by accounting researchers have a number of serious limitations for conducting this type of data analysis exercise. 
% As an alternative, structural modeling methods that are becoming popular in economics (especially industrial organization work) and marketing represent an approach that might be used to provide an improved understanding of causal mechanisms for many accounting research questions.

% Structural material here
\input{structural}

\section{Descriptive studies} \label{sec:desc}

Accounting is essentially an applied discipline and it would seem that most empirical research studies should be solidly grounded in the details of how institutions operate.
Unfortunately, there are very few studies published in top accounting journals that focus on providing deep description of institutions relevant to accounting research settings.
Part of this likely reflects the perception that research that pursues causal questions (i.e., tests of theories) is more highly prized and thus more likely to be published in top accounting journals.\footnote{
The \emph{Journal of Accounting Research} used to publish some papers in a section entitled ``Capsules and Comments."
The editor at the time (Nicholas Dopuch) would seem to place papers into this section if ``did not fit" as a main article, but examined new institutional data or ideas. 
Such a journal section might have provided a credible signal of a willingness to publish descriptive studies of institutionally interesting settings.}

We believe that accounting research could benefit substantially from more in-depth descriptive research.
As we discuss below, this type of research is essential for those who seek to develop structural models or improve our understanding of causal mechanisms.\footnote{
There are many ``classic" descriptive studies that have had a major impact on subsequent theoretical and empirical research in organizational behavior and strategy such as \citep{Cyert:1956fd,Bower:1986vd,Mintzberg1973nature}.
\citet{Cyert:1956fd} argue that ``a realistic description and theory of the decision-making process are of central importance to business administration and organization theory. Moreover, it is extremely doubtful whether \dots economics does in fact provide a realistic account of decision-making in large organizations operating in a complex world."}

One reason to value descriptive research is that it can uncover structures and mechanisms that exist, but which would be exceedingly difficult to arrive at from basic economic theory.
For example, using proxy statements, \citet{Healy:1985jg} studies the bonus contracts of 94 large US companies and identifies a common structure of these bonus plans, including the existence of caps and floors \citep[p.\,89]{Healy:1985jg}. The paper also suggests hypotheses worth investigating regarding the effects of these plan features on accounting decisions.
It seems highly unlikely that a model derived from fundamental economic theory would arrive at these plan features actually used by firms.
These institutional features can be used to identify precise mechanisms and also as elements of structural models in which other features might be motivated more directly by economic theory.

Recent published research suggests that increased recognition of the value of descriptive research.
\citet{Soltes:2013ba} examines the interactions between sell-side analysts and company management in one firm that granted him proprietary access to ``offer insights into which analysts privately meet with management, when analysts privately interact with management, and why these interactions occur."  
By comparing private interaction to observed interaction between analysts and managers on conference calls and highlighting that private interaction with management is an important communication channel for analysts, \citet{Soltes:2013ba} provides a plausible mechanism through which information transfers hypothesized in more traditional empirical papers actually occur.

That private communication with management is an important source of information is confirmed by  \citet{Brown:2015kd}. \citet{Brown:2015kd} survey and interview financial analysts to understand how they think about a variety of issues. 
Their findings suggest that analysts' views on earnings quality differ from those researchers focus on. 
For instance, analysts do not use the ``red flags" used by academics to identify manipulation; and analysts generally are not attempting to uncover manipulation and use forecasts, not as ends in themselves, but figure out the stock price target.
These insights should shape research seeking to develop hypotheses and models of accounting information and analyst behavior.

Other fields provide interesting examples likely to be of interest to accounting researchers.
For example, \citet{Ahern:2014id} examines 183 illegal insider networks using primary source documents from the SEC, DOJ, and various public records. 
It provides rich insights into investor networks and it suggests questions for future work.
For example, network relationships can be divided into familial (23\%), business-related (35\%), friendships (35\%), or ``not clear" (21\%).
Insiders are more likely to be an accountant or lawyer, less likely to be a Democrat, and more likely to have a ``criminal record."
These results provide new institutional insights have have the potential to identify causal mechanisms regarding information transfer and disclosure.
In economics, \citet{Bloom:2007ed} provide a descriptive study of 732 medium-sized firms to assess whether management practices, such as lean manufacturing and use of incentives, are related to productivity. These descriptive results have lead to the development of theoretical models of innovating and productivity along with increasingly sophisticated empirical studies.

% Although it is a matter of taste, a number of the papers in our review seem to ask research questions and use causal mechanisms that are far removed from real world issues.

%Select a topic or setting of real interest to accounting researchers - analysts, loan officers, executives making strategic decisions, compensation committee members, etc. 
% How do managers and boards make decisions on ``disclosure quality" (an aside - do they even know what disclosure quality means and do they care?  Does this construct have any real relevance in the real world?)  

% You need to go out and actually interview these people using structured and semi-structured interviews.  Not just one per company, but several at varying levels.
% Need to understand the setting, economics aspects, behavioral aspects.
% Map out the mechanism by which decisions of interest are actually made.
% Maybe there are multiple mechanisms depending on the situation.
% What are the contextual variables?

%Use these qualitative insights to justify research question selection.  Would conditional conservatism show up in conversations with real world managers?  If you had an unstructured conversation, what accounting topics would be ``top of mind" for managers, board members, bankers, etc.?
%
%No doubt we would find many cases where present accounting research substantially departs from known institutional details - do executive really behave like Black-Scholes would imply?
%How are compensation plans really designed and why?  Why don’t companies take advantage of ``academically obvious" tax changes?
%
%Ultimately uUse these mechanisms to develop structural models with a plausible causal mechanism.
%
%Punchline: if you want to explain something observed, maybe it is a good idea to understand the phenomenon of interest first.

%TODO: somewhere we want to note that there are some real field experiments -- John Roberts with the impact of consulting firms in India and a variety of related studies in Economics.  
%TODO: There was also the SEC decision to randomly assign short selling restrictions.

\section{Concluding remarks} \label{sec:conclude}
In this paper, we examined the approaches used by accounting researchers to draw causal inferences from analyses of observational (or non-experimental) data. 
The vast majority of empirical papers using such data seek to draw causal inferences, notwithstanding the well-known difficulties with doing so.
While some papers seek to use quasi-experimental methods to develop unbiased estimates of causal effects, we find that the assumptions required to deliver such estimates
are not often credible. We believe that clearer communication of research questions and design choices would help researchers avoid some of the conceptual traps that affect 
accounting research. One tool that may help in this regard are causal diagrams.

We also argued that  accounting research could benefit from a more complete understanding of causal pathways through the use of rigorous theory. 
In particular, we believe that structural models will see greater use in the coming years. Finally we see great value to in-depth descriptive studies that inform 
causal issues and deepen our knowledge of the behavior and institutions we seek to model. Although our suggestions do not completely resolve controversies 
surrounding causal inferences drawn from observational data, we believe they offer a viable and exciting path forward.

\end{doublespace}

\clearpage
\bibliography{jar_methods}

\clearpage

\input{causal_graphs}

\clearpage
\include{tables}

\appendix
\section{Causal diagrams: Formalities} \label{append}
 
 \subsection{Definitions and a result}
 We first introduce some basic definitions and a key result.
 
\begin{definition}[$d$-separation, block, collider]
A path $p$ is said to be \emph{$d$-separated} (or \emph{blocked}) by a set of nodes $Z$ if and only if
\begin{enumerate}
	\item $p$ contains a chain $i \rightarrow m \rightarrow j$ or a fork $i \leftarrow m \rightarrow j$ such that the middle node $m$ is in $Z$, or
	\item $p$ contains an inverted fork (or \emph{collider}) $i \rightarrow m \leftarrow j$ such that the middle node $m$ is not in $Z$ and such that no descendant of $m$ is in $Z$
\end{enumerate}
\end{definition}

\begin{definition}[Back-door criterion]
A set of variables $Z$ satisfies the \emph{back-door criterion} relative to a an ordered pair of variables $(X, Y)$ in a 
	DAG $G$ if:
	\begin{itemize}
		\item no node in $Z$ is a descendant of $X$; and
		\item $Z$ blocks every path between $X$ and $Y$ that contains an arrow into $X$.\footnote{The ``arrow into $X$" is the portion of the definition that is explains the ``back-door" terminology.}
	\end{itemize}
\end{definition}%
Given this criterion, \citet[p.\,79]{Pearl:2009vo} proves the following result.
%
\begin{theorem}[Back-door adjustment]
	If a set of variables $Z$ satisfies the back-door criterion relative to $(X, Y)$, then the causal effect of $X$ on $Y$ is identifiable and is given by the formula 
	\[ P(y | x) = \sum_{z} P(y | x, z) P(z), \]
where $P(y|x)$ stands for the probability that $Y = y$, given that $X$ is set to level $X=x$ by external intervention.\footnote{
How the quantities $P(y|x)$ map into estimates of causal effects is not critical to the current discussion, it suffices to note that in a given setting, it can be calculated if the needed variables are observable.}
\end{theorem}
%

 \subsection{Application of back-door criterion to Figure \ref{fig:basic}}
Applying the back-door criterion to Figure \ref{fig:confound} is straightforward and intuitive.
The set of variables $\{Z\}$ or simply $Z$ satisfies the criterion, as $Z$ is not a descendant of $X$ and $Z$ blocks the back-door path $X \leftarrow Z \rightarrow Y$.
So by conditioning on $Z$, we can estimate the causal effect of $X$ on $Y$.
This situation is a generalization of linear model in which $Y = X \beta + Z \gamma + \epsilon_Y$ and $\epsilon_Y$ is independent of $X$ and $Z$, but $X$ and $Z$ are correlated.
In this case, it is well known that omission of $Z$ would result in a biased estimate of $\beta$, the causal effect of $X$ on $Y$, but by including $Z$ in the regression, we get an unbiased estimate of $\beta$.
In this situation, $Z$ is a \emph{confounder}.

Turning to Figure \ref{fig:mech}, we see that $Z$, which is a \emph{mediator} of the effect of $X$ on $Y$, does not satisfy the back-door criterion, because $Z$ is a descendant of $X$.
However, $\emptyset$ (i.e., the empty set) does satisfy the back-door criterion.
Clearly, $\emptyset$ contains no descendant of $X$.
Furthermore, the only path other than $X \rightarrow Y$ that exists is $X \rightarrow Z \rightarrow Y$, which does not have a back-door into $X$.
Note that the back-door criterion not only implies that we need not condition on $Z$ to obtain an unbiased estimate of the causal effect of $X$ on $Y$, but that we should not condition of $Z$ to get such an estimate.

Finally in Figure \ref{fig:collider}, we have $Z$ acting as what \citet[p.\,17]{Pearl:2009kh} refers to as a ``collider" variable.\footnote{
The two arrows from $X$ and $Y$ ``collide" in $Z$.} 
Again, we see that $Z$ does not satisfy the back-door criterion, because $Z$ is a descendant of $X$.
However, $\emptyset$ again satisfies the back-door criterion.
First, contains no descendant of $X$.
Second, the only path other than $X \rightarrow Y$ that exists is $X \rightarrow Z \leftarrow Y$, which does not have a back-door into $X$.
Again, the back-door criterion not only implies that we need not condition on $Z$, but that we should not condition of $Z$ to get an unbiased estimate of the causal effect of $X$ on $Y$.

 \subsection{Causal diagrams and instrumental variables}

\begin{definition}[Instrument]
Let $G$ denote a causal graph in which $X$ has an effect on $Y$. 
Let $G_{\overline{X}}$ denote the causal graph created by deleting all arrows emanating from $X$.
A variable $Z$ is an \emph{instrument} relative to the total effect of $X$ on $Y$ if there exists a set of nodes $S$, unaffected by $X$, such that
\begin{enumerate}
\item $S$ $d$-separates $Z$ from $Y$ in $G_{\overline{X}}$
\item $S$ does not $d$-separate $Z$ from $X$ in $G$
\end{enumerate}
\end{definition}

Applying this definition to Figure \ref{fig:agl}, we can evaluating the instrument used in \citet{Armstrong:2013io}.
There we have have $S = \textit{Compensation}_{t-1}$,
$X =\textit{Shareholder support}_{t}$, $Y = \textit{Compensation}_{t-1}$, and $Z = \textit{ISS recommendation}_{t}$.
We use $U$ to denote the observed variables depicted in the dashed box of Figure \ref{fig:agl}.
If create $G_{\overline{X}}$ by deleting the single arrow emanating from $\textit{Shareholder support}_{t}$, we can see that there are two back-door paths running from $Y$ to $Z$: 
$Z \leftarrow S \rightarrow U \rightarrow Y$ and $Z \leftarrow S \rightarrow Y$.
However, both of these paths are blocked by $S$ and the first requirement is satisfied.
The second requirement is clearly satisfied as $Z$ is directly linked to $X$.\footnote{
This is a necessary condition, but assumptions about functional form are also critical in using an instrument to estimate a causal effect.
However, this is not essential to our argument here.}
%
Note that analysis can be expressed intuitively as requiring that the ISS recommendation only affects \textit{Compensation}$_{t+1}$ through its effect on \textit{Shareholder support}$_{t}$, and that \textit{Compensation}$_{t+1}$ has an effect on \textit{Shareholder support}$_{t}$.

But this analysis presumes that the causal diagram Figure \ref{fig:agl} is correct.
\citet[p.\,912]{Armstrong:2013io} note that the ``validity of this instrument depends on ISS recommendations not having an influence on future compensation decisions conditional on shareholder support (i.e., firms listen to their shareholders, with ISS having only an indirect impact on corporate policies through its influence on shareholders' voting decisions)."
This assumption represented in Figure \ref{fig:agl} by the \emph{absence} of an arrow from \textit{ISS recommendation}$_t$ to \textit{Compensation}$_{t+1}$.

Unfortunately, this assumption seems inconsistent with the findings of \citet{Gow:2013aa}, who provide evidence that firms are carefully calibrating compensation plans (i.e., factors that directly affect \textit{Compensation}$_{t+1}$) to comply with the requirements of ISS's policies, implying a path from \textit{ISS recommendation}$_t$ to \textit{Compensation}$_{t+1}$ that does not pass through \textit{Shareholder support}$_{t}$.
This path is represented in Figure \ref{fig:agl2} and the plausible existence of this path suggests that the instrument of \citet[p.\,912]{Armstrong:2013io} is not credibly valid for the causal effect they seek to estimate.


\end{document}
	
