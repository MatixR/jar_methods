\documentclass[12pt,reqno,titlepage]{amsart}

\usepackage[marginratio=1:1,margin=1in]{geometry}  % See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper} % ... or a4paper or a5paper or ... 
%\geometry{landscape}  % Activate for for rotated page geometry
% \usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
%\usepackage{amsfonts}
\usepackage{palatino}
\usepackage[longnamesfirst]{natbib}
\usepackage{hyperref} 
% \usepackage{paralist}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{attrib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pdflscape}
\usepackage{setspace}
\usepackage{ragged2e}
\raggedbottom

\setlength\parindent{1cm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\tikzset{every node/.style = 
		    	{shape = rectangle, rounded corners, fill = black!30!white,
		   		text width = 3cm, minimum height = 1.5cm, align = center, text = black},
		    every edge/.style = {draw, ->, line width=2pt, black}}

% The following are for Peter's tables		    
\newcommand\T{\rule[0em]{0pt}{1.5em}} % Top strut
\newcommand\B{\rule[-1em]{0pt}{0pt}} % Bottom strut

% To eliminate unnecessary space before bullet points
\usepackage{enumitem}
\setlist{nolistsep}

\title[Causal Inference in Accounting]{Causal Inference in Accounting Research}

\author{Ian D. Gow}
\author{David F. Larcker}
\author{Peter C. Reiss}

%\date{}   % Activate to display a given date or no date

\begin{document}
\usetikzlibrary{automata, shapes, calc, positioning}

\bibliographystyle{chicago}
% Quick LaTeX Guide for Dave (originally for Suraj).

% - Percent signs (%) mark comments. To get a percent sign, escape it by putting a backslash in front.
%  & is another special character in LaTeX. Use \& to get &.
% Note that each part of the document is in a separate file (so we can edit in parallel).
% Citations are automatic with the correct key. 
% LaTeX doesn't pay attention to multiple spaces. Also adjacent lines get collapsed into single paragraphs.
% Insert a blank line between lines that are part of two separate paragraphs.
% It's actually helpful to put every sentence on a separate line. 
% You need two line breaks to indicate a paragraph.
% \section, \subsection, and \subsubsection have the obvious meanings.
% Note that there is a file jar_methods.bib in the list of files to the right that this pulls bibliographic information from.
\begin{titlepage}
  \centering
  	\begin{large}
  	\textbf{Causal Inference in Accounting Research\footnote{We are grateful to our discussants, Christian Hansen and Miguel Minutti-Meza, and participants at the 2015 JAR Conference for helpful feedback. 
  	We also thank seminar participants at London Business School, Karthik Balakrishnan, Philip Berger, Robert Kaplan, Christian Leuz, Alexander Ljungqvist, Eugene Soltes, Daniel Taylor, Robert Verrecchia, Charles Wang, and Anastasia Zakolyukina for comments.}} \\	
  	\end{large}
  	\vspace{60pt}
	\textbf{Ian D. Gow} \\
	Harvard Business School \\
	email: igow@hbs.edu

  	\vspace{30pt}
	\textbf{David F. Larcker} \\
	Stanford Graduate School of Business \\
	Rock Center for Corporate Governance \\
	email: dlarcker@stanford.edu \\
		
	\vspace{30pt}
	\textbf{Peter C. Reiss} \\
	Stanford Graduate School of Business \\
	email: preiss@stanford.edu \\

	\vspace{30pt}
	\today

\end{titlepage}

\begin{abstract}
	This paper examines the approaches accounting researchers use to draw causal inferences using observational (or non-experimental) data. 
	The vast majority of accounting research papers draw causal inferences notwithstanding the well-known difficulties in doing so.
	While some recent papers seek to use quasi-experimental methods to improve causal inferences, these methods also make strong assumptions that are not always fully
	appreciated.
We believe that accounting research would benefit from: 
		 more in-depth descriptive research, including a greater focus on the study of causal mechanisms (or causal pathways); 
		increased emphasis on structural modeling of the phenomena of interest. 
We argue these changes offer a practical path forward for rigorous accounting research. 
\end{abstract}

\maketitle
\clearpage

\section{Introduction}

\begin{quotation}\begin{singlespace} 
There is perhaps no more controversial practice in social and biomedical research than drawing inferences from observational data.
Despite \dots problems, observational data are widely available in many scientific fields and are routinely used to draw inferences about the causal impact of interventions.
The key issue, therefore, is not whether such studies should be done, but how they may be done well.
\attrib{\citealt{Berk:1999uz}}
\end{singlespace}
\end{quotation}

\begin{doublespace} 
Most empirical research in accounting relies on observational (or non-experimental) data.
% (i.e., data produced by processes outside the control of the researcher). Even experimental data is ``outside the control of the researcher," so this is an imprecise statement.
This paper evaluates the different approaches accounting researchers use to draw causal inferences from observational data. 
Our discussion draws on developments in fields such as statistics, econometrics and epidemiology. 
The goal of this paper is to identify areas for improvement and suggest how empirical accounting research can improve inferences from the analysis of observational data.

The importance of causal inference in accounting research is clear from the research questions that accounting researchers seek to answer. 
Most long-standing questions in accounting research are causal: 
Does conservatism affect the terms of loan contracts?
Do higher quality earnings reports lead to lower information asymmetry? 
Did International Financial Reporting Standards cause an increase in liquidity in the jurisdictions that adopted them?
Do managerial incentives lead to managerial misstatements in financial reports?
That accounting researchers focus on causal inference is consistent with the view that ``the most interesting research in social science is about questions of cause and effect" \cite[p. 3]{Angrist:2008vk}.
Simply documenting descriptive correlations provides little basis for understanding what would happen should circumstances change, 
whereas using data to make inferences that support or refute broader theories could facilitate these kinds of predictions.


To provide insights into what is actually done in empirical accounting research, we examined all papers published in three leading accounting journals in 2014. 
% refer to Heckman-style methods as "treatment effect models?  Maybe cite Francis paper in TAR here?
% For instance, many researchers who estimate so-called treatment effect models are aware that random assignment is usually required to ensure that differences between the treatment and control samples are due to the treatment. 
% Despite this awareness, many accounting ``treatments" are not randomly assigned, and the researcher then is in the position of arguing that they can account for all other reasons that might lead to a difference between the treatment and control samples.
% 
% Few papers really care about the estimated causal effects (this is not a good thing), so this seems a bit too precise.
% But I think it is fair to say that it is difficult to draw causal inferences (e.g., equity incentives lead to more misstatements) without the kinds of assumptions you need to estimate causal effects.
While accounting researchers are aware of problems that can arise from the use of observational data to draw causal inferences, we found that most papers using such data seek to draw such inferences. 
Making causal inferences requires strong assumptions about the causal relations between variables studied.
For example, estimating the causal effect of $X$ on $Y$ requires that the researcher has controlled for variables that could confound estimates of such effects.
In Section \ref{sec:causal}, we provide an overview of causal inference using causal diagrams as a framework for thinking about the subtle issues involved.
We believe that these diagrams are also very useful for communicating the cause-and-effect logic underlying the typical regression analyses that rely on observational data.
Nonetheless, the difficulty of identifying, measuring, and controlling for all possible confounding variables leads many to be skeptical of the use of regression analyses of observational data for causal inference.

Recently, some social scientists have held out hope that better research designs and statistical methods can increase the credibility of causal inferences.
For example, \citet{Angrist:2010jv} suggest that ``empirical microeconomics has experienced a credibility revolution, with a consequent increase in policy relevance and scientific impact.''  
\citet[p. 26]{Angrist:2010jv} argue that such ``improvement has come mostly from better research designs, either by virtue of outright experimentation or through the well-founded and careful implementation of quasi-experimental methods."
Our survey of research published in 2014 finds five studies claiming to study natural experiments (or ``exogenous shocks") and ten studies using instrumental variables.
Thus, quasi-experimental methods are used by a small minority of papers  in accounting research, and we believe their use will increase in future research.\footnote{
We use the term ``quasi-experimental" methods to refer to those methods that have a plausible claim to ``as if" random assignment to treatment conditions.
The term ``as if" is used by \citet{Dunning:2012tt} to acknowledge the fact that assignment is not random in such settings, but is claimed to be \emph{as if} random assignment had occurred.}

In Section \ref{sec:quasi}, we examine and evaluate the use of quasi-experimental methods in accounting research. 
Quasi-experimental methods produce credible estimates of causal effects only under very strong maintained assumptions about the model and the data relied upon. 
For example, variations in treatments are rarely random, the list of controls rarely exhaustive, and instruments do not always satisfy the necessary inclusion and exclusion restrictions.
We explain some of these concerns using causal diagrams.
In general, it appears that the assumptions required to apply quasi-experimental methods are unlikely to be satisfied by observational data in most empirical accounting research settings.

% Dave: "One especially promising path is use of field experiments with randomized treatments to measure causal effects (a good example -- Roberts QJE paper)." % \citet{Roberts:2013cz} 
% Ian: "I think we could discuss these as interesting and a path forward, but scope them out as our focus is on observational data."
Ultimately, we believe that accounting research needs to recognize the stringent \emph{causal} assumptions that need to be maintained to apply statistical methods to derive estimates of causal effects for observational data.
Statistical methods alone cannot solve the inference issues that arise in observational data. 
The second part of the paper (Sections \ref{sec:mech} and \ref{sec:struct}) identifies approaches that can provide a plausible framework for guiding future accounting research: 

\begin{itemize}
\item There should be an increased emphasis on the study of causal mechanisms, i.e., the ``pathways" through which claimed causal effects are propagated.
We believe that evidence on the actions and beliefs of individuals and institutions can bolster causal claims based on associations, even absent compelling estimates of the causal effects.
We also suggest that more careful modeling of phenomena, using structural modeling or causal diagrams, can help to identify plausible mechanisms that warrant further study.
\item There should be an increased use of structural modeling methods. 
Structural models provide a more complete characterization of the behavior and institutions that underlie a phenomenon of interest.
We readily acknowledge that, while structural modeling does not solve endogeneity concerns, it makes the assumed causal structure explicit and gives the researchers a rigorous way to assess what would happen if some features of the model change (i.e., to provide counterfactuals).
We believe that causal diagrams can be a useful tool to convey the key elements of a structural model and can also act as a middle-level stand-in when structural modeling of a phenomenon is in its early stages or is incomplete.\footnote{``Middle-level" here refers to the placement of causal diagrams between relatively informal verbal reasoning and the rigors of a structural model.}
%TODO: Flesh this out somewhat. What are the benefits of doing structural modeling?
\item There are many important questions in accounting that have not yet been addressed by formal models.
	In these settings, it is important to conduct sophisticated descriptive research aimed at understanding the phenomena of interest so as to develop clearer cause and effect models.
	In our view, many hypotheses that are tested with observational data are only loosely tied to the accounting institutions and business phenomena of interest. 
	Hopefully, these descriptive studies will provide insights that theorists can use to build models that empiricists can actually ``take to data."
% http://www.chicagomanualofstyle.org/16/ch06/ch06_sec009.html
	%TODO: The 2017 JAR call for papers is arguably evidence of this concern being shared by others. Considering adding a footnote saying this.  DL -- this is a good idea
	%A better approach would be to draw on detailed studies that better \emph{describe} 
	%Such studies likely will have a greater chance of identifying causal pathways and entice theorists to build better models of the phenomena of interest.
	%TODO: Trim the "descriptive research" bullet point.
\end{itemize}

\vskip 10pt
The remainder of the paper is structured as follows.
Section \ref{sec:causal} provides an overview of the issues observational data pose for drawing causal inferences in accounting research; 
it suggests frameworks for identifying and analyzing these issues.
Section \ref{sec:quasi} evaluates the use of quasi-experimental methods in accounting research.
Section \ref{sec:mech} discusses mechanism-based causal inference and the value of descriptive research.
% In Section \ref{sec:desc} we argue for richer descriptive research that can shed light on causal issues.
Section \ref{sec:struct} illustrates how structural modeling approaches might be used by accounting researchers, with some emphasis of the strengths and weaknesses of this approach.
Concluding remarks are provided in Section \ref{sec:conclude}.

\section{Causal inference: An overview} \label{sec:causal}

\subsection{Causal inference in accounting research}
%TODO: Flag experimental research, then say we're focused on studies using observational data.
%TODO: Add a footnote to the guy doing something similar is AOS -- claims that only 3% of papers are causal.
%TODO: Add a footnote *somewhere* discussing other work on "causal diagrams" in accounting research. I think we want to do no more than mention that these exist and suggest that what we're doing is a bit more formal.

To get a sense for the importance of causal questions in accounting research,
we examined all papers published in 2014 in the \textit{Journal of Accounting Research}, \textit{The Accounting Review}, and the \textit{Journal of Accounting and Economics}.
We counted 139 papers, of which 125 are original research papers. Another 14 papers survey or discuss other papers.
We classify each of the 125 research papers into one of four categories:  ``Theoretical'' (7); ``Experimental'' (12); ``Field" (3); or ``Archival Data" (103). 
For our discussion below, we collect the field and archival data papers into a single  ``Observational" category.

For each non-theoretical paper, we determine whether the primary or secondary research questions are ``causal." 
Often the title reveals a causal question, with words such as ``effect of \dots" or ``impact of \dots" \citep[e.g.][]{Cohen:2014jl,Clorproell:2014cv}. 
In other cases, the abstracts reveal that authors have causal inferences as a goal. 
For example, \citet{deFranco:2014ct} asks ``how the tone of sell-side debt analysts' discussions about debt-equity conflict events \emph{affects} the informativeness of debt analysts' reports in debt markets.''

We recognize that some authors might disagree with our characterizations.
For example, a researcher might argue that a paper that claimed that ``theory predicts $X$ is associated $Y$ and, consistent with that theory, we show $X$ is associated with $Y$" is merely a descriptive paper that does not make causal inferences.
However, theories are invariably causal in that they posit how exogenous variation in certain variables leads to changes in other variables.
Further, by stating that ``consistent with \dots theory, $X$ is associated with $Y$," the clear purpose is to argue that the evidence tilts the scale, however slightly, in the direction of believing the theory is a valid description of the real world: in other words, causal inference.\footnote{
Papers that seek to estimate a causal effect of $X$ on $Y$ are a subset of papers we classify as causal.
A paper that argues that $Z$ is a common cause of $X$ and $Y$ and claims to find evidence of this is still making causal inferences (i.e., that $Z$ causes $X$ and $Z$ causes $Y$).
However, we do not find this kind of reasoning to be common in our survey.}
%TODO: Find an example of this kind of paper. I know there are some.

Of the 106 original papers using observational data, we coded 91 as seeking to draw causal inferences.\footnote{While we exclude research papers using experimental methods, all of these papers also seek to draw causal inferences.}
Of the remaining empirical papers, we coded 7 papers as having a goal of ``description'' (including two of the three field papers). 
For example, \citet{Soltes:2013ba} uses data collected from one firm to describe analysts' private interactions with management. Understanding how these interactions take place is key to understanding whether and how they transmit information to the market.
We coded 5 papers as having a goal of ``prediction.'' 
For example, \citet{Czerney:2014bv} examine whether the inclusion of ``explanatory language" in unqualified audit reports can be used to predict the detection of financial misstatements in the future.
We coded 3 papers as having a goal of ``measurement.'' 
For example, \citet{Cready:2014ji} examine whether inferences about traders based on trade size are reliable and suggest improvements to the measurement of variables used by accounting researchers.

In summary, we find that most original research papers use observational data and that about 90\% of these papers seek to draw causal inferences.
The most common estimation methods used in these studies include ordinary least-squares (OLS) regression, difference-in-difference estimates, and propensity-score matching.
While it is widely understood that OLS regressions that use observational data produce unbiased estimates of causal effects only under very strong assumptions, the credibility of these assumptions is rarely explicitly addressed.\footnote{
There are settings where difference-in-difference and fixed effect estimators may deliver causal estimates.
For example, if assignment to treatment is random, then it is possible for a difference-in-difference estimate using pre- and post-treatment data to yield unbiased estimates of causal effects.
But in this case, it is the detailed understanding of the research setting, not the method \emph{per se}, that makes these estimates credible.}
%TODO: So why do we see so many papers using OLS for causal inference? My conjecture: If the results is the "right" one, no-one cares about endogeneity. So WTF are we doing when we do research? (Even by the standards of this paper, this point is a little "heavy" I think.)

\subsection{Causal inference: A brief overview of recent developments}
In recent decades, the definition and logic of causality has been revisited by researchers in fields as diverse as epidemiology, sociology, statistics, and computer science. 
Work by \citet{Rubin:1974im,Rubin:1977dv} and \citet{Holland:1986p7458} formalized ideas from the potential-outcome framework of \citet{Neyman:1923aa}, leading to the so-called Rubin causal model. 
Other fields have used path analysis, as initially studied by geneticist Sewell Wright \citep{Wright:1921aa}, as an organizing framework.
In economics and econometrics, early proponents of structural models were quite clear about how causal statements must be tied to theoretical economic models.
As discussed by \citet{Heckman:2015ez}, \citet{Haavelmo:1943cl,Haavelmo:1944jq} promoted structural models ``based on a system of structural equations that define causal relationships among a set of variables."
%standard econometric texts generally avoid explicit discussion of causation.
%For example, Greene (2003) does not discuss causality except for Granger causality, which is widely recognized as a purely statistical notion quite distinct from notions of one variable causing another.
% However, some economists have explic
\citet[p.\,979]{Goldberger:1972cq} promoted a similar notion: 
``By structural equation models, I refer to stochastic models in which each equation represents a causal link, rather than a mere empirical association \dots
Generally speaking the structural parameters do not coincide with coefficients of regressions among observable variables, but the model does impose constraints on those regression coefficients."
\citet{Goldberger:1972cq} focuses on linking such approaches to the path analysis of Wright.

An important point worth emphasizing is that model-based causal reasoning is distinct from statistical reasoning. 
Suppose we observe data on $x$ and $y$ and make the strong assumption that we know that causality is one-way. 
How do we distinguish between whether $X$ causes $Y$ or $Y$ causes $X$? 
Statistics can help us determine whether $X$ and $Y$ are correlated, but correlations do not establish causality.
Only with assumptions about causal relations between $X$, $Y$, and other variables (i.e., a theory) can we infer causality.
While theories may be informed by evidence (e.g., prior research may suggest a given theory is more or less plausible), they also encode our understanding of causal mechanisms (e.g., that barometers do not cause rain).

% \subsection{Causal diagrams: A primer}
Computer and decision scientists, as well as researchers in other disciplines, have recently sought to develop an analytical framework for thinking about causal models and their connection to probability statements \citep{Pearl:2009kh}.
Pearl's framework, which he calls the structural causal model, uses causal diagrams to describe causal relationships. 
These diagrams encode causal assumptions and visually communicate how a causal inference is being drawn from a given research design.
Given a \emph{correctly specified} causal diagram, these criteria can be used to verify conditioning strategies, instrumental variable designs, and mechanism-based causal inferences.\footnote{While \citet[p.248]{Pearl:2009kh} defines an instrument in terms of causal diagrams, additional assumptions (e.g., linearity) are often needed to estimate causal effects using an instrument \citep{Angrist:1996p7456}.}

We use Figure \ref{fig:basic} to illustrate the basic ideas of causal diagrams and how they can be used to facilitate causal inference.
Figure \ref{fig:basic} depicts three variants of a simple causal graph.
Each graph depicts potential relationships among the three (observable) variables.
In each case, we are interested in understanding how the presence of a variable $Z$ impacts the estimation of the causal effect of $X$ on $Y$.
The only difference between the three graphs is the direction of the arrows linking either $X$ and $Z$, or $Y$ and $Z$.
The boxes (or ``nodes") represent random variables and the arrows (or ``edges") connecting boxes represent hypothesized causal relations, with each arrow pointing from a cause to a variable assumed to be affected by it.
% \footnote{
%That arrows have a direction accounts for the ``D" in DAG (Directed Acyclic Graph). 
%The acyclic (``A") component means that there must be no cycles in the graph. 
%Cycles cause obvious problems in causal reasoning.
% An example would be $X \rightarrow Y \rightarrow Z \rightarrow X$. 
%In this graph there is no ultimate cause. 
%Graphs make a distinction between observed and unobserved random variables.
%In some cases, an unobserved joint determinant of two random variables will not be represented explicitly, but replaced by a dashed, ``undirected" edge between those two random variables.}

\citet{Pearl:2009vo} shows that, if we are interested in assessing the causal effect of $X$ on $Y$, we may be able to do so by conditioning on a set of variables, $Z$, that satisfies certain criteria.
These criteria imply that very different conditioning strategies are needed for each of the causal diagrams (see Appendix \ref{append} for a more formal treatment).

%what \citet{Pearl:2009vo} labels the ``back-door criterion" \citep[p.79]{Pearl:2009vo}.\footnote{
%Intuitively, the back-door criterion requires that $Z$ blocks (and does not open) ``back-door" paths.
%A back-door path can be thought of as a way for $X$ to be associated with $Y$ due to associations with other variables rather than causal links from $X$ to $Y$.}
While conditioning on variables is much like the standard notion of ``controlling for" such variables in a regression, there are critical differences.
First, conditioning means estimating effects for each distinct level of the set of variables in $Z$. 
This nonparametric concept of conditioning on $Z$ is more demanding than simply including $Z$ as another regressor in a linear regression model.\footnote{Including variables in a linear regression framework ``controls for" only under strict assumptions, such as linearity in the relations between $X$, $Y$, and $Z$.}
Second, the inclusion of a variable in $Z$ may not be an appropriate conditioning strategy. 
Indeed, it can be that the inclusion of $Z$ results in biased estimates of causal effects.

We now discuss what each of the three graphs in Figure \ref{fig:basic} suggest about how one might model the causal effect of $X$ on $Y$.	
Figure \ref{fig:confound} is straightforward. 
It shows that we need to condition on $Z$ in order to estimate the causal effect of $X$ on $Y$.
Note the notion of ``condition on" again is more general than just including $Z$ in a parametric (linear) model.\footnote{
Inclusion of $Z$ blocks the ``back-door" path from $Y$ to $X$ via $Z$.} The need to condition on $Z$ arises because $Z$ is what is known as a \emph{confounder}.

%Nevertheless, under the
%strong assumptions that: $Y = X \beta + Z \gamma + \epsilon_Y$, $\epsilon_Y$ is independent of $X$ and $Z$, and $X$ and $Z$ are correlated,
%the diagram suggests that the omission of $Z$ would result in a biased estimate of $\beta$.

Figure \ref{fig:mech} is a bit different. Here $Z$ is a \emph{mediator} of the effect of $X$ on $Y$.
No conditioning is required in this setting to obtain an unbiased estimate of the effect of $X$ on $Y$.
But, it is not simply the case that we need not condition on $Z$ to obtain an unbiased estimate of the causal effect of $X$ on $Y$, but that we \emph{should} not condition on $Z$ to get such an estimate.
Including $Z$ can lead to bias of any sign and even to finding a net causal effect of $X$ on $Y$ when no such effect exists.
%\footnote{
%For example, the effect of $X$ via $Z$ might counteract other effects. By including $Z$ as a control, we would consider these other effects t

Finally in Figure \ref{fig:collider}, we have $Z$ acting as what is referred to as a ``collider" variable \citep{Glymour:2008aa,Pearl:2009kh}.\footnote{
The two arrows from $X$ and $Y$ ``collide" in $Z$.} 
Again, not only do we not need to condition on $Z$, but that we \emph{should} not condition on $Z$ to get an unbiased estimate of the causal effect of $X$ on $Y$.
While in epidemiology, the issue of ``collider bias \dots can be just as severe as confounding" \citep[p.\,186]{Glymour:2008aa}, collider bias appears to receive less attention in accounting research than confounding.
Many intuitive examples of collider bias involve selection or stratification.
Admission to a college could be a function of combined test scores ($T$) and interview performance ($I$) exceeding a threshold, i.e., $T + I \geq C$. 
Even if $T$ and $I$ are unrelated unconditionally, a regression of $T$ on $I$ conditioned on admission to college is likely to show a negative relation between these two variables.

\subsection{Causal diagrams: Applications in accounting}
A typical paper in accounting research will include many variables  to ``control for" the potential confounding of causal effects.
While many of these variables should be considered confounders, less attention is given to explaining why it is reasonable to assume that they are not mediators or colliders.
Such a discussion is important because the inclusion of ``controls" that are mediators or colliders will generally lead to bias.

One paper that does discuss this distinction is \citet{Larcker:2007aa}, who use a multiple regression (or logistic) model of the form:\footnote{We alter the mathematical notation of  \citet{Larcker:2007aa} to conform with notation we use here.}
\begin{equation}
Y = \alpha + \sum_{r=1}^R \gamma _r Z_r + \sum_{s=1}^S \beta_s X_s + \epsilon \label{eqn:lrt1}
\end{equation}

\noindent \citet{Larcker:2007aa} suggest that 

\begin{quote}\begin{singlespace} 
``One important feature in the structure of Equation \ref{eqn:lrt1} is that the governance factors [$X$] are assumed to have no impact on the controls (and thus no indirect impact on the dependent variable). 
As a result, this structure may result in conservative estimates for the impact of governance on the dependent variable. Another approach is to only include governance factors as independent variables, or:
\begin{equation}
Y = \alpha + \sum_{s =1}^S \beta_s X_s + \epsilon \label{eqn:lrt2}
\end{equation}
The structure in Equation \ref{eqn:lrt2} would be appropriate if governance impacts the control variables and both the governance and control variables impact the dependent variable (i.e., the estimated regression coefficients for the governance variables will capture the total effect or the sum of the direct effect and the indirect effect through the controls).''
\end{singlespace} \end{quote}

But there are some subtle issues here.
If some elements of $Z_r$ are mediators and others are confounders, then both equations will be subject to bias. 
Equation \ref{eqn:lrt2} will be biased due to omission of confounders, while Equation \ref{eqn:lrt1}  will be biased due to inclusion of mediating variables.
Additionally, the claim that the estimates are ``conservative" is only correct if the indirect effect via mediators is of the same sign as the direct (i.e., unmediated) effect. 
If this is not the case, then the relation between the magnitude (and even the sign) of the direct effect and the indirect effect is unclear.

Additionally, this discussion does not allow for the possibility of colliders.
For example, governance plausibly affects leverage choices, while performance is also likely to affect leverage.
If so, ``controlling for" leverage might induce associations between governance and performance even absent a true relation between these variables.\footnote{
Note that \citet{Larcker:2007aa} do not in fact use leverage as a control when performance is a dependent variable.}
%For example, \citet{Cadman:2014cr} study, \emph{inter alia}, the effect of being a venture capital (VC)-backed firm on CEO incentive horizons after IPO. In their analysis they include as controls variables such as \emph{Toptier Underwriter} and \emph{R\&D/Assets}.\footnote{See Table 5 of \citet{Cadman:2014cr}.} But, given the timing of events, being VC-backed affecting the choice of underwriter is the most plausible causal relation between these two variables. Also, given that \citet{Cadman:2014cr} include pre-IPO observations, it is plausible that the CEO's incentive horizons would affect variables such as \emph{Toptier Underwriter} and \emph{R\&D/Assets}. If these assertions are correct,  \emph{Toptier Underwriter} and \emph{R\&D/Assets} are colliders. While these variables may have little impact on the results of \citet{Cadman:2014cr}, we argue that more discussion about why researchers include controls is warranted.
While the with-and-without-controls approach used by \citet{Larcker:2007aa} has intuitive appeal, a more robust approach requires careful thinking about the plausible causal relations between the treatment variables, the outcomes of interest, and the candidate control variables.

\section{Quasi-experimental methods in accounting research} \label{sec:quasi}
While most studies in accounting use methods of conditioning on confounding variables in some form of regression or matching model, a number of studies use quasi-experimental methods that rely on ``as if" random assignment to identify causal effects \citep{Dunning:2012tt}.
Of the 91 papers in our 2014 survey seeking to draw a causal inference from observational data, we classify 14 as relying on quasi-experimental methods.
Despite the low count, we believe that papers using these methods are considered stronger research contributions, and there seems a clear trend toward the use of quasi-experimental methods.
Additionally, a number of papers use methods, such as difference-in-difference estimators and fixed effects that are widely believed to approximate quasi-experimental methods.
In this section, we discuss and evaluate the use of quasi-experimental  and similar methods in accounting research. 

\subsection{Natural experiments}
Natural experiments occur when observations are assigned by nature (or some other force outside the control of the researcher) to treatment and control groups in a way that is random or ``as if'' random \citep{Dunning:2012tt}. 
Truly (as if) random assignment to treatment and control provides a sound basis for causal inference, enhancing the appeal of natural experiments for social science research.
However, \citet[\,p.3, emphasis added]{Dunning:2012tt} argues that this appeal ``may provoke \emph{conceptual stretching}, in which an attractive label is applied to research designs that only implausibly meet the definitional features of the method.'' 

Our survey of accounting research in 2014 identified five papers that exploited either a ``natural experiment'' or an ``exogenous shock'' to identify causal effects.\footnote{These are \citet{Lo:2013jk,Aier:2014ii,Kirk:2014gx,Houston:2014hv}; and \citet{Hail:2014fq}.}
An examination of these papers reveals how difficult it is to find a plausible natural experiment in observational data.

The most important concern is that that most ``exogenous shocks" (e.g., SEC regulatory changes or court rulings) generally do not randomly assign firms into treatment and control groups.
For example, an early version of Dodd-Frank contained a provision that would force companies to remove a staggered board structure.\footnote{See \citet{Larcker:2011hs}.}
It is tempting to use this event to assess the valuation consequences of having a staggered board by looking at excess returns for firms with and without a staggered board around the announcement of this Dodd-Frank provision.
Although potentially interesting, this ``natural experiment" does not randomly assign firms to treatment and control groups regarding a staggered board.  That is, firms made an endogenous choice about staggered boards and the regulation is potentially forcing firms to change their choice. But firms might have a variety of margins through which they might respond to such a requirement, some of which may have valuation consequences of their own. Absent an account of these margins, an event study that includes a staggered board treatment variable does not isolate the (pure) effect of staggered boards on valuations. 

% This example is just not good. It's talking about high-faluting issues conditional on random assignment of treatment that are much more subtle than the problems in actual accounting research.
Another important concern is that there should be a strong reason to believe that the natural experiment impacted assignment to treatment and this impact is uncorrelated with unobserved factors that might impact the outcome of interest. 
% Too often, researchers focus exclusively on arguing the natural experiment is ``exogenous."
In general, even claims of random assignment to treatment do not suffice to deliver unbiased estimates of causal effects.
An example of a drug trial can help underscore these points. 
Suppose we wish to understand whether a drug lowers blood pressure. 
Imagine patients in the trial are drawn from two hospitals.
One hospital is randomly selected as the hospital in which the drug will be administered. 
The other hospital's patients serve as controls. 
Suppose in addition that we know the patient populations in both hospitals are similar. 

Most researchers would argue that we have all the ingredients for a successful treatment effect study.
% With two observations?! This is a terrible example. What's the point? 
% That even random assignment doesn't deliver unbiased estimates of causal effects.
In particular, assignment to treatment is random.
Now imagine that patients actually have to take the drug for it to have an effect.
In this case, if there are unobserved reasons why some assigned to treatment opt out, modify the dosage, or stop taking medications for which there might be interactions, then being assigned to treatment is not the same as treatment. 
To take an extreme example, suppose the drug has a slight negative effect on blood pressure, everyone in fact takes the drug, but doctors in the hospital where patients are treated tell patients to stop taking their regular blood pressure medication. 
In this case, if regular blood pressure medications lower blood pressure more than the new drug, we might conclude the new drug actually \emph{raises} blood pressure! 
In sum, even showing that a treatment is randomly assigned does not guarantee that a regression will uncover the causal effect of interest. 
% More work is required.

Finally, it is important to carefully consider the choice of explanatory variables in studies that rely on natural experiments.
In particular, researchers sometimes inadvertently use covariates that are affected by the treatment in their analysis.  
As noted by \citet[p.\,116]{Imbens:2015aa}, including such post-treatment variables as covariates can undermine the validity of causal inferences.\footnote{See the discussion of mediators above.}
%%% NOTE: IDG edit below.
% As an example, consider what the introduction of doctor fixed effects might do in a treatment effect regression study of the blood pressure drug.
% We have a better example below. I think few would be able to see the point regarding fixed effects. (I take the point to be that fixed effects are inherently, at least in part, post-treatment variables). 

%First, some studies using exogenous shocks plausibly suffer from issues of confounding, as treatment assignment is non-random. For example, \citet{Hail:2014fq} study the effect of the ``exogenous shocks" of mandatory IFRS adoption and enforcement of insider trading laws on firms' dividend payments. While these shocks sort firms into treatment and control groups, they clearly do not do so randomly, as they apply to firms in specific countries and a variety of time-varying country-level effects plausibly exist.\footnote{According to Table 1 of \citet{Hail:2014fq}, about 60\% of treatment firms are European firms that adopted IFRS in 2005. Figure 1, Panel A of \citet{Hail:2014fq} suggests that most of the impact of IFRS adoption occurs three years after adoption; i.e., for European firms in 2008, when there may have been other reasons for reducing dividends that applied to those firms more than controls.}

%Second, because ``exogenous shocks" often do not directly sort firms into treatment and control groups, they rely on assumptions analogous to those required for instrumental variables. That is, the exogenous shock should not only be random, but should only affect the outcome through its effect on the treatment.\footnote{In some cases, if the necessary assumptions apply, it would be more appropriate to use instrumental variable methods to estimate causal effects.In other cases, \citep[e.g.][]{Aier:2014ii}, the treatment of interest is unobserved, making such an approach unfeasible.} For example, \cite{Aier:2014ii} exploit a 1991 Delaware court ruling as a ``natural experiment'' for the purpose of understanding the causal effect of debtholders' demand for conservatism (the treatment variable) on financial reporting conservatism (the outcome of interest).\footnote{The court ruling ``expanded the scope of directors' fiduciary duties to include creditors when a Delaware incorporated firm is in the `vicinity of insolvency.'"} For the court ruling to be a valid instrument for debtholders' demand for conservatism, it must only affect the outcome through its effect on the treatment of interest.But if the 1991 Delaware court ruling  ruling in question caused directors to dispose of assets leading to recognition of losses and affecting measures of conservatism, as it plausibly did, then the identification strategy is invalid.\footnote{Similar issues plausibly affect \citet{Houston:2014hv}, which uses the ``exogenous shock" of the 2008 financial crisis and the ``natural experiment" of midterm elections to study ``whether the political connections of listed firms in the United States affect the cost and terms of loan contracts,'' and \citet{Kirk:2014gx}, who ``exploit the natural experiment setting created by the exogenous shock of Reg FD" to examine ``the effect of investments in internal investor relations (IR) departments on firm outcomes."} 
% Random assignment is also dubious here.

%The evidence from research published in 2014 suggests that accounting researchers apply the term ``natural experiment" to circumstances where it is not clear that it applies. While ``exogenous shocks" may provide interesting settings for research, if random assignment does not apply, then researchers should exercise caution in giving causal explanations to associations observed in the data. Readers should be alert to the fact that terms like ``exogenous shock" and ``natural experiment" are often used when ``as if" random assignment is not plausible.

Extending our survey beyond research published in 2014, we find papers with very credible natural experiments.
One such paper is \citet{Michels:2015aa}, who exploits the difference in disclosure requirements for significant events that occur before financial statements are issued. 
Because the timing of his events (e.g., fires and natural disasters) relative to balance sheet date is plausibly random, the assignment to the disclosure and recognition conditions is also plausibly random. 
Nevertheless, even in this relatively straightforward setting, \citet{Michels:2015aa} recognizes the possibility of different materiality criteria for disclosed and recognized events, which could affect the relation been underlying events and observed disclosures, and takes care to address this concern.

Another credible natural experiment is examined in \citet[p.\,80]{Li:2015he}, who study a regulatory experiment in which the SEC ``mandated temporary suspension of short-sale price tests for a set of randomly selected pilot stocks." 
\citet[p.\,79]{Li:2015he} conjecture ``that managers respond to a positive exogenous shock to short selling pressure \dots by reducing the precision of bad news forecasts." 
But if the treatment affects the properties of these forecasts, and \citet[p.\,79]{Li:2015he} sought to condition on such properties, they would risk undermining the ``natural experiment" aspect of their setting.
%\footnote{\citet{Li:2015he} include the magnitude of the forecast surprise (\textit{MFSURP}) in their regressions (e.g., regressions in Table 2 where abnormal returns around the forecast is the dependent variable).

%But if the exogenous shock affects the properties of the forecast (i.e., makes it endogenous), the ``natural experiment" aspect of the research design is undone by the decision to include such properties in the regression analysis.\footnote{\citet{Li:2015he} include the magnitude of the forecast surprise (\textit{MFSURP}) in their regressions (e.g., regressions in Table 2 where abnormal returns around the forecast is the dependent variable).
%See \citet[p.\,116]{Imbens:2015aa} for discussion of ``the dangers of using a post-treatment variable \dots as a covariate."}

When true natural experiments can be found, they are an excellent setting for drawing causal inferences from observational data. 
Unfortunately, credible natural experiments are very rare.
Certainly researchers should exploit these natural experiments when they occur \citep[e.g.,][]{Michels:2015aa,Li:2015he}, but care is needed in doing so.

\subsection{Instrumental variables}
\citet[p.114]{Angrist:2008vk} describe instrumental variables (IV) as ``the most powerful weapon in the arsenal" of tools in econometrics. 
Accounting researchers have long used instrumental variables to address concerns about endogeneity \citep{Larcker:2010fq,Lennox:2012it} and continue to do so.
Our survey of research published in 2014 identifies 10 papers using instrumental variables.\footnote{
These are \citet{Cannon:2014im,Cohen:2014jl,Kim:2014fm,Vermeer:2014bs,Fox:2014io,Guedhami:2013cj,Houston:2014hv,deFranco:2014ct,Erkens:2014hj} and \citet{Correia:2014fp}.}
Much has been written on the challenges for researchers in using instrumental variables (IV) as the basis for causal inference \citep[e.g.,][]{Roberts:2013cz}, and it is useful to use this background to evaluate the application of this approach in accounting research. 

\subsubsection{Evaluating IVs requires careful theoretical causal (not statistical) reasoning}

With respect to accounting research, \citet{Larcker:2010fq} lament that ``some researchers consider the choice of instrumental variables to be a purely statistical exercise with little real economic foundation'' and call for 
``accounting researchers \dots to be much more rigorous in selecting and justifying their instrumental variables.'' 
\citet[p.117]{Angrist:2008vk} argue that ``good instruments come from a combination of institutional knowledge and ideas about the process determining the variable of interest."
One study that illustrates this is \citet{Angrist:1990dk}.
In that setting, the draft lottery is well understood as random and the process of mapping from the lottery to draft eligibility is well understood.
Furthermore, there are good reasons to believe that the draft lottery does not affect anything else directly except for draft eligibility.\footnote{Though some have questioned the exclusion restriction even in this case, arguing that the outcome of the draft lottery may have caused some, for example, to move to Canada \citep[see][]{Imbens:2015aa}.}

Note that simply arguing that the only effect of an instrument on the outcome variable of interest is via the treatment of interest does not suffice to establish the exclusion restriction.
Even if the claim that $Z$ only affects $Y$ via its affect on $X$ is true, the researcher also needs to argue that variation in the instrument ($Z$) is as-if random.
For example, suppose that the only effect of $Z$ on $Y$ occurs via $X$, but that $Z$ is a function of a variable $W$ that is also associated with $Y$. 
In this case, instrumental-variable estimates of the effect of $X$ on $Y$ will be biased.
Thus, a researcher should also account for the sources of variation in the chosen instrument and why these are not expected to be associated with  variation in the outcome variable.\footnote{
In the case of \citet{Angrist:1990dk}, this was plausibly satisfied by the use of a lottery for assignment of $Z$ to subjects.}

%%%NOTE: I don't think the following was the criticism:
% These points notwithstanding, even this study has been critiqued because the lottery instruments are weak.
%\footnote {Of course, this seemingly ``ideal" instrument has been subject to considerable criticism. I think the idea is that even if you had a low draft number it was not clear that you actually went to the army.  In fact, upper class kids did not go (and the probably had much better skills) than the white hillbilly trash and minorities that ended up going to VN}
%TODO: Get reference for criticism of Angrist instrument

Unfortunately, there are few (if any) accounting variables that meet the requirement that they randomly assign observations to treatments, and do not affect the outcome of interest outside of effects on the treatment variable.
%NOTE: I think it's a bad idea to use examples that no-one uses. And the director-age thing is getting into a specific paper without explaining what we have in mind. I think saying "X is exogenous" is the kind of language that we need to get away from.
%%% One has to make fundamental assumptions, derive a model, and then write out an estimation equation before one can give useful meaning to "X is exogenous"! There is grave linguistic confusion here!
%Consider, for example, instruments such as firm size, leverage, corporate governance, or director age. 
%Not even director age can be taken as exogenous. %
%Further, its hard to believe that variables such as firm size and leverage can be excluded from the equation of interest. 
Sometimes researchers turn to lagged values of endogenous variables or industry averages as instruments, but these too are subject to criticism.\footnote{
%or fixed effects 
See \citet{Reiss:2007ej} for a discussion regarding the implausibility of general claims that industry averages are valid instruments.}  

% For example, to address endogeneity \citet{Cohen:2014jl} use ``two instrumental variables. The first is the natural log of industry size, measured as the number of companies within each two-digit SIC. The second measures industry competition using the Herfindahl-Hirschman index, which is well-established as a measure of competitive industries. Our untabulated results using this approach are qualitatively similar to our main analysis, thus indicating that endogeneity is not a concern when assessing the reliability of our findings.''\footnote{Three other studies used a similar approach. \citet{Vermeer:2014bs} ``use Maddala's (1988) two-stage procedure'' in order to ``control for endogeneity'' without providing any explanation at all and in fact seem to be assuming the each of three endogeneous variables can used as an instrument for the other two.\citet[p.48]{Fox:2014io} state in a footnote that they ``instrumented for the price index employing a two stage least squares estimator'' without further details, simply noting that their ``conclusions are robust with respect to these concerns.''\citet{Cannon:2014im} uses ``industry-level capacity unit cost and selling price changes'' as instruments for firm-level capacity unit cost changes with no more justification than the fact that these ``are outside management's control.'' But being outside management control does not make a variable an adequate instrument.}

%In most remaining cases, the reasoning in support of the validity of an instrument is evidently flawed. 

%As an illustration, \citet{:2014fm} examine the interesting research question of how outside directors affect firm performance. One of their key variables is director tenure which they acknowledge as being endogenous. They then use director age as an instrument for director tenure. However, their justification for this instrument seem instead to provide reasons to believe that it is not valid. 
%``Importantly, research finds little or no association between age and performance \dots and a small negative association between age and executive functions \dots. 
%Related to directors, Ferris et al. (2003) suggest that any positive effects from director experience increasing with age may be offset by older directors having less energy, posing a last-period risk, and viewing directorships as lucrative part-time jobs for their retirement years.'' 
%But these arguments seem to invalidate age as an instrument for tenure. 
%For age to be a valid instrument, there should be no unblocked causal path between age and performance except for the path via tenure.
%That possible positive effects \emph{may} be offset by negative effects is not a valid basis for claiming age to be a valid instrument.  \citet{:2014fm} certainly provide some useful descriptive results regarding an important governance question.  Although we focus on this single paper, this critique can be broadly applied across most instrumental variable applications in accounting.  

%\footnote{We omit discussion of  \citet{Erkens:2014hj,Houston:2014hv} and \citet{deFranco:2014ct} for reasons of space. But in each case, the instruments have obvious flaws and no convincing arguments for their validity are offered (details available on request).}

\subsubsection{There are no simple (statistical) tests for the validity of instruments}

Some accounting researchers appear to believe that statistical tests can resolve the question of whether their instrument is ``valid."
% American usage requires that periods and commas go in quotes. Doesn't make sense to me (but I'm not American).
Indeed, many studies choose to test the validity of their instrumental variables using statistical tests \citep[see][]{Larcker:2010fq}.
% This claim sometimes takes the form of our selected instrument is correlated with (i) the endogenous variable and (ii) not correlated with the outcome variable of interest. 
%It is well-known that such tests lean heavily on the maintained assumption that the instrument is uncorrelated with the structural equation error.
% This assumption is hard to defend in practice absent a model that delivers the relationship being estimated.
%
%Although perhaps obvious, the standard statistical tests applied by authors using instrumental variables provide little insight into the quality of the chosen instruments. 
% \citet{Guedhami:2013cj} use $\textit{CAPITAL}$, an indicator for a firm being located in a capital city, as an instrument for political connectivity in a study looking at the effect of political connections on the use of a Big 4 auditor ($\textit{BIG 4}$).
%For example, the only justification \citet{Guedhami:2013cj} provide for their instrument is that ``importantly, the correlation between $\textit{CAPITAL}$ and $\textit{BIG 4}$ is small in our data set $(\rho = 0.05)$, helping to justify the validity of this exclusion restriction.''\footnote{
% \citet{Guedhami:2013cj} cite \citet{Larcker:2010fq} as a reference for this approach, even though \citet{Larcker:2010fq} carefully explain why simple tests like this cannot be used to justify instruments.}
%
%\citet{Correia:2014fp} is relatively thorough. \citet{Correia:2014fp} 
But such tests of instruments are of dubious value. 
Consider, for example, the following simulation of a setting where $X$ does not cause $y$, but we nevertheless estimate the regression $y = X \beta + \epsilon$. 
To make matters interesting, suppose $\rho(X, \epsilon) > 0$ (i.e., $X$ is correlated with the error). Clearly, if we estimated the equation by OLS, we would
conclude that there is a (positive) relationship between $X$ and $y$. Suppose that after being told that $X$ is ``endogenous", we found
three instruments: $z_1$, $z_2$ and $z_3$. Unbeknown to us, the three instruments were determined as follows:
$z_1 = X +\eta_1$, $z_2 = \eta_2$, and $z_3 = \eta_3$, with $\eta_1, \eta_2,  \eta_3 \sim N(0, \sigma_{\eta}^2)$ and independent. 
That is, $z_1$ is $X$ plus noise (e.g., industry averages or lagged values of $X$ would seem to approximate $z_1$), while $z_2$ and $z_3$ are random noise (many variables could be candidates here).

%\footnote{A paper using instruments with apparently similar properties is \citet{Correia:2014fp}. 
%\citet{Correia:2014fp} uses ``average level of political contributions made by the other firms in the same industry'' as an instrument for political contributions by a firm, as well as two additional instruments: ``the percentage of sales made to the government, and the number of years in the previous five years in which there was a close election involving two candidates in the firm's state.''  \citet{Reiss:2007ej} suggest that there is no reason to view industry averages as valid instruments.}
%Obviously, these ``instruments" are silly choices and completely inappropriate.

Assuming that $X$ and $\epsilon$ are bivariate-normally distributed with variance of $1$ and $\rho(X, \epsilon)=0.2$, and that $\sigma_{\eta}=0.03$, we performed 1,000 IV regression simulations with 1,000 firm-level observations in each case. 
Both the OLS and IV coefficients are close, with the IV estimated coefficient averaging $0.201$.
The IV coefficient estimates are statistically significant at the 5\% level 100\% of the time.\footnote{Note that this coefficient is close to $\rho(X, \epsilon) = 0.2$, which is to be expected given how the data were generated.} 
Based on a test statistic of 30, which easily exceeds the thresholds suggested by \citet{Stock:2002aa}, the null hypothesis of weak instruments is rejected 100\% of the time. 
The \citet{Sargan:1958aa} test of overidentifying restrictions fails to reject a null hypothesis of valid instruments (at the 5\% level) 95.7\% of the time.

This example illustrates why it is that no statistical test allows the researcher to verify that their instruments satisfy the exclusion 
restriction.\footnote{This is a corollary of the ``causal reasoning is not statistical reasoning" point made above.}
Obviously, causal inferences based on such instrumental variables is completely inappropriate.
Yet, this shows that it is quite possible for completely spurious instruments to deliver bad inferences, yet easily pass tests for weak instruments and tests of overidentifying restrictions. 

%\footnote{It is also common for accounting researchers to claim that they have established the validity of their instruments by implementing some type of Hausman test.  It is very clear that these types of overidentifying tests require the researcher to actually have one valid IV. 

%NOTE: Each of the other sub-sub-sections here has a title of the form of an observation we make. 
% So I changed this one to conform with this pattern.
\subsubsection{Causal diagrams can clarify causal reasoning}
To illustrate the application of causal diagrams to the evaluation of instrumental variables, we consider \citet{Armstrong:2013io}.
%
\citet{Armstrong:2013io} study the effect of shareholder voting (\textit{Shareholder support}$_{t}$) on future executive compensation ($\textit{Comp}_{t+1}$) .
Because of the plausible existence of unobserved confounding variables that affect both future compensation and shareholder support, a simple regression of $\textit{Comp}_{t+1}$ on \textit{Shareholder support}$_{t}$ and controls would not allow \citet{Armstrong:2013io} to obtain an unbiased estimate of the causal relation.
Among other analyses, \citet{Armstrong:2013io} use an instrumental variable to estimate the causal relation of interest.
\citet{Armstrong:2013io} claim that their instrument is valid. Their reasoning is represented graphically in Figure \ref{fig:agl}.
By conditioning on $\textit{Comp}_{t-1}$ and using Institutional Shareholder Services (ISS) recommendations as an instrument, \citet{Armstrong:2013io} argue that they can identify a consistent estimate of the causal effect of shareholder voting on $\textit{Comp}_{t+1}$, even though there is an unobserved confounder, namely determinants of future compensation observed by shareholders, but not the researcher.\footnote{
In Figure \ref{fig:agl}, we depict the unobservability of this variable (to the researcher) by putting it in a dashed box.
Note that we have omitted the controls included by \citet{Armstrong:2013io} for simplicity, though a good causal analysis would consider these carefully.}

While the authors note this possibility: ``validity of this instrument depends on ISS recommendations not having an influence on future compensation decisions conditional on shareholder support (i.e., firms listen to their shareholders, with ISS having only an indirect impact on corporate policies through its influence on shareholders' voting decisions)," they are unable to test the assumption \citep[p.\,912]{Armstrong:2013io}.
Unfortunately, this assumption seems inconsistent with the findings of \citet{Gow:2013aa}, who provide evidence that firms calibrate compensation plans (i.e., factors that directly affect $\textit{Comp}_{t+1}$) to comply with ISS's policies so as to get a favorable recommendation from ISS. 
As depicted in Figure \ref{fig:agl2}, this implies a path from $\textit{ISS recommendation}_t$ to $\textit{Comp}_{t+1}$ that does not pass through $\textit{Shareholder support}_{t}$, suggesting that the instrument of \citet[p.\,912]{Armstrong:2013io} is not valid.\footnote{\citet{Armstrong:2013io} recognize the possibility that the instrument they use is not valid and conduct sensitivity analysis to examine the robustness of their result to violation of the exclusion restriction assumptions. 
This analysis suggests that their estimate is highly sensitive to violation of this assumption.}
% While the descriptive results are informative for thinking about the impact of shareholder voting, it is difficult for {Armstrong:2013io} to draw a causal inference regarding how shareholder voting affects compensation plan design.
% Note: This was really a throwaway result not central to the paper, which mainly documented null results.

% BBKL discussion will go between here ...

% ... and here. Don't edit between these two lines.

\subsubsection{IV in accounting research: An evaluation}
A review of instrumental variable applications in our 2014 survey suggests that accounting researchers have paid little heed to the suggestions and warnings of  \citet{Larcker:2010fq,Lennox:2012it} and \citet{Roberts:2013cz}.
This is perhaps not surprising, as most studies do not have a theoretical model that can explain why a variable can naturally be excluded from
the equation of interest but still matter. 
Thus, while instruments work in theory, in practice there is a substantial burden of proof on researchers to justify appropriateness of making the stringent assumptions that IV estimators require.
 % \citet{Houston:2014hv} use variables variables that are related to the location of the company's headquarters as instruments for political connection and argue that ``these instruments should not be conceptually related to loan spreads. The key insight here is that the geographic locations of headquarters for companies are predetermined and are unlikely to affect banks' financing decision on loan costs. In summary, our identification assumption is that the costs of bank loans are not directly related to the companies' geographic locations, after controlling for a series of firm and loan characteristics'' (p.228). In justifying the relevance of the instrument, the authors seem eager to justify a connection, suggesting that ``the presumption is that the company's geographic location affects the company's ability to attract politically connected directors.'' But it far from clear why a company's geographic location would not also affect the its ability to attract directors with connections to \emph{financial institutions}, which plausibly affects financing terms directly \citep{Guner:2008tp}.\footnote{\citet{Houston:2014hv} also use firm age as an instrument, arguing that ``firm age affects a firm's incentive and capability in building up political connections''; but it is not clear why firm age would not also affect a firm's ``incentive and capability in building up'' financial connections.} 
 
% Researchers tend to very unclear about the determinants of their selected instruments.  In many cases, it seems that the instruments are also endogenous.  This makes it very difficult to to rule out the possibility that the instrument directly affects (or is correlated with) variables other than the endogenous variable of interest.
 % \citet{Erkens:2014hj} ``use the following three instrumental variables that capture the extent to which lenders are more likely to serve on a firm's board, which is studied for its potential effect on accounting conservatism. We use \emph{Industry importance to primary lender} because industry specialization increases the importance of acquiring information about a firm's industry, \emph{Primary lender within 50 mile radius} because physical proximity to lenders' headquarters reduces the cost of serving on the board, and \emph{Number of commercial banks within 50 mile radius} because the close proximity of multiple banks increases competition for board seats from other lenders.'' If industry specialization affects information-acquisition incentives, it seems it would do so through channels outside of board membership. With respect to the second instrument, it's quite likely that proximity affects information-gathering independent of service on the board. With respect to the third instrument, it is also implausible that the only direct effect of this variable is one on the service of bankers on the board (for example, this may lead to lower search costs in choosing potential lenders).

 %\citet{deFranco:2014ct} ``find that the number of covenants is positively related to the interest rate, likely due to endogeneity between the interest rate and covenants.'' To address this using they use ``the number of covenants by calendar year indicators as the instrument'' for the number of covenants. Apart from the issues with using an average as an instrument discussed in \citet{Reiss:2007ej}, the authors justify their instrument by suggesting that ``the strictness of covenant packages significantly deteriorated during the years of the credit boom that preceded the financial crisis.'' But it seems likely that the credit boom would have a direct effect on interest rates on bond issues. 

\subsection{Regression discontinuity designs}
Recently, RD designs have attracted the interest of accounting researchers, as a number of phenomena of interest to accounting researchers involve discontinuities. 
For example, whether an executive compensation plan is approved is a discontinuous function of shareholder support \citep[e.g.,][]{Armstrong:2013io} and whether a firm initially had to comply with provisions of the Sarbanes-Oxley Act was a discontinuous function of market float \citep{Iliev:2010ic} .

In discussing the recent ``flurry of research" using regression discontinuity (RD) designs in other fields, \citet[p.\,282]{Lee:2010hy} point out that they ``require seemingly mild assumptions compared to those needed for other nonexperimental approaches \dots and that causal inferences from RD designs are potentially more credible than those from typical `natural experiment' strategies."
% I would add a reference to the "first application" -- Thistlewaite, D.; Campbell, D. (1960). "Regression-Discontinuity Analysis: An alternative to the ex post facto experiment". Journal of Educational Psychology 51 (6): 309–317
%Imbens, G.; Lemieux, T. (2008). "Regression Discontinuity Designs: A Guide to Practice". Journal of Econometrics 142 (2): 615–635. 
While RD designs make relatively mild assumptions, in practice these assumptions may be violated.
In particular, manipulation of the running variable (or the variable that determines whether an observation is assigned to a treatment) may occur and researchers should carefully examine their data for this possibility \citep[see, e.g.,][]{Listokin:2008p5958,McCrary:2008ft}.

Another issue with RD designs is that the causal effect estimated is a local estimate (i.e., it relates to observations close to the discontinuity).
This effect may be very different from the effect at points away from the discontinuity.
For example, in designating a public float of \$75 million, the SEC may have reasoned that at that point the benefits of Sarbanes-Oxley were approximately equal to the fixed costs of complying with the law.
If true, we would expect to see an estimate of approximately zero effect, even if there were substantial benefits of the law for shareholders of firms having a public float well above the threshold.
% [Don't Follow -- Badly Worded]  Similarly, that a vote that receives approximately 50\% support may suggest that costs and benefits are approximately balanced, while %measures that receive much greater or much less support may have very different levels of benefits and costs, making estimates from RD designs relatively %
%uninformative of the average treatment effects.

Another critical assumption is the bandwidth used in estimation (i.e., in effect how much weight is given to observations according to their distance from the cutoff).
We encourage researchers using RD designs to employ methods that exist to estimate optimal bandwidths and the resulting estimates of causal effects \citep[e.g.,][]{Imbens:2011}.

%It is also important to note that so-called ``quasi-RD" designs have only a superficial resemblance to RD designs.
%For example, as he cannot observe ``the specific covenant thresholds in [his] primary dataset," \citet{Tan:2013ce} is constrained to estimate a ``quasi-RD" design like that estimated in \citet{Roberts:2009ka}.
% But this ``regression discontinuity design" is essentially ordinary-least squares with an indicator for covenant violation and thus does not represent a method for estimating unbiased causal effects with observational data.

%TODO: Seems like we want to include in this section:  plot the data and if you can't see it in the data, it probably is not actually there (Imbens), do not use the high level polynomial approach, and other similar issues.  Probably something on whether the magnitude of the results is actually believable -- Yonca's, the prize winning JF paper, and others results seem implausibly large for governance topics.
% This would be referring to specific papers.

\subsection{Other methods}

\subsubsection{Difference-in-difference and fixed effect estimators}
Accounting researchers have come to view some statistical methods as requiring fewer assumptions and thus being less subject to problems when it comes to drawing causal inferences. 
\citet[p.\,12]{Angrist:2010jv} include so-called difference-in-difference (DD) estimators on their list of such quasi-experimental methods, along with ``instrumental variables and regression discontinuity methods."\footnote{As \citet[p.\,228]{Angrist:2008vk} argue that ``DD is a version of fixed effects estimation," we discuss these methods together.}
Enthusiasm for DD designs perhaps stems from a belief that these are ``quasi-experimental" methods in the same sense as the other two approaches cited by \citet[p.\,12]{Angrist:2010jv}.
But the essential feature that instrumental variables and regression discontinuity methods rely on is the ``as if" random treatment assignment mechanism.
If treatment assignment is driven by unobserved confounding variables, then DD and fixed-effect estimates of causal effects will be biased and inconsistent. 
As few settings in accounting satisfy random treatment assignment, there is a heavy burden on researchers using DD or fixed-effect estimators to explain why they believe these methods allow them to recover unbiased estimates of causal effects.

%Even when assignment to treatment is random, care needs to be taken in interpreting estimates as causal effects.
%For example, \citet[p.\,1305]{Cadman:2014cr} conjecture that ``VCs have strong incentives to design compensation schemes that provide CEOs with short-horizon incentives in the fiscal years after the IPO." 
%The main analysis in support of this hypothesis is a regression DD analysis \citep[pp.\,233--241]{Angrist:2008vk} using the pre-IPO year as the pre-treatment period, and the two years after IPO as the post-treatment period. 
%However, one treatment of interest (i.e., being VC-backed) is likely implemented well before the ``pre-treatment" period, making it difficult to consider the pre-treatment values of the outcome variable as not being caused by the treatment.

\subsubsection{Propensity score matching}
Another method that has become popular in accounting research is propensity score matching (PSM).
Regression methods can be viewed as making model-based adjustments to address confounding variables.  
Stuart and Rubin (2007) argue that 

\begin{quote}\begin{singlespace} 
``[M]atching methods are preferable to these model-based adjustments for two key reasons. 
First, matching methods do not use the outcome values in the design of the study and thus preclude the selection of a particular design to yield a desired result.
Second, when there are large differences in the covariate distributions between the groups, standard model-based adjustments rely heavily on extrapolation and model-based assumptions.
Matching methods highlight these differences and also provide a way to limit reliance on the inherently untestable modeling assumptions and the consequential sensitivity to those assumptions."\end{singlespace} 
\end{quote}
For these reasons, PSM methods can prove useful when faced with observational data.
However, PSM does \emph{not} provide ``the closest archival approximation to a true random experiment" and does \emph{not} represent ``the most appropriate and rigorous research design for testing the effects of an ex ante treatment" \citep[p.\,1429]{Kirk:2014gx}.
\citet[pp.\,73-75]{Rosenbaum:2009ul} points out that matching is ``a fairly mechanical  task," and when assignment to treatment is driven by unobservable variables, PSM-based estimates may be biased as much as regression estimates.
We agree with \citet{MinuttiMeza:2014fn} who argues that ``matching does not necessarily eliminate the endogeneity problem resulting from unobservable variables driving [treatment] and [outcomes]."

%TODO: Add a general evaluation of the use of PSM for causal inferences in accounting 
%TODO: Expand survey to discuss use of PSM in accounting.
%TODO: Discuss papers matching on post-treatment variables.
%TODO: Find a home for discussion of bounding. I don't think it belongs with PSM.

\subsection{Quasi-experimental methods: An evaluation}
We agree that the revolution in econometric methods for causal inference represents an opportunity for accounting researchers.
However, we suggest caution. 
The assumptions required for these methods to deliver credible estimates of causal effects are unlikely to be met in many applications that rely on observational data. 
This suggests that researchers might temper their conclusions in light of the stringency of these assumptions.
Additionally, researchers should discuss how their conclusions are likely to generalize beyond the (special) environment in which they were found.

\section{Causal mechanisms, causal inference, and descriptive studies} \label{sec:mech}

In the first part of the paper, we have argued that, while causal inference is the goal of most accounting research, it is extremely difficult to find settings and statistical methods that can produce reliable estimates of causal effects. Does this mean accounting researchers must give up making causal statements? We believe the answer is no. There are viable paths forward. The objective of the second part of this paper is to discuss these paths forward. The first path we discuss is an increased focus on causal mechanisms. 
Accounting research is not alone in its reliance on observational data with the goal of drawing causal inferences. 
It is therefore natural to look to other fields using observational data to identify causal mechanisms and ultimately to draw causal inferences. Epidemiology and medicine are two fields that are often singled out in this regard. In what follows, we briefly provide examples and highlight the features of the examples that enhanced the credibility of the inferences drawn. A key implication of this discussion is that accounting researchers need to identify clearly and rigorously the causal mechanism that is producing the observed results in their research studies.

\subsection{John Snow and cholera}
A widely cited case of causal inference involves John Snow's work on cholera. 
As there are many excellent accounts of Snow's work, we will focus on the barest details.
As discussed in \citet[p.\,339]{Freedman:2009ur} ``John Snow was a physician in Victorian London. In 1854, he demonstrated that cholera was an infectious disease, which could be prevented by cleaning up the water supply. The demonstration took advantage of a natural experiment. A large area of London was served by two water companies. The Southwark and Vauxhall company distributed contaminated water, and households served by it had a death rate `between eight and nine times as great as in the houses supplied by the Lambeth company,' which supplied relatively pure water."
But there was much more to Snow's work than the use of a convenient natural experiment. 
First, Snow's reasoning (much of which was surely done before ``the arduous task of data collection" began) was about the mechanism through which cholera spread. Existing theory suggested ``odors generated by decaying organic material." 
Snow reasoned qualitatively that such a mechanism was implausible.
Instead, drawing on his medical knowledge and the facts at hand, Snow conjectured that ``a living organism enters the body, as a contaminant of water or food, multiplies in the body, and creates the symptoms of the disease. 
Many copies of the organism are expelled with the dejecta, contaminate water or food, then infect other victims" \citep[p.\,342]{Freedman:2009ur}.
 
With a hypothesis at hand, Snow then needed to collect data to prove it. 
His data collection involved a house-to-house survey in the area surrounding the Broad Street pump operated by Southwark and Vauxhall.
As part of his data collection, Snow needed to account for anomalous cases (such as the brewery workers who drank beer, not water).
It is important to note that this qualitative reasoning and diligent data collection were critical elements establishing (to a modern reader) the ``as if" random nature of the treatment assignment mechanism provided by the Broad Street pump. 
Snow's deliberate methods contrast with a shortcut approach, which would have been to argue that in his data he had a natural experiment. 
 
Another important feature of this example is that widespread acceptance of Snow's hypothesis did not occur until compelling evidence of the precise causal mechanism was provided. 
 ``However, widespread acceptance was achieved only when Robert Koch isolated the causal agent (\emph{Vibrio cholerae}, a comma-shaped bacillus) during the Indian epidemic of 1883" \citep[p. 342]{Freedman:2009ur}.
 Only once persuasive evidence of a plausible mechanism was provided (i.e., direct observation of microorganisms now known to cause the disease) did Snow's ideas become widely accepted. 
 
We expect the same might be true in the accounting discipline if researchers carefully articulate the assumed causal mechanism for their observations.
It is, of course, necessary for researchers to show that the proposed mechanism is actually consistent with behavior in the institutional setting being examined.
As we discuss below, detailed descriptive studies of institutional phenomenon provide an important part of the information to evaluate the proposed mechanism. 

\subsection{Smoking and heart disease}
A more recent illustration of plausible causal inference is discussed by \citet{Gillies2011-GILTRT-3}.
\citet{Gillies2011-GILTRT-3} focuses on the paper by \citet{Doll:1976aa}, which studies the mortality rates of male doctors between 1951 and 1971. 
The data of \citet{Doll:1976aa} showed ``a striking correlation between smoking and lung cancer" \citep[p. 111]{Gillies2011-GILTRT-3}.
 \citet{Gillies2011-GILTRT-3} argues that ``this correlation was accepted at the time by most researchers (if not quite all!) as establishing a causal link between smoking and lung cancer."
Indeed Doll and Peto themselves say explicitly (p.\,1535) that the excess mortality from cancer of the lung in cigarette smokers is caused by cigarette smoking."
In contrast, while \citet{Doll:1976aa} had highly statistically significant evidence of an association between smoking and heart disease, they were cautious about drawing inferences of a direct causal explanation for the association. 
\citet[p. 1528]{Doll:1976aa} point out that ``to say that these conditions were related to smoking does not necessarily imply that smoking caused \dots them. The relation may have been secondary in that smoking was associated with some other factor, such as alcohol consumption or a feature of the personality, that caused the disease."
 
\shortcites{Morrow:1995gz}
\citet{Gillies2011-GILTRT-3} then discusses extensive research into atherosclerosis between 1979 and 1989 and concludes that ``by the end of the 1980s, it was established that the oxidation of LDL was an important step in the process which led to atherosclerotic plaques."
Later research provided ``compelling evidence that smoking causes oxidative modification of biologic components in humans.\footnote{This evidence is much higher levels of a new measure (levels of $F_2$-isoprostanes in blood samples) of the relevant oxidation in the body due to smoking.
This conclusion was greatly strengthened by the finding that levels of $F_2$-isoprostanes in the smokers fell significantly after two weeks of abstinence from smoking" \citep[pp.\,1201--2]{Morrow:1995gz}.}
\citet[p.\,120]{Gillies2011-GILTRT-3} points out that this evidence alone did not establish a confirmed mechanism linking smoking with heart disease, because the required oxidation needs to occur in the artery wall, not in the blood stream, and it fell to later research to establish this missing piece.\footnote{
``Smoking produced oxidative stress. 
This increased the adhesion of leukocytes to the \dots artery, which in turn accelerated the formation of atherosclerotic plaques" \citep[p.\,123]{Gillies2011-GILTRT-3}.}
Thus, through a process involving multiple studies over two decades, a plausible set of causal mechanisms between smoking and atherosclerosis was established. 

\citet{Gillies2011-GILTRT-3} avers that the process by which a causal link between smoking and atherosclerosis was established illustrates the ``Russo-Williamson thesis."
 \citet[p.\,159]{Russo:2007iz} suggest that ``mechanisms allow us to generalize a causal relation: while an appropriate dependence in the sample data can warrant a causal claim `$C$ causes $E$ in the sample population,' a plausible mechanism or theoretical connection is required to warrant the more general claim `$C$ causes $E$.' 
Conversely, mechanisms also impose negative constraints: if there is no plausible mechanism from $C$ to $E$, then any correlation is likely to be spurious.
Thus mechanisms can be used to differentiate between causal models that are underdetermined by probabilistic evidence alone."

The Russo-Williamson thesis was arguably also at work in the case of Snow and cholera, where the establishment of a mechanism (i.e., \emph{Vibrio cholerae}) was essential before the causal explanation offered by Snow was widely accepted.
It also appears in the case of smoking and lung cancer, which was initially conjectured based on correlations, prior to a direct biological explanation being offered.\footnote{
The persuasive force of Snow's natural experiment, coming decades before the work of \citet{Neyman:1923aa} and \citet{Fisher:1935aa}, might be considered greater today.}

\subsection{Causal mechanisms in accounting research}
Our view is that accounting researchers can learn from fields such as epidemiology, medicine, and political science. 
These fields grapple with observational data and eventually draw inferences that are causal. 
While randomized controlled trials are a gold standard of sorts in epidemiology, in many cases it is unfeasible or unethical to use such trials.
For example, in political science, it is not possible to randomly assign countries to treatment conditions such as democracy or socialism.
Nevertheless, these fields have often been able to draw plausible causal inferences by establishing clear mechanisms, or causal pathways, from putative causes to putative effects. 

One paper that has a fairly compelling identification strategy is \citet{Brown:2015ik}, which examines ``the influence of mobile communication on local information flow and local investor activity using the enforcement of state-wide distracted driving restrictions."
The authors find that ``these restrictions \dots inhibit local information flow and \dots the market activity of stocks headquartered in enforcement states."
\citet[p.\,229]{Miller:2015ec} suggest that ``given the authors' setting and research design, it is difficult to imagine a story under which the types of reverse causality or correlated omitted variables explanations that we normally worry about in disclosure research are at play."
However, notwithstanding the apparent robustness of the research design, the results would be much more compelling if there were more detailed evidence regarding the precise causal mechanism through which the estimated effect occurs and the authors appear to go to lengths to provide such an account.\footnote{
\citet[pp.\,277-278]{Brown:2015ik} ``argue that constraints on mobile communication while driving could impede or delay the collection and diffusion of local stock information across local individuals. 
Anecdotal evidence suggests that some individuals use car commutes as opportune times to gather and disseminate stock information via mobile devices. 
For instance, some commuters use mobile devices to collect and pass on stock information either electronically or by word-of-mouth to other individuals within their social network.
Drivers also use mobile devices to wirelessly check stock positions and prices in real-time, stream the latest financial news, or listen to earnings calls."}
For example, evidence of trading activity by local investors while driving prior to, but not after, the implementation of distracted driving restrictions would add considerable support to conclusions in \citet{Brown:2015ik}.\footnote{
Note that the authors disclaim reliance on trading while driving: ``our conjectures do not depend on the presumption that local investors are driving when they execute stock trades \dots [as] we expect such behavior to be uncommon."
However, even if not \emph{necessary}, given the small effect size documented in the paper (approximately 1\% decrease in volume), a small amount of such activity could be \emph{sufficient} to provide a convincing account in support of their results.}


As another example, many published papers have suggested that managers adopt conditional conservatism as a reporting strategy to obtain benefits such as reduced debt costs.
However, as  \citet[p.\,317]{Beyer:2010cj}  point out, an ex-ante commitment to such a reporting strategy ``requires a mechanism that allows managers to credibly commit to withholding good news or to commit to an accounting information system that implements a higher degree of verification for gains than for losses," yet research has only recently begun to focus on the mechanisms through which such commitments are made \citep[e.g.,][]{Erkens:2014hj}. 

It is very clear that we need a much better understanding of the precise causal mechanisms for important accounting research questions.
A clear discussion of these mechanisms will enable reviewers and readers to see what is being assumed and assess the reasonableness of the theoretical causal mechanisms.

\subsection{Descriptive studies} \label{sec:desc}

Accounting is an applied discipline and it would seem that most empirical research studies should be solidly grounded in the details of how institutions operate.
These descriptions can form a basis for identifying and justifying causal mechanisms for explaining empirical results.
Unfortunately, there are very few studies published in top accounting journals that focus on providing deep description of institutions relevant to accounting research settings.
Part of this likely reflects the perception that research that pursues causal questions (i.e., tests of theories) is more highly prized, and thus more likely to be published in top accounting journals.\footnote{
At one point, the \emph{Journal of Accounting Research} published papers in a section entitled ``Capsules and Comments."
The editor at the time (Nicholas Dopuch) would seem to place a paper into this section if it ``did not fit" as a main article, but examined new institutional data or ideas. 
Such a journal section might have provided a credible signal of a willingness to publish descriptive studies of institutionally interesting settings.}
We believe that accounting research can benefit substantially from more in-depth descriptive research.
As we discuss below, this type of research is essential to improve our understanding of causal mechanisms and develop structural models.\footnote{
There are many ``classic" descriptive studies that have had a major impact on subsequent theoretical and empirical research in organizational behavior and strategy \citep[e.g.,][]{Cyert:1956fd,Bower:1986vd,Mintzberg1973nature}.
\citet{Cyert:1956fd} argue that ``a realistic description and theory of the decision-making process are of central importance to business administration and organization theory. Moreover, it is extremely doubtful whether \dots economics does in fact provide a realistic account of decision-making in large organizations operating in a complex world."}

One reason to value descriptive research is that it can uncover realistic structures and mechanisms that would be exceedingly difficult to arrive at from basic economic theory or the simple intuition of the researcher.
In the compensation area, the early research by \citet{Lewellyn:1968aa} and the more recent work by \citet{Frydman:2010bc} are also essentially descriptive studies that caused researchers to explore why certain patterns of remuneration arrangements are used, revised, or eliminated over time.
These types of data motivate researchers to frame research studies that have the potential to uncover the causal mechanisms that produce these institutional observations.

A good example in the accounting literature is the study by \citet{Healy:1985jg}.  
Using proxy statement disclosures and conversations with actual executives and consultants, \citet{Healy:1985jg} studies the bonus contracts of 94 large US companies and identifies a common structure of these bonus plans, including the existence of caps and floors. 
The paper also suggests hypotheses worth investigating regarding the effects of these plan features on accounting decisions.
It seems highly unlikely that a model derived from fundamental economic theory would arrive at these plan features that are actually used by firms. 

In the same way, the descriptive examination of debt covenants by \citet{SmithJr:1979hv}, \cite{Kalay:1982gv}, and many others motivated a similar research agenda.
Institutional knowledge about the features of covenants provided the basis of tests regarding managerial wealth transfer and accounting manipulation decisions by managers.
Moreover, descriptive statistics regarding covenants also provided \cite{Dichev:2002} with the data to show that leverage is not a valid proxy for ``closeness to covenant."
This is an important finding because the empirical literature to this point simply assumed that leverage was a reliable and valid measure for potential covenant violations.  
An in-depth examination of actual debt covenants and an understanding of how covenant violations are dealt with by financial institutions would have substantially improved much of the research on how debt covenants influence firm behavior (i.e., so-called ``positive theory" research).

In the corporate governance area, the descriptive data on board of director interlocks in \citet{brandeis1913breaking}, \citet{us1951report}, and \cite{united1978interlocking} provided novel descriptive insights into the structure of boards of directors.
These and other similar studies had an important impact on starting the large literature on the functioning of boards of directors. 
Similarly, the initial collection of equity ownership by executives, directors, and large shareholders by the \cite{securities1936official} enabled researchers to understand the extent to which ownership is separated from control and examine the implications of the classic \cite{berle1932modern} hypotheses regarding economic activity.

Similarly, the descriptive data on anti-takeover provisions collected by the Investor Responsibility Research Center (IRRC) has provided the basis for a considerable amount of research on the market for corporate control. 
\cite{Gompers:2003tl}, \cite{Bebchuk:2009ii}, and many others use these data to form and test a multitude of research questions related to corporate governance.  
Perhaps more importantly, \cite{Daines:2001hi} provided an institutionally grounded examination of how these specific anti-takeover provisions actually work from a legal perspective (which contrasts with conjectures made by researchers in other disciplines).  
The \cite{Daines:2001hi} analysis provides a good example of how descriptive data combined with institutional and legal knowledge can provide appropriate insights into the workings of corporate governance.

The descriptive disclosure data compiled by the Association for Investment Management and Research (AIMR) has had a similar impact on financial accounting research. 
These ratings reflect assessments of analysts specializing in specific industries about the informativeness of disclosures made by firms in their respective industries.
Analysts evaluate the timeliness, detail and clarity of the corporate disclosure, and various disclosure scores are computed. 
These data provided a variety of useful descriptive statistics regarding differences in disclosure across firms, industries, and time.
We suspect that these statistics were instrumental in motivating \citet{Lang:1993iv,Lang:1996dk}, \cite{Healy:1999ig}, and many others to provide new insights into whether firm disclosure is associated with performance, consensus among investors, stock liquidity, and other important outcome variables.
In related work, \cite{Groysberg:2011dk} provide an informative analysis of how analysts are compensated using descriptive proprietary data and statistical analyses to uncover the fundamental features of the reward system.

Recent published research suggests an increased recognition of the value of descriptive research. 
\citet{Soltes:2013ba} examines the interactions between sell-side analysts and company management in one firm that granted proprietary access to its data to ``offer insights into which analysts privately meet with management, when analysts privately interact with management, and why these interactions occur."
By comparing private interaction to observed interaction between analysts and managers on conference calls and highlighting that private interaction with management is an important communication channel for analysts, \citet{Soltes:2013ba} provides a plausible mechanism through which information transfers hypothesized in more traditional empirical papers actually occur. 

That private communication with management is an important source of information is confirmed by \citet{Brown:2015kd}. 
\citet{Brown:2015kd} survey and interview financial analysts to understand how they think about a variety of issues. 
Their findings suggest that analysts' views on earnings quality differ from those researchers focus on. 
For instance, analysts do not use the ``red flags" used by academics to identify manipulation; and analysts generally are not attempting to uncover manipulation and use forecasts, not as ends in themselves, but to figure out the stock price target. 
These insights should shape research seeking to develop hypotheses and models of accounting information and analyst behavior. 

Despite the dearth of descriptive research in top accounting journals, we believe that our discipline can benefit substantially from this style of research. 
An interesting question is what makes a descriptive study an important contribution that should be published in a top journal.
An obvious required attribute is that the descriptive study examines an interesting institutional question where researchers care about understanding the phenomenon producing the observations.
Stated differently, would anyone change their research agenda or their (causal) interpretations of prior work if provided with these descriptive results?

The descriptive research needs to be neutral and unbiased in terms of data collection and interpretations.
If expert opinions are used, can we be assured that the opinions are not biased because of their business dealings?
Data collected using surveys or interviews by consulting firms may provide great descriptive data, but researchers need to be convinced that the data are not confounded by selection bias and other similar concerns.

The research should also provide deep insight into the causal mechanisms underlying observed institutional data.
There may well be alternative mechanisms suggested by the research, and these alternatives may be a function of nuances and contextual variables for the setting.
This is an especially important outcome from descriptive studies because there are likely to be different causal mechanisms depending on the precise attributes of the setting.  

Obviously, the evaluation of descriptive research is somewhat subjective, but the evaluation of more traditional accounting research is similarly subjective. 
As a group, we do not have much experience assessing the likely contribution of descriptive research and we are unfamiliar with some of the research methods that are used in descriptive research. 
However, given the possibility that descriptive research can help us identify actual causal mechanisms for accounting phenomenon, it would seem that our profession should be willing to use some journal space for high quality descriptive work.

\section{Structural modeling} \label{sec:struct}

\subsection{Structural modeling: An overview}

In Sections \ref{sec:causal} and \ref{sec:quasi}, we suggested that researchers minimally consider using diagrams to communicate the basis for their causal inferences, and in Section \ref{sec:mech} we suggested that researchers be more precise in describing how their data permit causal
inferences.
This section explores a formal approach to developing a causal model, 
namely the ``structural" approach.
Structural models are empirical models that are derived from theoretical models of behavior.
The term structural model originated with economists and statisticians working at the Cowles Foundation in the 1940s and 1950s.
The earliest structural models used economic models of consumer and producer behavior to derive demand and supply equations.
By adding an equilibrium condition, such as quantities demanded equal quantities supplied, economists obtained a set of mathematical equations that could be used to understand movements in observed prices and quantities. A question then arose as to whether economists could reverse engineer this modeling process and use observed prices and quantities to recover the underlying demand and supply relations.
The models made it clear that the empiricist could only recover estimates of the unobserved demand and supply equations if certain exogenous (instrumental) variables were available. 

The impact of these early models on empirical work in economics encouraged other social scientists to begin using theoretical models to interpret data. 
Structural models have found widest application in situations where causality is an issue, such as the determinants of educational choices, voting, contraception, addiction, and financing decisions. 
Other applications of structural models are discussed in \citet{Reiss:2007ej} and \citet{Reiss:2011go}.

A structural empirical model comprises a theoretical model of the phenomenon of interest and a stochastic model that links the theoretical model to the observed data.
The theoretical model minimally describes who makes decisions, the objectives of decision makers, and constraints on their behavior.
In developing and analyzing the theoretical model, the researcher decides what conditions (variables) matter and what is endogenous and exogenous.
While the theoretical model typically draws on economic principles, it could also be derived from behavioral theories in other fields, such as psychology and sociology.\footnote{Some
researchers refer to any mathematical model fit to data as a structural model. For instance,
one might assume that the number of restatements in an industry follows a Poisson process
and then fit the parameters of the Poisson model using industry-level data on restatements.
We do not view such models as structural because they lack specific behavioral or institutional components that permit a causal inference.
We would classify this approach as descriptive or statistical modeling.}

Structural models offer a number of benefits for empirical researchers.
First, structural modeling is a process that forces a researcher to make explicit assumptions about what determines behavior and outcomes (i.e., the casual mechanism). 
Second, structural models make it clear what data are needed to identify unobserved parameters and random variables, such as coefficients of risk aversion.
Third, structural models provide a foundation for estimation and inference. 
Finally, structural models facilitate counterfactual analyses, such as what might happen under conditions not observed in the data. 
% I might add a set of limitations here using Welch as a framework -- 
To illustrate these benefits, as well as some of their limitations, we next explore an accounting application.

\subsection{Structural models in accounting: An illustration}
This section develops a model of managerial incentives to misstate accounting information. 
This topic has been the focus of many papers in recent years \citep[see][]{Armstrong:2010jd}.
The key question in this literature is whether certain kinds of managerial incentives increase the tendency for managers to misstate (or attempt to misstate) financial information.
A number of papers hypothesize that tying managers' compensation to the information that they provide will increase their desire to misstate that information.
However, some researchers suggest that, by aligning the long-term interests of shareholders and managers, certain kinds of incentives could actually \emph{reduce} misstatements \citep{Burns:2006ce}.

\citet{Efendi:2007ja} illustrates a fairly typical descriptive empirical paper in this literature. 
\citet[p.\,687]{Efendi:2007ja} estimate a logistic regression with an indicator for restatements as the dependent variable and measures of CEO incentives as independent variables of interest, along with controls for firm size, financial structure, and corporate governance proxies.\footnote{
\citet{Efendi:2007ja} also employ a case-control design, which involves matching firms with restatements with firms without.
We do not focus on that aspect of their research design in our discussion here.}
 
A key assumption implicit in much of this literature is that restatements are a good proxy for actual \emph{mis}statements \citep[e.g.,][]{Efendi:2007ja,Armstrong:2010jd}. 
This assumption is made because in practice accounting researchers only observe misstatements that are detected and corrected by external monitors after the financial statements were issued. 
Examples of these external monitors include whistleblowers, regulators, media, and others \citep[e.g.,][]{Dyck:2010kh}.  
For simplicity,  we refer to the actions of these external monitors collectively as ``subsequent investigations."
If subsequent investigations are perfect and detect all misstatements, then there is a one-to-one correspondence between misstatements and restatements.\footnote{There will still be a difference between \emph{attempted} misstatements and actual misstatements due to the external auditor correcting some attempted misstatements.}
Realistically, these subsequent investigations are not perfect, meaning that we need to recognize the difference between misstatements and restatements when estimating the effect of managerial incentives on misstatements.  

In the following analysis, we consider two alternative models of the causal mechanism linking managerial incentives to accounting restatements.
Each model explicitly considers the incentives of the manager and the role of the external auditor. 
The two models, however, lead to different conclusions about how CEO incentives affect
restatements. 
These differences permit us to illustrate the value of having a theoretical model that can interpret competing empirical estimates, as well as the difficulty of interpreting estimates in the absence of such models.

\subsubsection{Model 1: A non-strategic auditor model}
We assume that firm misstatements are deliberate and are made by a single agent, whom we refer 
to as the `CEO.' 
The CEO is assumed to be rational in the sense that he or she trades off private expected benefits and costs of misstatements when deciding whether to misstate.
Specifically, suppose that the CEO receives a benefit of $B^*$ from the \emph{successful} manipulation of earnings (i.e., a misstatement that is not detected either by the firm's auditors before a report is released or by subsequent investigations). 

Besides the CEO, we assume that the firm's auditors independently detect and correct attempted misstatements at a constant rate $p_A$ and that the (conditional) probability of subsequent investigations catching a misstatement is $p_I$.
Given these assumptions, the probability of a misstatement getting past the firm's  auditor and subsequent investigations is $(1-p_A) \times (1 - p_I)$.
The CEO's expected benefit from a successful misstatement is then
$$ B^* = (1-p_I) \times (1-p_A) \times B$$
where $B$ is a gross benefit to the manager from a misstatement. 

Assume the CEO must exert a fixed cost of effort $C_M$ in order to misstate performance. 
Combining this cost with the manager's expected benefits from of misstatement gives
\begin{equation}\label{bencost}
		y_M^* = 
		  \begin{cases}
				\mbox{Misstate} & \mbox{if }\, (1-p_I) \times (1-p_A) \times B - C_M \ge 0\\
				\mbox{Don't misstate,} & \mbox{otherwise}.
		  \end{cases}.
\end{equation}
This (structural) inequality describes the unobserved misstatement process. 
In general, researchers will not observe the structural parameters of
interest: $B$, $C_M$, $p_A$, or $p_I$.

To complete the structural model and recover these parameters, 
the researcher must add assumptions that relate the parameters to the data available.
Suppose we only observe a (zero-one) indicator variable $y$ for restatements.
These restatements are the result of three decisions:
\begin{enumerate}
\item The manager misstates (or not).
\item The firm auditor detects and corrects an attempted misstatement (or not) .
\item A subsequent investigation detects a misstatement and a restatement occurs (or not).
\end{enumerate}

Mathematically, this sequence can be modeled as
\begin{equation}\label{restateeqn}
 y = I(\mbox{Restate}) = I(y^*_M \ge 0) \, \times\, (1 - I(y^*_A \ge 0)) \, \times\, I(y^*_I \ge 0)
\end{equation}
where $I(\cdot)$ is a zero-one indicator function equaling one when the condition in parentheses is true.
The unobserved variables $y^*_M$, $y^*_A$ and $y^*_I$ reflect the criteria that underlie the CEO's, 
the firm's auditor's, and subsequent investigators' decisions.
Notice that equation (\ref{restateeqn}) uses $(1 - I(y^*_A \ge 0))$, an indicator for the firm's auditor missing the misstatement.

Equation (\ref{restateeqn}) somewhat resembles a traditional binary discrete choice model. The easiest
way to see this is to take expectations (from the researcher's standpoint).
Assuming the decision variables are independent,
\begin{equation} \label{equilpr}
\begin{array}{lcl}
 E(y) & = & E\, \left[\; I(y^*_M \ge 0) \, \times\, (1 - I(y^*_A \ge 0)) \, \times\, I(y^*_I \ge 0) \; \right]\\[1em]
 & = &  \mbox{Pr(Misstate)} \times \mbox{Pr(Auditor Misses)} \times
\mbox{Pr(Investigation Finds)}\\[1em]
& = & \beta^* \times (1-p_A) \times p_{I} = \mbox{Pr(Restate)} ,
\end{array}\end{equation}
where $\beta^*$ is the (researcher's) forecasted probability that a misstatement occurs, or, from equation (\ref{bencost}),
\begin{equation}\label{betaplus}
\beta^*= \mbox{Pr}\left(\, (1 - p_A)(1 - p_I) B - C_M \ge 0 \,\right)
\end{equation}

At this point, the theory has delivered a structure for relating the \emph{unobserved} probability of a misstatement, $\beta^*$, to the potentially estimable probability of a restatement.
Now we face a familiar structural modeling problem, which is that the model does not anticipate all the reasons why in practice these probabilities might vary across firm accounting statements.
For example, the theory so far does not point to reasons why CEOs might differ in their benefits and costs of misstatements. 
To move theoretical relations closer to the data, researchers typically allow parts of the model to depend on differentiating variables.
Often the specifications of these dependencies are ad hoc. 
Empiricists are willing to do this, however, because they believe that it is important to account for practical aspects of the application that the theory does not recognize.
% IDG: This is pretty unsatisfactory, of course. If "ad hoc" means "not derived from the theory being used," then I'd say that most modeling will be "ad hoc";  but I think that economic theory leaves you well short of the data in pretty much any accounting application. 
% In this case, "ad hoc" should be informed by detailed understanding of the setting.

To illustrate this approach, and following suggestions of what might matter from the
accounting research literature, suppose the CEO's unobserved costs and benefits vary as follows:
%
\begin{equation}
\begin{array}{lcl}\label{eqns1}
B & = & b_0 + b_1 \, \mbox{EQUITY} + X_B\beta\\[.5em]
C_M & = & m_0 + m_1 \, \mbox{SALARY} + X_C\gamma + \xi \text{,} % \\[.5em]
\end{array}
\end{equation}
where EQUITY is the fraction of a CEO's total pay that is stock-based compensation, 
the $X_B$ are other observable factors that impact the manager's benefits from misstatements,
SALARY is the CEO's annual base salary, and the $X_C$ are observable factors impacting the CEO's perceived costs of misstatements.\footnote{
For expositional purposes, we assume away $X_B$ and $X_C$ in our analysis.}
The EQUITY variable is intended to capture the idea that, the more a CEO is rewarded for performance, the greater his or her incentive to misstate results so as to increase (perceived) performance.
Thus, we would expect the unknown coefficient $b_1$ to be positive if providing more equity incentives increases the tendency of the CEO to misstate earnings, but expect $b_1 < 0$ if it reduces that tendency.
Similarly, we include the variable SALARY as a driver of the cost of making misstatements, with the idea that a CEOs caught misstating might lose their job, including salary (and other benefits).
% IDG: Of course, this doesn't really fit the model.
% I want to say that -- The idea is that if the CEO is caught manipulating, the personal penalty or loss borne by the CEO is proportional to his or her compensation. However, this does not seem consistent with the model because the manager always bears this cost regardless of whether he manipulates or not.  Any other justification for this variable come to mind?  Maybe if the salary is high, this means that the CEO is really important and it is more costly for him to fool around with manipulating, as opposed to working on strategy development
Thus, we would expect the unknown coefficient $m_1$ also to be positive.
For now, we leave the other $X$ variables unnamed.

We have no strong theoretical reason for the assumption of linearity. Its motivation is practical, as it facilitates estimation of the model unknowns (as we shall shortly see).\footnote{Another key variable in the above model is the unobserved cost $\xi$.
While it makes sense to say that the researcher cannot measure all misstatement costs, why not also allow for unobserved benefits as well?
The answer here is that adding an unobserved benefit would not really add to the model as it is the net difference that the model is trying to capture.
The sense in which it could matter is if we thought we observed the probabilities $p_A$ and $p_I$.
In this case, we might be able to distinguish between the cost and benefit unobservables based on their variances.}
 
With these assumptions, the probability of a restatement becomes
\begin{equation} \label{restate1}
\mbox{Pr(Restate)} = \theta_0 \mbox{Pr}\left(\, \theta_1 + \theta_2 \mbox{EQUITY} + \theta_3 \mbox{SALARY}  \ge \xi \,\right) .
\end{equation}
The new $\theta$ parameters are functions of the underlying incentive parameters as
follows: $\theta_0=(1-p_A) \times p_I, \theta_1 = (1 - p_A)(1 - p_I) b_0 - m_0, 
\theta_2 = (1 - p_A)(1 - p_I) b_1,$ and $\theta_3 = - m_1$. 
Apart from the scalar multiple $\theta_0$, which can be absorbed into the probability statement (and thus is not identified), this probability model has the form of  a familiar binary choice (e.g., a probit or logit model).
Thus, the value of the structure imposed so far is that it can motivate the application of a familiar statistical model \citep[as in][]{Efendi:2007ja}, as well as explain how the estimated coefficients are potentially connected to quantities that impact the probability of a misstatement.

%Do we want to say something about whether knowledge of the theta parameters can be used to infer the b's.  For examples, the ratio of theta 2 to theta 1 will be b1 divided by b0.  If the model is correct, we might be able to recover the underlying structural parameters.

\subsubsection{Estimation the non-strategic auditor model}

To illustrate how to estimate this structural model, we simulated a data set containing 10,000 firm-year observations on whether or not financial results were restated.\footnote{The parameter values used to generate the data are: $a_0 = 0.5 , a_1 = 3.5 , a_2 = 3.5,
m_0 = 7, m_1 = 1.5, b_0 = 20, b_1 = 10, p_0 =0.75,
v_0 = 0.05, p_I = 0.45$ and $r_0 = 60$.
For those interested, the data are available at \url{http://web.stanford.edu/~preiss/Data_page.html}}

For verisimilitude, we included variables that have previously been used to model restatements. 
RESTATE is a zero-one indicator variable for whether a firm restated (RESTATE = 1) their financial results in a given year.  
%TODO: Provide a reference for BIG4 in this kind of regression.
The variable BIG4 also is a zero-one indicator for whether the firm's auditor is one of the four largest U.S. accounting firms. 
It is included in the specifications because Big 4 auditing firms might have more accounting expertise and this expertise might make them more likely to catch misstatements. 
Similarly, the corporate governance literature suggests that board oversight from directors with accounting or finance backgrounds reduces the likelihood of misstatements. We proxy this possibility with FINDIREC, the percentage of directors that have professional accounting or finance backgrounds.
%TODO: Get refs.
Finally, the variables INT and SEG are included to capture the complexity and costs of audits. 
%TODO: From AAZ: Link to variable definitions.
%TODO: Get reference for audit model.
Specifically, INT is a zero-one indicator for whether the firm does a majority of its business outside the U.S.
We assume that international companies have higher auditing costs. 
Similarly SEG is a count of the firm's business segments. 
We assume that more segments likely will increase the costs of auditing. 

Table \ref{tab:desc} reports descriptive statistics for our sample. 
CEOs on average receive about one million dollars in base pay and their equity-related pay averages 45\% of their total pay.
About three-quarters of the sample has a Big 4 accounting firm as its auditor. 
The fraction of directors with financial expertise is less than ten percent. 
The average firm has about 1.5 business segments and is primarily based in the United States.

Table \ref{tab:logit} reports the results of logit regressions in which the dependent variable is the restatement indicator variable.
These specifications parallel prior descriptive statistical models that correlate restatements with other variables that might impact misstatements.
The table contains both a simple specification containing an intercept along with the two CEO  compensation variables, and a more intricate specification involving the other variables in the dataset.
For each specification, we report the estimated coefficients of the logit and the corresponding marginal effects evaluated at the sample means of the exogenous variables.

The results for the pay coefficients in both specifications run counter to those the previous accounting literature might predict and counter to those predicted by the structural model that assumes the benefit coefficient on equity pay, $b_1$, is greater than zero.
Specifically, more base pay is associated with more restatements, while more equity-based 
compensation is associated with fewer restatements.

Besides the intercepts and the EQUITY and SALARY coefficients, the only other coefficients that are statistically significant are those on INT and SEG.
While we can say (descriptively) that INT and SEG are associated with higher restatement rates, unless we take a position on how they enter $X_C$ or $X_B$, it is difficult to interpret whether these signs make sense.

The question we now address is what to make of the fact that the coefficients on EQUITY seem inconsistent with our informal arguments and with the prediction from our structural model that assumes $b_1>0$.
One possible interpretation of this finding is that the our beliefs about the effects of  incentives on misstatements were wrong.
Another possibility is that the measures we employ and the functional forms assumed are incorrect, which leads to spurious results.
Yet another possibility is that our theory of misstatements is incorrect.
It is this last possibility that we consider here.

\subsubsection{Model 2: A strategic auditor model}
A key weakness of the previous model is that it ignores the incentives of the  external auditor.
According to PCAOB guidance in Auditing Standard No. 12, assessment of the risk of material misstatement should take into account ``incentive compensation arrangements."
Similarly, Auditing Standard No. 8 suggests that audit effort should increase if risk is higher.
To make the model richer in a manner consistent with these institutional details, we assume that auditors trade off the costs of audit effort against the reputational losses they might incur should they miss a managerial misstatement that is subsequently detected.\footnote{
Here we have in mind the findings of \citet{Dyck:2010kh} who show that many egregious forms of misstatements are detected subsequently by employees, directors, regulators, and the media.} 

In the previous model, the firm's auditor impacted the manager's misstatement benefits through $p_A$ (which is assumed to be constant). 
Suppose that $p_A$ is in fact a choice variable for the firm's auditor. 
To make matters simple, suppose that the auditor detects manipulation with probability $p_{AH}$ if they exert high effort and  otherwise they detect manipulation with the lower probability $p_{AL}$. 
Let the cost of high effort be a fixed cost $C_A > 0$. 
Without loss of generality suppose the cost of low effort is zero. 
When deciding whether to audit with high or low effort, the auditor perceives a cost to its reputation, $C_R$, due to not detecting a misstatement that is caught by subsequent investigations. 
This structure implies that the total cost of high effort to the auditor is $C_A + (1-p_{AH}) \times p_I \times C_R$ or the cost of high effort plus the expected cost of missing a misstatement that is subsequently caught with probability $p_I$. 
The total expected cost of
low effort is similarly equal to $(1-p_{AL}) \times p_I \times C_R$. 

To complete this new model, we need to make an (equilibrium) assumption about how the CEO and firm auditor interact. 
Following the literature, we assume that the two simultaneously and independently make decisions, and that their strategies form a Nash equilibrium.
That is, we assume the players' strategies are such that they optimize their objectives  taking the actions of the other players as fixed. This means that in a Nash equilibrium, the players are taking actions that they cannot unilaterally improve upon.

In this type of auditing game, the Nash equilibrium has the CEO and the auditor playing mixed (randomized) strategies.
That is, the auditor will independently exert high effort with probability $\alpha^*$ and the CEO independently misstates with probability $\beta^*$. 
These probabilities are such that each party has no incentive to change strategy. 
That is:
%\vspace{-3mm}
\begin{enumerate}
\item the CEO
is indifferent between misstating and not misstating, or:
\begin{equation}\label{manager}
(1 - p_A^*)(1 - p_I) B - C_M = 0 
\end{equation}
where $p_A^* = \alpha^* p_{AH }+ (1-\alpha^*) p_{AL}$ is the equilibrium 
probability a misstatement is detected; and,
\item the auditor is indifferent between exerting high and low effort, or
$$ \beta^* (1-p_{AH}) p_I C_R + C_A = \beta^* (1-p_{AL}) p_I C_R .$$
\end{enumerate}

Solving these two equations for the equilibrium probabilities $\alpha^*$ and $\beta^*$
yields:
\begin{equation}\label{equilstrat}
\begin{array}{lcl}
  \alpha^* &= & \dfrac{ ( 1 - p_{AL}) (1 - p_I) B- C_M}{ (1 - p_I) (p_{AH}-p_{AL}) B}\\[1.5em]
  \beta^* &= & \dfrac{C_A}{(p_{AH}-p_{AL}) p_I C_R}  
\end{array}
\end{equation}
From these equations, we can calculate the equilibrium probability of a restatement\footnote{
As part of the solution, we require $\alpha^*$ and $\beta^*$ to be probabilities between
zero and one. 
This is true provided $C_R$ and $B$ satisfy the inequalities
$ C_R > \frac{C_A}{(p_{AH}-p_{AL})p_I} $
and 
$ B > \frac{C_M}{1 - p_I}  $.}
\begin{equation} \label{equilpr1}
\begin{array}{lcl}
\mbox{Pr(Restate)} & = &  \mbox{Pr(Misstate)} \times \mbox{Pr(Auditor Misses)} \times
\mbox{Pr(Investigation Finds)}\\[1em]
& = & \beta^* \times (1-p_A^*) \times p_I
\end{array}\end{equation}
This equation tells us how the probability of a restatement is related to the unobserved frequency of misstatements.
In particular, if we knew the frequency with which auditors and subsequent investigations caught misstatements, we could easily link the two. 
Otherwise, we would have to estimate these probabilities (or make assumptions about them).

Substituting the equilibrium strategies (\ref{equilstrat}) into (\ref{equilpr1}) yields
\begin{equation} \label{equilpr2}
\begin{array}{lcl}
\mbox{Pr(Restate)}& = &  \dfrac{C_AC_M(1-p_{AL})}{(p_{AH}-p_{AL})(1-p_I)C_RB}.
\end{array}\end{equation}
We now are in a position to use the theory to help interpret the conflicting logistic regression results
in Table \ref{tab:gmm}. 

Equation (\ref{equilpr2}) shows that the presence of a strategic external auditor
changes how the CEO's incentives impact the probability of a restatement.\footnote{Notice that
the probability statement in equation  (\ref{equilpr2})  differs from that in equation (\ref{restate1}).
The probability statement in equation  (\ref{equilpr2}) reflects the randomness of the strategies, whereas in equation (\ref{restate1}) it reflects variables the researcher does not observe.} 
Partial derivatives of equation (\ref{equilpr2}) show that the restatement probability is:
\begin{itemize}
\item Decreasing in the benefit $B$ that the CEO enjoys from misstatement.
\item Increasing in the personal cost of manipulation $C_M$ incurred by the CEO .
\item Decreasing in the reputational cost $C_R$ incurred by the external auditor.
\item Increasing in the cost of high effort $C_A$ incurred by the external auditor.
\end{itemize}

Thus, in contrast to the model with a non-strategic auditor, increasing the benefit that managers enjoy from misstatement, or decreasing the misstatement cost, leads to fewer restatements being observed by researchers.
These two effects might explain the negative sign on EQUITY and the positive sign on SALARY observed in the previous logit results. 
% Thus, this structural model has the potential to rationalize patterns observed in the data with beli
%NOTE: The logit results and an assumption that b_1 < 0 *also* rationalizes the "patterns observed in the data."

To have a better sense of how one might connect the strategic auditor theory to the
logistic models in Table \ref{tab:logit}, suppose, similar to the motivation for equation (\ref{eqns1}), that 
\begin{equation}\begin{array}{lcl}\label{eqns2}
B & = & b_0 + b_1 \, \mbox{EQUITY} \\[.5em]
C_M & = & m_0 + m_1 \, \mbox{SALARY} \\[.5em]
C_A & = & a_0 + a_1 \, \mbox{INT} + a_2 \, \mbox{SEG}\\[.5em]
C_R & = & r_0, \quad p_{AH}   =  p_0, \; \mbox{ and } \; p_{AL}  =  v_0 \\[.5em]
\end{array}
\end{equation}
where $ a_0, a_1, a_2, r_0, b_0, p_0$ and $v_0$ are constant parameters. 
Inserting these expressions into the expected restatement rate (\ref{equilpr2}) gives
\begin{align}
\begin{split} 
\mbox{Pr(Restate)} & =   \dfrac{C_AC_M(1-p_{AL})}{(p_{AH}-p_{AL})(1-p_I)C_RB} \\
&= \dfrac{(1-v_0)(a_0 + a_1 \, \mbox{INT} + a_2 \, \mbox{SEG})(m_0 + m_1 \, \mbox{SALARY})}
{(p_0-v_0)r_0(b_0 + b_1 \, \mbox{EQUITY})} \label{equilpr3}
\end{split}
\end{align}
\begin{equation}\label{equilpr4}
 =  \dfrac{\theta_0 + \theta_1\mbox{\small INT} + \theta_2\mbox{\small SEG} + \theta_3\mbox{\small SALARY}
+ \theta_4\mbox{\small INT} \times \mbox{\small SALARY}+ \theta_5\mbox{\small SEG} \times \mbox{\small SALARY}}
{1 +  \theta_6\mbox{\small EQUITY}}
\end{equation}
Notice that the $\theta$'s absorb unknown quantities such as $r_0$ and $p_0$, and that the denominator intercept
is normalized to one. This last restriction is required to identify the ratio of the two linear functions.

Although this model does not have a logit form, it is potentially estimable using a nonlinear estimation method such as generalized method of moments (GMM).\footnote{
Other estimation strategies are possible, but we do not consider them here.}
GMM attempts to match sample moments to what the structural model implies 
these moments should be. 
For example, an obvious sample moment would be the average restatement rate in the sample.
The corresponding theoretical moment would be the probability expression in equation (\ref{equilpr4}).
In our estimations, we use sample moments of the form:
\begin{equation}\label{gmoment}
\mathcal{M}_j = \sum_{i=1}^{10,000} \; X_{ji}^\prime \left[ \; \mbox{RESTATE}_{i} - \mbox{Pr(Restate}_i\mbox{)} \; \right]. 
\end{equation}
where Pr(Restate) comes from equation (\ref{equilpr4}).\footnote{
To ensure that the model parameters imply restatement probabilities between zero and one, we add a penalty function to the GMM objective function.
This penalty increases with the number of estimated probabilities below zero or above one.
For most replications this penalty is immaterial to the results obtained.}
Practically, we need at least as many moments as we have $\theta$ parameters to estimate (there are seven $\theta$'s in the model).
The $X_j$ used in the moments include all explanatory variables,
plus some interactions (see Table \ref{tab:gmm} for a list).
Again, to illustrate how we estimate the $\theta$ parameters of equation (\ref{equilpr4}), one of the $X$'s is a dummy variable for whether the firm is an international company. The corresponding moment equation in (\ref{gmoment}) seeks to match international companies' average restatement rate to the model's prediction for that rate.

Table \ref{tab:gmm} reports the results of estimating the new (strategic auditor) structural model. 
The results show that in this particular case, even without sample information on the unobserved probabilities $p_A$ and $p_I$, we can recover estimates of the model parameters up to normalizations.
For instance, the coefficient ratio $\theta_4/\theta_1$ estimates the ratio of cost parameters $m_1/m_0$.
The parameter $m_1$ is the cost coefficient on SALARY and 
$m_0 > 0$ is a fixed cost of manipulation. The sign of $\theta_4/\theta_1$ thus reveals the sign of $m_1$.
From the theory, we expect the sign to be positive, and this is what we find in the estimation results.\footnote{The same is not true of $\theta_3/\theta_0$.}

Similarly, $\theta_6$ equals the (scaled) misstatement benefit coefficient on the EQUITY variable.
Recall that the descriptive logit regression coefficients in Table \ref{tab:logit} suggest EQUITY has a negative affect on misstatements.
In contrast, we now find the expected positive relation because we explicitly model the difference between misstatements and restatements in our structural estimation. 

Table \ref{tab:gmm} contains three different sets of estimates.
Column (1) are for an exactly identified model where there are as many instruments as parameters to estimate.
Column (2) presents estimates for a over-identified model in which
there are more moments than parameters to estimate.
Column (3) presents constrained GMM estimates, where the constraints are motivated by the fact that pairs of ratios of the $\theta$ coefficients are equal (e.g., $\theta_3/\theta_0
= \theta_4/\theta_1$). 
The three sets of estimates are similar, with the constrained estimates seemingly yielding
much more statistical precision.

\subsubsection{Implications of structural modeling analyses}
From the discussion above, it seems that there are (at least) two alternative explanations (or hypotheses) for the results we find.
One hypothesis is that the process generating the data is best modeled with a non-strategic auditor and that the effect of EQUITY on incentives to misstate is either negative (or perhaps zero).
The support for this hypothesis comes from Table \ref{tab:logit}, which is an appropriate regression analysis for the model with a non-strategic auditor, where a negative (and weakly statistically significant) coefficient on EQUITY is found.
However a second, and in our view a considerably more plausible, hypothesis is that the process generating the data is best modeled with a strategic auditor and that the effect of EQUITY on incentives to misstate is positive (or perhaps zero).
The support for this hypothesis comes from Table \ref{tab:gmm}, which is predicated on the model with a strategic auditor, and where a positive coefficient on $b_1$ (the parameter linking EQUITY to benefits from misstatement) is found.

The point of this discussion is not to resolve the debate regarding the effect of incentives on misstatements. 
Rather, the goal is to illustrate the necessity of having an underlying structural model of the process by which the data we observe were generated.
The importance of such models was illustrated in Sections \ref{sec:causal} and \ref{sec:mech}, where we used causal diagrams as a kind of (non-parametric) causal model.
Here we have shown more can be inferred from a formal model tied to behavioral assumptions.

Not only does a structural model enable us to derive sharper predictions regarding the relations between variables for various parameterizations, but it also provides a basis for actually estimating those relations. 
In particular, the comparative statistics of the model shed light on the difference between restatements and misstatements, and what assumptions (e.g., a strategic or a non-strategic auditor) and data were needed to draw inferences about misstatements from restatements. 
Additionally, we were able to recover some of the primitive parameters impacting incentives for managers to misstate results, as well as perform counterfactual analyses. Finally, although structural modeling does not allow us to completely resolve questions of causality, if the model is based on reasonable assumptions and has a close fit to the data, we arguably have better insight into the likely causal relations underlying the phenomenon being examined. 
% While there is the potential for disappointment in the simplistic theory we used, we see room for improving models as an opportunity rather than a defining limitation.

\subsubsection{Counterfactuals}

While the coefficient magnitudes only allow us to infer relative
coefficient magnitudes and signs, we nevertheless can use the model to perform counterfactual calculations.
A counterfactual calculation asks what is the consequence of changing some parameters or variables while holding others fixed.
There are many different counterfactuals that could be considered. 
For illustrative purposes, we evaluate what would happen to misstatements and restatements if we do away with equity-based compensation and nothing else changes in the model.
The value of having an equilibrium model to analyze this change is that we explicitly allow the auditing process to adjust to the removal of CEO incentives to misstate. 
From the equilibrium strategies in equation (\ref{equilstrat}), we see that removing equity-based pay ($b_1=0$ or EQUITY=0) does not change the equilibrium frequency of misstatements (i.e., $\beta^*$), but does change the frequency of high effort auditing ($\alpha^*$).
Inserting the estimated parameters into Equation (\ref{equilpr3}), we find
$$ \dfrac{\mbox{Pr(Restate }\vert \mbox{ No Equity)}}{\mbox{Pr(Restate }\vert \mbox{ Equity)}}=\dfrac{\beta^* \times (1-p_A^{**}) \times p_{I}}
{\beta^* \times (1-p_A^{*}) \times p_{I}} = \dfrac{(1-p_A^{**})}{(1-p_A^*)} = 1.24.$$
This result tells us that the restatement rate would increase by 24\% (or from 9.99\% to 12.36\%) if equity-based incentives were removed or
did not impact the benefits of misstatements. 
The fact that the restatement rate goes up may at first seem somewhat odd given that the benefits to the CEOs have fallen. 
The model, however, shows that the increase  comes about because the auditors exert \emph{less} effort in detecting misstatements, thereby catching fewer, leaving more for subsequent investigations to detect.

\subsection{Limitations of structural models}

When we discuss structural models with colleagues, we sometimes encounter two negative reactions.
One objection appears to be that these models are both too complicated and too unrealistic to yield much insight on practical accounting research questions.
The other is related to a preconception that structural modelers believe that all empirical work needs to be structural.

To be clear, we do not believe that all accounting researchers need estimate structural models. 
Indeed, in many situations, descriptive models can be more informative.
Further, no structural modeling exercise should go forward unless the researcher is convinced that the benefits of a structural model would outweigh the substantial costs entailed in developing and estimating a structural model.

We have mentioned some of the benefits of structural models, particularly when it comes to having a clear basis for making causal statements.
What are the costs to developing and estimating structural models? 
First, structural models can be technically demanding to develop. 
Additionally, when constructing a theoretical model that can be taken the data, the empirical researcher is typically forced to make simplifications that a pure theorist might never make and that other empiricists criticize as unrealistic.
Further, as we saw in the restatement model, the structural modeler often is in the uncomfortable position of ex post adding features, such as covariates, to the model that make it more in line with the realities of the application.

While these costs are important to recognize, it is also important to realize that without structural models, there likely will continue to be a substantial divide between theoretical and empirical research in accounting.
With few exceptions, theoretical accounting researchers do not explain how to map the specifics of their models to data. 
In many cases, extant theory is not sufficient to motivate the hypotheses tested by empirical researchers.
%For example, \citet{Huang:2014cs} study the effect ``tone management'' on capital market outcomes. Developing a formal theory of the relation between firm performance, managerial psychological states, and measures of tone would be a complex undertaking involving economics, psychology, and linguistics.Building on such a (hypothetical) foundation to solve the complex game involving managers and capital markets would be extremely ambitious.
Consistent with the existence of this gap, few empirical research papers in accounting rely on formal theoretical models to motivate their hypotheses. 
Often when empirical researchers \emph{do} rely on theoretical papers to motivate hypotheses, the predictions claimed to be derived from those papers have little obvious connection with the actual content of those papers.
% For example, \citet{Hollander:2010jg} suggests that ``one camp [of analytical research] argues that firms with good information quality will issue less expansive disclosures because information asymmetry is lower in such firms \citep[e.g.][]{Verrecchia:1983}," notwithstanding specific results in \citet{Verrecchia:1983} that support this claim.
% Another camp shows that as information quality increases, which, in turn, increases the quality of the manager's information, managers are incentivized to disclose more because investors will deem such disclosures more credible (Verrecchia [1990b])
Instead, almost all empirical research papers in accounting use more informal, verbal approaches to hypothesis development.

Second, although structural models can in principle make it clear what a researcher is assuming about causality, it is incumbent on the structural modeler to make the model's causal relations clear.
One way to do this is to provide a causal diagram.
To illustrate, Figure \ref{fig:audit} provides a causal diagram representing the strategic auditor restatement model.\footnote{
While the mixed-strategy of our model has $\beta$ not being a function of $B$ or $C_M$ and $\alpha$ not being a function of $C_A$ or $C_R$, we have retained these links as being plausible in a more general model.}
As can be seen, we are assuming that $p_I$ is independent of EQUITY. 
But is quite plausible that these investigations are conducted (in part) by a regulator who is as strategic as the auditor in our model, thus giving rise to a link between EQUITY and $p_I$.
We also assume that EQUITY is exogenous, whereas it is plausibly related to the complexity of the business, which may also affect the cost of auditing.
These links could be added to the structural model, albeit at some cost.
Evaluation of their accuracy then could be made via in- or out-of-sample goodness-of-fit tests.

Third, just because a researcher can write down a theoretical model and estimate it does not make the empirical model ``right."
Clearly there is a risk of incorrect causal inferences being drawn from estimation of a structural model based on faulty assumptions.
But, that structural models are capable of recovering behavioral information provided the model is correct can be seen in Table \ref{tab:gmm} by comparing the ``true" coefficients in the last column (i.e., those used to generate the data) with those in the other columns.
Because we also used the same data to estimate the logit regression in Table \ref{tab:logit}, we are able to establish that if we used the wrong (non-strategic) auditor structural model, we would have been led astray in our inferences about the effect of EQUITY.

In practice we do not have this kind of insight into the correct process that generated the data.
Hence, the researcher will need to weigh models based upon how well the model's assumptions match the practical and institutional realities of the phenomena being studied.
%TODO: Tone down or eliminate?
% \cite{Nikolaev:2014er} provide an analysis of misreporting and accounting quality.
% The critical assumption in \citet[p.\,6]{Nikolaev:2014er} is the ``idea that both operating cash flow and earnings can be viewed as noisy measures of underlying economic performance."
%DL: Not sure that I understand the discussion here on Nikolaev
%But the fact that cash flow from operations is intended to measure something quite different (e.g., investing cash flows affect subsequent earnings quite differently from operating cash flows) suggests that this may not be the best assumption with which to identify a structural model.
Despite these challenges, we believe that there is significant value in making the theory underlying empirical research transparent and rigorous.\footnote{The use of structural models in accounting research has been fairly limited to date.  
Recent examples include \citet{Gerakos:2013cl}, \citet{Zakolyukina:2015aa}, and \citet{Bertomeu:2015aa}.  
%NOTE: I don't think this applies to Nikolaev, so I took that paper out.
These three papers model an institutionally rich problem, estimate the derived model, provide estimates for important structural parameters, and also give interesting counterfactuals based on their theoretical models. 
We view these papers as useful initial steps in applying structural approaches to accounting research questions.}

%\subsection{Structural models in accounting research} The use of structural models in accounting research has been fairly limited to date. There are certainly examples of regression models being derived (or perhaps influenced) by theoretical models. For example, \citet{Lambert:1987} use the model of \citet{Holmstrom:1979aa} (and a variety of simplifying assumptions) to help specify a regression function linking CEO compensation to firm performance. While somewhat structural in orientation, the approach suppresses the fundamental causal mechanism associated with compensation decisions and does little to actually estimate structural parameters of the model of \citet{Holmstrom:1979aa}.

% I think Gerakos's paper has a manager who seeks to smooth earnings.\citet{Gerakos:2013cl} assume a simple stochastic process for earnings and assume that managers seek to smooth earnings \citet[p.\,57]{Gerakos:2013cl} find evidence that ``unmanipulated earnings are more correlated with contemporaneous returns and have higher volatility than reported earnings."Similar to our simple model above, \citet{Zakolyukina:2015aa} is concerned with how equity incentives motivate managers to manipulate earnings.  \citet{Bertomeu:2015aa} examine management forecasts using a formal disclosure model to estimate whether managers strategically withhold information from shareholders.These three papers model an institutionally rich problem, estimate the derived model, provide estimates for important structural parameters, and also give interesting counterfactuals based on their theoretical models. We view these papers as useful initial steps in applying structural approaches to accounting research questions.
% \footnote{Maybe review some structural finance papers -- Luke Taylor on CEO labor markets, Whited on debt?, Terry jmp about "meet or beat" and macroeconomic research and development.} % Is this footnote for the editors or for us?


\section{Concluding remarks} \label{sec:conclude}
In this paper, we examined the approaches used by accounting researchers to draw causal inferences from analyses of observational (or non-experimental) data. 
The vast majority of empirical papers using such data seek to draw causal inferences, notwithstanding the well-known difficulties with doing so.
While some papers seek to use quasi-experimental methods to develop unbiased estimates of causal effects, we find that the assumptions required to deliver such estimates are not often credible. 
We believe that clearer communication of research questions and design choices would help researchers avoid some of the conceptual traps that affect 
accounting research.
One tool that may help in this regard are causal diagrams.

We also argued that  accounting research could benefit from a more complete understanding of causal pathways. 
In particular, we believe that structural models based on rigorous theory will see greater use in the coming years.
Finally we see great value to in-depth descriptive studies that inform causal issues and deepen our knowledge of the behavior and institutions we seek to model.
Although our suggestions do not completely resolve controversies surrounding causal inferences drawn from observational data, we believe they offer a viable and exciting path forward.

\end{doublespace}

\clearpage
\bibliography{jar_methods}

\clearpage

\input{causal_graphs}

\clearpage
\include{tables}

\appendix
\section{Causal diagrams: Formalities} \label{append}
In this appendix, we provide a more formal treatment of some of the ideas on causal diagrams discussed in the text.
See \citet{Pearl:2009vo} for more detailed coverage.
 
 \subsection{Definitions and a result}
 We first introduce some basic definitions and a key result.
 
\begin{definition}[$d$-separation, block, collider]
A path $p$ is said to be \emph{$d$-separated} (or \emph{blocked}) by a set of nodes $Z$ if and only if
\begin{enumerate}
	\item $p$ contains a chain $i \rightarrow m \rightarrow j$ or a fork $i \leftarrow m \rightarrow j$ such that the middle node $m$ is in $Z$, or
	\item $p$ contains an inverted fork (or \emph{collider}) $i \rightarrow m \leftarrow j$ such that the middle node $m$ is not in $Z$ and such that no descendant of $m$ is in $Z$
\end{enumerate}
\end{definition}

\begin{definition}[Back-door criterion]
A set of variables $Z$ satisfies the \emph{back-door criterion} relative to a an ordered pair of variables $(X, Y)$ in a 
	DAG $G$ if:
	\begin{itemize}
		\item no node in $Z$ is a descendant of $X$; and
		\item $Z$ blocks every path between $X$ and $Y$ that contains an arrow into $X$.\footnote{The ``arrow into $X$" is the portion of the definition that is explains the ``back-door" terminology.}
	\end{itemize}
\end{definition}%
Given this criterion, \citet[p.\,79]{Pearl:2009vo} proves the following result.
%
\begin{theorem}[Back-door adjustment]
	If a set of variables $Z$ satisfies the back-door criterion relative to $(X, Y)$, then the causal effect of $X$ on $Y$ is identifiable and is given by the formula 
	\[ P(y | x) = \sum_{z} P(y | x, z) P(z), \]
where $P(y|x)$ stands for the probability that $Y = y$, given that $X$ is set to level $X=x$ by external intervention.\footnote{
How the quantities $P(y|x)$ map into estimates of causal effects is not critical to the current discussion, it suffices to note that in a given setting, it can be calculated if the needed variables are observable.}
\end{theorem}
%

 \subsection{Application of back-door criterion to Figure \ref{fig:basic}}
Applying the back-door criterion to Figure \ref{fig:confound} is straightforward and intuitive.
The set of variables $\{Z\}$ or simply $Z$ satisfies the criterion, as $Z$ is not a descendant of $X$ and $Z$ blocks the back-door path $X \leftarrow Z \rightarrow Y$.
So by conditioning on $Z$, we can estimate the causal effect of $X$ on $Y$.
This situation is a generalization of linear model in which $Y = X \beta + Z \gamma + \epsilon_Y$ and $\epsilon_Y$ is independent of $X$ and $Z$, but $X$ and $Z$ are correlated.
In this case, it is well known that omission of $Z$ would result in a biased estimate of $\beta$, the causal effect of $X$ on $Y$, but by including $Z$ in the regression, we get an unbiased estimate of $\beta$.
In this situation, $Z$ is a \emph{confounder}.

Turning to Figure \ref{fig:mech}, we see that $Z$, which is a \emph{mediator} of the effect of $X$ on $Y$, does not satisfy the back-door criterion, because $Z$ is a descendant of $X$.
However, $\emptyset$ (i.e., the empty set) does satisfy the back-door criterion.
Clearly, $\emptyset$ contains no descendant of $X$.
Furthermore, the only path other than $X \rightarrow Y$ that exists is $X \rightarrow Z \rightarrow Y$, which does not have a back-door into $X$.
Note that the back-door criterion not only implies that we need not condition on $Z$ to obtain an unbiased estimate of the causal effect of $X$ on $Y$, but that we should not condition of $Z$ to get such an estimate.

Finally in Figure \ref{fig:collider}, we have $Z$ acting as what \citet[p.\,17]{Pearl:2009kh} refers to as a ``collider" variable.\footnote{
The two arrows from $X$ and $Y$ ``collide" in $Z$.} 
Again, we see that $Z$ does not satisfy the back-door criterion, because $Z$ is a descendant of $X$.
However, $\emptyset$ again satisfies the back-door criterion.
First, contains no descendant of $X$.
Second, the only path other than $X \rightarrow Y$ that exists is $X \rightarrow Z \leftarrow Y$, which does not have a back-door into $X$.
Again, the back-door criterion not only implies that we need not condition on $Z$, but that we should not condition of $Z$ to get an unbiased estimate of the causal effect of $X$ on $Y$.

\subsection{Causal diagrams and instrumental variables}

We now discuss how \emph{correct} causal diagrams can be used to identify valid (or invalid) instruments.
 

\begin{definition}[Instrument]
Let $G$ denote a causal graph in which $X$ has an effect on $Y$. 
Let $G_{\overline{X}}$ denote the causal graph created by deleting all arrows emanating from $X$.
A variable $Z$ is an \emph{instrument} relative to the total effect of $X$ on $Y$ if there exists a set of nodes $S$, unaffected by $X$, such that
\begin{enumerate}
\item $S$ $d$-separates $Z$ from $Y$ in $G_{\overline{X}}$
\item $S$ does not $d$-separate $Z$ from $X$ in $G$
\end{enumerate}
\end{definition}

Applying this definition to Figure \ref{fig:agl}, we can evaluating the instrument used in \citet{Armstrong:2013io}.
There we have have $S = \textit{Comp}_{t-1}$,
$X =\textit{Shareholder support}_{t}$, $Y = \textit{Comp}_{t-1}$, and $Z = \textit{ISS recommendation}_{t}$.
We use $U$ to denote the observed variables depicted in the dashed box of Figure \ref{fig:agl1}.
If create $G_{\overline{X}}$ by deleting the single arrow emanating from $\textit{Shareholder support}_{t}$, we can see that there are two back-door paths running from $Y$ to $Z$: 
$Z \leftarrow S \rightarrow U \rightarrow Y$ and $Z \leftarrow S \rightarrow Y$.
However, both of these paths are blocked by $S$ and the first requirement is satisfied.
The second requirement is clearly satisfied as $Z$ is directly linked to $X$.\footnote{
This is a necessary condition, but assumptions about functional form are also critical in using an instrument to estimate a causal effect.
However, this is not essential to our argument here.}
%
Note that this analysis can be expressed intuitively as requiring that the ISS recommendation only affects \textit{Comp}$_{t+1}$ through its effect on \textit{Shareholder support}$_{t}$, and that the ISS recommendation has an effect on \textit{Shareholder support}$_{t}$.

But this analysis presumes that the causal diagram Figure \ref{fig:agl1} is correct.
\citet[p.\,912]{Armstrong:2013io} note that the ``validity of this instrument depends on ISS recommendations not having an influence on future compensation decisions conditional on shareholder support (i.e., firms listen to their shareholders, with ISS having only an indirect impact on corporate policies through its influence on shareholders' voting decisions)."
This assumption represented in Figure \ref{fig:agl1} by the \emph{absence} of an arrow from \textit{ISS recommendation}$_t$ to \textit{Comp}$_{t+1}$.

Unfortunately, this assumption seems inconsistent with the findings of \citet{Gow:2013aa}, who provide evidence that firms are carefully calibrating compensation plans (i.e., factors that directly affect \textit{Comp}$_{t+1}$) to comply with the requirements of ISS's policies, implying a path from \textit{ISS recommendation}$_t$ to \textit{Comp}$_{t+1}$ that does not pass through \textit{Shareholder support}$_{t}$.
This path is represented in Figure \ref{fig:agl2} and the plausible existence of this path suggests that the instrument of \citet[p.\,912]{Armstrong:2013io} is not credibly valid for the causal effect they seek to estimate.

\end{document}
